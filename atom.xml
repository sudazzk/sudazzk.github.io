<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>JuTzungKuei</title>
  
  <subtitle>zzk&#39;s homepage</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://www.zhuzongkui.top/"/>
  <updated>2019-08-10T02:28:38.207Z</updated>
  <id>http://www.zhuzongkui.top/</id>
  
  <author>
    <name>Zongkui Zhu</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>中文知识图谱问答 CCKS2019 CKBQA - 参赛总结</title>
    <link href="http://www.zhuzongkui.top/2019/08/04/ccks2019_ckbqa/"/>
    <id>http://www.zhuzongkui.top/2019/08/04/ccks2019_ckbqa/</id>
    <published>2019-08-04T08:03:44.000Z</published>
    <updated>2019-08-10T02:28:38.207Z</updated>
    
    <content type="html"><![CDATA[<p>[TOC]</p><ul><li>调研时间：2019.06.05~2019.06.19</li><li>参赛时间：2019.06.28~2019.07.25</li></ul><h1 id="中文知识图谱问答（从0到0-6-）"><a href="#中文知识图谱问答（从0到0-6-）" class="headerlink" title="中文知识图谱问答（从0到0.6+）"></a>中文知识图谱问答（从0到0.6+）</h1><ul><li>Chinese Knowledge Base Question Answering（CKBQA）</li></ul><h2 id="一、任务"><a href="#一、任务" class="headerlink" title="一、任务"></a>一、任务</h2><h3 id="1、任务定义"><a href="#1、任务定义" class="headerlink" title="1、任务定义"></a>1、任务定义</h3><h4 id="什么是知识库？"><a href="#什么是知识库？" class="headerlink" title="什么是知识库？"></a>什么是知识库？</h4><ul><li><p>一条条知识，而把大量的知识汇聚起来就成了知识库。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ex1：奥巴马出生在火奴鲁鲁</span><br><span class="line">ex2：姚明是中国人</span><br><span class="line">ex3：谢霆锋的爸爸是谢贤</span><br></pre></td></tr></table></figure></li><li><p>知识来源：维基百科、百度百科等百科全书</p></li><li>特点：非结构化的自然语言、不适合计算机去处理</li><li>三元组（triple）（为了方便计算机的处理和理解，需要更加形式化、简洁化的方式去表示知识）<ul><li>ex1：奥巴马出生在火奴鲁鲁 —&gt; (奥巴马，出生地，火奴鲁鲁)</li><li>（主语，谓语，宾语）subject predicate object</li><li>（实体，属性，属性值）entity attribute value</li><li>（头实体，关系，尾实体）head_entity relation tail_entity</li></ul></li><li>进一步，把实体看作是结点，把关系看作是一条边，包含大量三元组的知识库就构成了一个庞大的知识图谱<h4 id="什么是知识库问答？"><a href="#什么是知识库问答？" class="headerlink" title="什么是知识库问答？"></a>什么是知识库问答？</h4></li><li>基于知识库问答（knowledge base question answering, KBQA）</li><li>即，给定自然语言问题，通过对问题进行语义理解和解析，进而利用知识库进行查询、推理得出答案。</li><li>按应用领域划分：开放领域（百科知识问答等）和特定领域（金融、医疗、宗教、客服等）</li><li>评价指标：召回率、精确率、F1值、MRR（平均倒数排序）</li><li><script type="math/tex; mode=display">Q\text{为问题集合，}A_i\text{为对第i个问题给出的答案集合，}G_i\text{为第i个问题的标注答案集合}</script></li><li><script type="math/tex; mode=display">Macro Precision = \frac{1}{|Q|} \sum_{i=1}^{|Q|}P_i，P_i = \frac {|A_i \cap G_i|} {|A_i|}</script></li><li><script type="math/tex; mode=display">Macro Recall = \frac{1}{|Q|} \sum_{i=1}^{|Q|}R_i，R_i = \frac {|A_i \cap G_i|} {|G_i|}</script></li><li><script type="math/tex; mode=display">Averaged F1 =  \frac{1}{|Q|} \sum_{i=1}^{|Q|} \frac{2P_iR_i}{P_i+R_i}</script></li><li>两大关键技术<ul><li>==实体链指==：将问句中的实体名字链接到知识库中特定的实体上，涉及到实体识别和实体消歧。</li><li>==关系抽取==：将问句中的实体关系抽取出来，涉及到词性标注、词法句法分析、关系分类等。</li></ul></li><li>举个栗子：姚明的老婆是什么星座？<ul><li>（姚明，妻子，叶莉）—&gt;（叶莉，星座，天蝎）</li></ul></li></ul><h3 id="2、相关评测"><a href="#2、相关评测" class="headerlink" title="2、相关评测"></a>2、相关评测</h3><ul><li>简单问题：<code>NLPCC 2015-2018</code></li><li>复杂问题：<code>CCKS  2018-2019</code></li></ul><h3 id="3、相关数据"><a href="#3、相关数据" class="headerlink" title="3、相关数据"></a>3、相关数据</h3><h4 id="训练集"><a href="#训练集" class="headerlink" title="训练集"></a>训练集</h4><h5 id="NLPCC-2016（14609条）"><a href="#NLPCC-2016（14609条）" class="headerlink" title="NLPCC 2016（14609条）"></a>NLPCC 2016（14609条）</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&lt;question id=1&gt;《机械设计基础》这本书的作者是谁？</span><br><span class="line">&lt;answer id=1&gt;杨可桢，程光蕴，李仲生</span><br><span class="line">==================================================</span><br><span class="line">&lt;question id=2&gt;《高等数学》是哪个出版社出版的？</span><br><span class="line">&lt;answer id=2&gt;武汉大学出版社</span><br><span class="line">==================================================</span><br></pre></td></tr></table></figure><h5 id="CCKS-2019-（2298条）"><a href="#CCKS-2019-（2298条）" class="headerlink" title="CCKS 2019 （2298条）"></a>CCKS 2019 （2298条）</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">q1:莫妮卡·贝鲁奇的代表作？</span><br><span class="line">select ?x where &#123; &lt;莫妮卡·贝鲁奇&gt; &lt;代表作品&gt; ?x. &#125;</span><br><span class="line">&lt;西西里的美丽传说&gt;</span><br><span class="line"></span><br><span class="line">q2:《湖上草》是谁的诗？</span><br><span class="line">select ?x where &#123; ?x &lt;主要作品&gt; &lt;湖上草&gt;. &#125;</span><br><span class="line">&lt;柳如是_（明末&quot;秦淮八艳&quot;之一）&gt;</span><br></pre></td></tr></table></figure><h4 id="知识库"><a href="#知识库" class="headerlink" title="知识库"></a>知识库</h4><h5 id="NLPCC-2016-知识库网盘"><a href="#NLPCC-2016-知识库网盘" class="headerlink" title="NLPCC 2016 知识库网盘"></a>NLPCC 2016 <a href="https://pan.baidu.com/s/1dEYcQXz" target="_blank" rel="noopener">知识库网盘</a></h5><ul><li><p>三元组（43063796条）</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">空气干燥 ||| 别名 ||| 空气干燥</span><br><span class="line">空气干燥 ||| 中文名 ||| 空气干燥</span><br><span class="line">空气干燥 ||| 外文名 ||| air drying</span><br><span class="line">空气干燥 ||| 形式 ||| 两个</span><br><span class="line">空气干燥 ||| 作用 ||| 将空气中的水份去除</span><br></pre></td></tr></table></figure></li><li><p>提及-实体（7623034条）</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">空气 干燥 ||| 空气干燥</span><br><span class="line">air drying ||| 空气干燥 氧化结膜干燥</span><br><span class="line">罗育德 ||| 罗育德</span><br><span class="line">鳞 ||| 鳞       公子鳞</span><br><span class="line">squama ||| 鳞   鳞片</span><br></pre></td></tr></table></figure></li></ul><h5 id="CCKS-2019-知识库网盘，密码：hcu8"><a href="#CCKS-2019-知识库网盘，密码：hcu8" class="headerlink" title="CCKS 2019 知识库网盘，密码：hcu8"></a>CCKS 2019 <a href="https://pan.baidu.com/s/1MOv9PCTcALVIiodUP4bQ2Q" target="_blank" rel="noopener">知识库网盘，密码：hcu8</a></h5><ul><li><p>三元组（41009142条）</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&lt;美国奥可斯（香港）国际控股集团&gt;        &lt;公司名称&gt;      &quot;美国奥可斯（香港）国际控股集团有限公司&quot; .</span><br><span class="line">&lt;美国奥可斯（香港）国际控股集团&gt;        &lt;成立时间&gt;      &quot;2007-06-28&quot; .</span><br><span class="line">&lt;美国奥可斯（香港）国际控股集团&gt;        &lt;经营范围&gt;      &lt;培训&gt; .</span><br><span class="line">&lt;美国奥可斯（香港）国际控股集团&gt;        &lt;经营范围&gt;      &lt;影视&gt; .</span><br><span class="line">&lt;美国奥可斯（香港）国际控股集团&gt;        &lt;公司口号&gt;      &quot;品牌立业，质量最好&quot; .</span><br></pre></td></tr></table></figure></li><li><p>提及 + 实体 + order（13930118条）</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">献陵    献陵_（唐高祖李渊陵墓） 1</span><br><span class="line">献陵    明献陵  2</span><br><span class="line">献陵    献陵_（朝鲜太宗献陵）   3</span><br><span class="line">佛罗伦萨        佛罗伦萨_（意大利托斯卡纳大区首府）     1</span><br><span class="line">佛罗伦萨        佛罗伦萨足球俱乐部      2</span><br></pre></td></tr></table></figure></li><li><p>实体 + 类型 + 值（25182628条）</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&lt;美国奥可斯（香港）国际控股集团&gt;        &lt;类型&gt;  &lt;文学作品&gt; .</span><br><span class="line">&lt;美国奥可斯（香港）国际控股集团&gt;        &lt;类型&gt;  &lt;文化&gt; .</span><br><span class="line">&lt;寻美中国&gt;      &lt;类型&gt;  &lt;品牌&gt; .</span><br><span class="line">&lt;青春是我和你一杯酒的深&gt;        &lt;类型&gt;  &lt;文学作品&gt; .</span><br><span class="line">&lt;青春是我和你一杯酒的深&gt;        &lt;类型&gt;  &lt;网络小说&gt; .</span><br></pre></td></tr></table></figure></li></ul><h3 id="4、相关工作"><a href="#4、相关工作" class="headerlink" title="4、相关工作"></a>4、相关工作</h3><ul><li>NLPCC2015 第1名评测论文</li><li>NLPCC2016 第1-4名评测论文</li><li>NLPCC2017 第1-2名评测论文 + 会议论文</li><li>NLPCC2018 第1名评测论文</li><li>CCKS2018  第1-3名评测论文</li><li>某论坛博客：BERT + KBQA</li></ul><h2 id="二、方法"><a href="#二、方法" class="headerlink" title="二、方法"></a>二、方法</h2><h2 id="三、总结"><a href="#三、总结" class="headerlink" title="三、总结"></a>三、总结</h2><h2 id="四、参考文献"><a href="#四、参考文献" class="headerlink" title="四、参考文献"></a>四、参考文献</h2><p><a href="http://tcci.ccf.org.cn/conference/2015/pages/page05_evadata.html" target="_blank" rel="noopener">NLPCC2015 评测：Open Domain Question Answering</a><br><a href="http://tcci.ccf.org.cn/conference/2016/pages/page05_evadata.html" target="_blank" rel="noopener">NLPCC2016 评测：Open Domain Chinese Question Answering</a><br><a href="http://tcci.ccf.org.cn/conference/2017/taskdata.php" target="_blank" rel="noopener">NLPCC2017 评测：Open Domain Question Answering</a><br><a href="http://tcci.ccf.org.cn/conference/2018/taskdata.php" target="_blank" rel="noopener">NLPCC2018 评测：Open Domain Question Answering</a><br><a href="http://www.ccks2018.cn/?page_id=16" target="_blank" rel="noopener">CCKS2018 评测 任务四：开放领域的中文问答任务</a>，<a href="https://biendata.com/competition/CCKS2018_4/" target="_blank" rel="noopener">CCKS2018 COQA 比赛平台</a><br><a href="http://www.ccks2019.cn/?page_id=62" target="_blank" rel="noopener">CCKS2019 评测 任务六：中文知识图谱问答</a>，<a href="https://biendata.com/competition/ccks_2019_6/" target="_blank" rel="noopener">CCKS2019 CKBQA 比赛平台</a>，<a href="http://pkubase.gstore-pku.com/" target="_blank" rel="noopener">PKU BASE的在线查询终端：gStore</a>，<a href="https://www.w3.org/TR/rdf-sparql-query/" target="_blank" rel="noopener">SPARQL语法规则</a></p><ul><li><a href="http://tcci.ccf.org.cn/conference/2015/papers/246.pdf" target="_blank" rel="noopener">NLPCC2015 1st</a> Ye Z, Jia Z, Yang Y, et al. Research on open domain question answering system[M]//Natural Language Processing and Chinese Computing. Springer, Cham, 2015: 527-540.</li><li><a href="https://link.springer.com/chapter/10.1007%2F978-3-319-50496-4_65" target="_blank" rel="noopener">NLPCC2016 1st</a> Lai Y, Lin Y, Chen J, et al. Open domain question answering system based on knowledge base[M]//Natural Language Understanding and Intelligent Applications. Springer, Cham, 2016: 722-733.</li><li><a href="https://link.springer.com/chapter/10.1007/978-3-319-50496-4_86" target="_blank" rel="noopener">NLPCC2016 2nd</a> Yang F, Gan L, Li A, et al. Combining deep learning with information retrieval for question answering[M]//Natural Language Understanding and Intelligent Applications. Springer, Cham, 2016: 917-925.</li><li><a href="https://link.springer.com/chapter/10.1007/978-3-319-50496-4_25" target="_blank" rel="noopener">NLPCC2016 3rd</a> Xie Z, Zeng Z, Zhou G, et al. Knowledge base question answering based on deep learning models[M]//Natural Language Understanding and Intelligent Applications. Springer, Cham, 2016: 300-311.</li><li><a href="https://link.springer.com/chapter/10.1007/978-3-319-50496-4_82" target="_blank" rel="noopener">NLPCC2016 4th</a> Wang L, Zhang Y, Liu T. A deep learning approach for question answering over knowledge base[M]//Natural Language Understanding and Intelligent Applications. Springer, Cham, 2016: 885-892.</li><li><a href="http://tcci.ccf.org.cn/conference/2017/papers/2003.pdf" target="_blank" rel="noopener">NLPCC2017 1st</a> Lai Y, Jia Y, Lin Y, et al. A Chinese question answering system for single-relation factoid questions[C]//National CCF Conference on Natural Language Processing and Chinese Computing. Springer, Cham, 2017: 124-135.</li><li><a href="http://tcci.ccf.org.cn/conference/2017/papers/2041.pdf" target="_blank" rel="noopener">NLPCC2017 2nd</a> Zhang H, Zhu M, Wang H. A Retrieval-Based Matching Approach to Open Domain Knowledge-Based Question Answering[C]//National CCF Conference on Natural Language Processing and Chinese Computing. Springer, Cham, 2017: 701-711.</li><li><a href="http://xbna.pku.edu.cn/CN/10.13209/j.0479-8023.2017.155" target="_blank" rel="noopener">NLPCC2017 会议</a> 周博通, 孙承杰, 林磊, et al. 基于LSTM的大规模知识库自动问答[J]. 北京大学学报：自然科学版, 2018.</li><li><a href="https://link.springer.com/chapter/10.1007/978-3-319-99501-4_35" target="_blank" rel="noopener">NLPCC2018 1st</a> Ni H, Lin L, Xu G. A Relateness-Based Ranking Method for Knowledge-Based Question Answering[C]//CCF International Conference on Natural Language Processing and Chinese Computing. Springer, Cham, 2018: 393-400.</li><li><a href="http://ceur-ws.org/Vol-2242/" target="_blank" rel="noopener">CCKS2018 1st</a> A QA Search Algorithm based on the Fusion Integration of Text Similarity and Graph Computation</li><li><a href="http://ceur-ws.org/Vol-2242/" target="_blank" rel="noopener">CCKS2018 2nd</a> A Joint Model of Entity Linking and Predicate Recognition for Knowledge Base Question Answering </li><li><a href="http://ceur-ws.org/Vol-2242/" target="_blank" rel="noopener">CCKS2018 3rd</a> Semantic Parsing for Multiple-relation Chinese Question Answering </li></ul><h2 id="五、相关博客"><a href="#五、相关博客" class="headerlink" title="五、相关博客"></a>五、相关博客</h2><ul><li><a href="https://zhuanlan.zhihu.com/p/53796189" target="_blank" rel="noopener">基于知识图谱的问答系统入门之—NLPCC2016KBQA数据集</a> ，<a href="https://github.com/jkszw2014/bert-kbqa-NLPCC2017" target="_blank" rel="noopener">Github：bert-kbqa-NLPCC2017</a></li><li><a href="https://zhuanlan.zhihu.com/p/62946533" target="_blank" rel="noopener">基于BERT的KBQA探索</a>，<a href="https://github.com/WenRichard/KBQA-BERT" target="_blank" rel="noopener">Github：KBQA-BERT</a></li><li><a href="https://github.com/songlei1994/ccks2018" target="_blank" rel="noopener">CCKS2018 CKBQA 1st 方案</a></li><li><a href="http://blog.openkg.cn/自由讨论-kbqa从入门到放弃-入门篇/" target="_blank" rel="noopener">自由讨论 | KBQA从入门到放弃—入门篇</a></li><li><a href="https://zhuanlan.zhihu.com/p/28553553" target="_blank" rel="noopener">KBQA从入门到放弃 - Part 2 | 每周话题精选 #09</a></li><li><a href="https://blog.csdn.net/u012892939/article/details/79451978" target="_blank" rel="noopener">KBQA 知识库问答领域研究综述（未完待续。。）</a></li><li><a href="https://zhuanlan.zhihu.com/p/34585912" target="_blank" rel="noopener">基于知识库的问答：seq2seq模型实践</a></li><li><a href="https://yzhihao.github.io/2017/07/15/KBQA.html" target="_blank" rel="noopener">KBQA 个人总结</a></li><li><a href="https://zhuanlan.zhihu.com/p/27141786" target="_blank" rel="noopener">揭开知识库问答KB-QA的面纱0·导读篇</a></li><li><a href="http://octopuscoder.github.io/2018/02/04/知识图谱问答总结/" target="_blank" rel="noopener">知识图谱问答总结</a></li><li><a href="https://blog.csdn.net/TgqDT3gGaMdkHasLZv/article/details/78146295" target="_blank" rel="noopener">肖仰华 | 基于知识图谱的问答系统</a></li><li><a href="https://blog.csdn.net/keyue123/article/details/85266355" target="_blank" rel="noopener">基于知识图谱的问答系统(KBQA)</a></li><li><a href="https://blog.csdn.net/u012892939/article/details/79476756" target="_blank" rel="noopener">各类QA问答系统的总结与技术实现（持续更新）</a></li><li><a href="http://pelhans.com/2018/04/28/xiaoxiangkg-note9/" target="_blank" rel="noopener">知识图谱入门 (九)知识问答</a></li><li><a href="https://zhuanlan.zhihu.com/p/27665853" target="_blank" rel="noopener">KBQA: 基于开放域知识库上的QA系统 | 每周一起读</a></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;[TOC]&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;调研时间：2019.06.05~2019.06.19&lt;/li&gt;
&lt;li&gt;参赛时间：2019.06.28~2019.07.25&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&quot;中文知识图谱问答（从0到0-6-）&quot;&gt;&lt;a href=&quot;#中文知识图谱问
      
    
    </summary>
    
      <category term="总结" scheme="http://www.zhuzongkui.top/categories/%E6%80%BB%E7%BB%93/"/>
    
    
      <category term="NLP" scheme="http://www.zhuzongkui.top/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>常用的排序算法</title>
    <link href="http://www.zhuzongkui.top/2019/04/17/sort/"/>
    <id>http://www.zhuzongkui.top/2019/04/17/sort/</id>
    <published>2019-04-17T12:50:46.000Z</published>
    <updated>2019-08-04T08:45:31.324Z</updated>
    
    <content type="html"><![CDATA[<h2 id="一、复杂度"><a href="#一、复杂度" class="headerlink" title="一、复杂度"></a>一、复杂度</h2><div class="table-container"><table><thead><tr><th>排序方式</th><th>平均T(n)</th><th>最坏T(n)</th><th>最好T(n)</th><th>空间复杂度</th><th>稳定性</th></tr></thead><tbody><tr><td>插入排序</td><td>O(n^2)</td><td>O(n^2)</td><td>O(n)</td><td>O(1)</td><td>稳定</td></tr><tr><td>冒泡排序</td><td>O(n^2)</td><td>O(n^2)</td><td>O(n)</td><td>O(1)</td><td>稳定</td></tr><tr><td>选择排序</td><td>O(n^2)</td><td>O(n^2)</td><td>O(n^2)</td><td>O(1)</td><td>不稳定</td></tr><tr><td>希尔排序</td><td>O(n^1.3)</td><td>O(n^2)</td><td>O(n)</td><td>O(1)</td><td>不稳定</td></tr><tr><td>快速排序</td><td>O(nlogn)</td><td>O(n^2)</td><td>O(nlogn)</td><td>O(logn)</td><td>不稳定</td></tr><tr><td>堆排序</td><td>O(nlogn)</td><td>O(nlogn)</td><td>O(nlogn)</td><td>O(1)</td><td>不稳定</td></tr><tr><td>归并排序</td><td>O(nlogn)</td><td>O(nlogn)</td><td>O(nlogn)</td><td>O(n)</td><td>稳定</td></tr><tr><td>基数排序</td><td>O(d(n+r))</td><td>O(d(n+r))</td><td>O(d(n+r))</td><td>O(n+rd)</td><td>稳定</td></tr></tbody></table></div><hr><h2 id="二、代码实现"><a href="#二、代码实现" class="headerlink" title="二、代码实现"></a>二、代码实现</h2><h3 id="1、直接插入排序-insert"><a href="#1、直接插入排序-insert" class="headerlink" title="1、直接插入排序 insert"></a>1、直接插入排序 insert</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 将 L[i] 插入到已经有序的子序列 L[1 2 ... i-1] 中</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">InsertSort</span><span class="params">(arr)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, len(arr)):</span><br><span class="line">        j = i</span><br><span class="line">        <span class="keyword">while</span> j &gt; <span class="number">0</span> <span class="keyword">and</span> arr[j] &lt; arr[j<span class="number">-1</span>]:</span><br><span class="line">            arr[j], arr[j<span class="number">-1</span>] = arr[j<span class="number">-1</span>], arr[j]</span><br><span class="line">            j -= <span class="number">1</span></span><br></pre></td></tr></table></figure><h3 id="2、冒泡排序-bubble"><a href="#2、冒泡排序-bubble" class="headerlink" title="2、冒泡排序 bubble"></a>2、冒泡排序 bubble</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 对相邻的元素两两进行比较，顺序相反则进行交换，这样每一趟最大的元素浮到顶端</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">BubbleSort</span><span class="params">(arr)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(arr)<span class="number">-1</span>):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(len(arr)<span class="number">-1</span>-i):</span><br><span class="line">            <span class="keyword">if</span> arr[j] &gt; arr[j+<span class="number">1</span>]:</span><br><span class="line">                arr[j], arr[j+<span class="number">1</span>] = arr[j+<span class="number">1</span>], arr[j]</span><br></pre></td></tr></table></figure><h3 id="3、简单选择排序-select"><a href="#3、简单选择排序-select" class="headerlink" title="3、简单选择排序 select"></a>3、简单选择排序 select</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 第i趟排序从 L[i,i+1,...,n] 中选择关键字最小的元素与 L[i] 比较</span></span><br><span class="line"><span class="comment"># 每一趟从待排序的数据元素中选择最小的元素作为首元素</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">SelectSort</span><span class="params">(arr)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(arr)<span class="number">-1</span>):</span><br><span class="line">        minv = i</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(i+<span class="number">1</span>, len(arr)):</span><br><span class="line">            <span class="keyword">if</span> arr[j] &lt; arr[minv]:</span><br><span class="line">                minv = j</span><br><span class="line">        <span class="keyword">if</span> minv != i:</span><br><span class="line">            arr[minv], arr[i] = arr[i], arr[minv]</span><br></pre></td></tr></table></figure><h3 id="4、希尔排序-shell（跳着插入排序）"><a href="#4、希尔排序-shell（跳着插入排序）" class="headerlink" title="4、希尔排序 shell（跳着插入排序）"></a>4、希尔排序 shell（跳着插入排序）</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">距离为 di 的记录放在同一个组中，进行直接插入排序</span><br><span class="line">di = <span class="number">4</span>, <span class="number">2</span>, <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ShellSort</span><span class="params">(arr)</span>:</span></span><br><span class="line">    step = len(arr) / <span class="number">2</span></span><br><span class="line">    <span class="keyword">while</span> step &gt; <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(step, len(arr)):  <span class="comment"># 插入排序</span></span><br><span class="line">            j = i</span><br><span class="line">            <span class="keyword">while</span> j &gt;= step <span class="keyword">and</span> arr[j] &lt; arr[j-step]:</span><br><span class="line">                arr[j], arr[j-step] = arr[j-step], arr[j]</span><br><span class="line">                j -= step</span><br><span class="line">        step = step / <span class="number">2</span></span><br></pre></td></tr></table></figure><h3 id="5、快速排序-quick"><a href="#5、快速排序-quick" class="headerlink" title="5、快速排序 quick"></a>5、快速排序 quick</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">待排序表 L[<span class="number">1</span>,<span class="number">2</span>,...,n] 中任取一个元素 pivot 作为基准或枢纽，</span><br><span class="line">将表划分成两部分，一部分小于 pivot，一部分大于或等于 pivot</span><br><span class="line">L[<span class="number">1</span>,<span class="number">2</span>,...,k<span class="number">-1</span>] 和 L[k+<span class="number">1</span>,...,n] , L[k] = pivot</span><br><span class="line"></span><br><span class="line">操作：</span><br><span class="line">以当前表中第一个元素作为枢纽，对表进行划分</span><br><span class="line">将表中比枢纽值大的元素向右移动，小的向左移动</span><br><span class="line">移动采用从两端往中间夹入的方式（可用于求n个元素中第k小的元素）</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Partition</span><span class="params">(arr, begin, end)</span>:</span>  <span class="comment"># 划分元素</span></span><br><span class="line">    pivot = arr[begin]  <span class="comment"># 选取第一个元素作为基准</span></span><br><span class="line">    left = begin + <span class="number">1</span></span><br><span class="line">    right = end</span><br><span class="line">    <span class="keyword">while</span> <span class="keyword">True</span>:</span><br><span class="line">        <span class="keyword">while</span> left &lt;= right <span class="keyword">and</span> arr[left] &lt;= pivot:</span><br><span class="line">            left += <span class="number">1</span></span><br><span class="line">        <span class="keyword">while</span> left &lt;= right <span class="keyword">and</span> arr[right] &gt;= pivot:</span><br><span class="line">            right -= <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> left &lt; right:</span><br><span class="line">            arr[left], arr[right] = arr[right], arr[left]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">    arr[begin], arr[right] = arr[right], pivot <span class="comment"># 划分元素放到中间位置</span></span><br><span class="line">    <span class="keyword">return</span> right  <span class="comment"># 返回划分元素的下标</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">QuickSort</span><span class="params">(arr, begin, end)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> begin &lt; end:</span><br><span class="line">        k = Partition(arr, begin, end)</span><br><span class="line">        Partition(arr, begin, k<span class="number">-1</span>)</span><br><span class="line">        Partition(arr, k+<span class="number">1</span>, end)</span><br></pre></td></tr></table></figure><h3 id="6、堆排序-heap-sort"><a href="#6、堆排序-heap-sort" class="headerlink" title="6、堆排序 heap sort"></a>6、堆排序 heap sort</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">大顶堆（完全二叉树）：子节点小于父节点</span><br><span class="line">建堆 A[<span class="number">0</span>,<span class="number">2</span>,...,n<span class="number">-1</span>]，移除根节点，将A[<span class="number">0</span>]与A[n<span class="number">-1</span>]交换，</span><br><span class="line">做最大堆调整的递归运算，建堆 A[<span class="number">0</span>,<span class="number">2</span>,...,n<span class="number">-2</span>]，将A[<span class="number">0</span>]与A[n<span class="number">-2</span>]交换，</span><br><span class="line">直到 A[<span class="number">0</span>]与A[<span class="number">1</span>]交换</span><br><span class="line">思想可用于求大量元素中最小的或最大的几个元素</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">MaxHeap_adjust</span><span class="params">(arr, low, high)</span>:</span></span><br><span class="line">    tmp = arr[low]  <span class="comment"># 父节点</span></span><br><span class="line">    <span class="keyword">while</span> <span class="number">2</span> * low + <span class="number">1</span> &lt;= high:</span><br><span class="line">        child = <span class="number">2</span> * low + <span class="number">1</span>  <span class="comment"># 左子节点</span></span><br><span class="line">        <span class="keyword">if</span> child &lt; high <span class="keyword">and</span> arr[child] &lt; arr[child+<span class="number">1</span>]:</span><br><span class="line">            child += <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> arr[child] &lt; tmp:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        arr[low] = arr[child]</span><br><span class="line">        low = child</span><br><span class="line">    arr[low] = tmp</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">MaxHeapSort</span><span class="params">(arr)</span>:</span></span><br><span class="line">    n = len(arr)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(n/<span class="number">2</span><span class="number">-1</span>, <span class="number">-1</span>, <span class="number">-1</span>):  <span class="comment"># 从下往上调</span></span><br><span class="line">        MaxHeap_adjust(arr, i, n<span class="number">-1</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(n<span class="number">-1</span>, <span class="number">0</span>, <span class="number">-1</span>):</span><br><span class="line">        arr[i], arr[<span class="number">0</span>] = arr[<span class="number">0</span>], arr[i]  <span class="comment"># 最大值放最后面</span></span><br><span class="line">        MaxHeap_adjust(arr, <span class="number">0</span>, i<span class="number">-1</span>)  <span class="comment"># 重新调整一下</span></span><br></pre></td></tr></table></figure><h3 id="7、二路归并排序-merge（分治）"><a href="#7、二路归并排序-merge（分治）" class="headerlink" title="7、二路归并排序 merge（分治）"></a>7、二路归并排序 merge（分治）</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">n 个记录看成是 n 个有序的子表，两两归并，得到 n/<span class="number">2</span> 个有序表，再归并</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Merge</span><span class="params">(left, right)</span>:</span></span><br><span class="line">    i, j = <span class="number">0</span>, <span class="number">0</span></span><br><span class="line">    res = []  <span class="comment"># 缺点：需要辅助空间</span></span><br><span class="line">    <span class="keyword">while</span> i &lt; len(left) <span class="keyword">and</span> j &lt; len(right):</span><br><span class="line">        <span class="keyword">if</span> left[i] &lt;= right[j]:</span><br><span class="line">            res.append(left[i])</span><br><span class="line">            i += <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            res.append(right[j])</span><br><span class="line">            j += <span class="number">1</span></span><br><span class="line">    res += left[i:] <span class="keyword">if</span> i &lt; len(left) <span class="keyword">else</span> right[j:]</span><br><span class="line">    <span class="keyword">return</span> res</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">MergeSort</span><span class="params">(arr)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> len(arr) &lt;= <span class="number">1</span>:</span><br><span class="line">        <span class="keyword">return</span> arr</span><br><span class="line">    middle = len(arr) / <span class="number">2</span></span><br><span class="line">    left = MergeSort(arr[:middle])</span><br><span class="line">    right = MergeSort(arr[middle:])</span><br><span class="line">    <span class="keyword">return</span> Merge(left, right)</span><br></pre></td></tr></table></figure><h3 id="8、基数排序-radix（桶排序）"><a href="#8、基数排序-radix（桶排序）" class="headerlink" title="8、基数排序 radix（桶排序）"></a>8、基数排序 radix（桶排序）</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">对数字最高位优先和最低位优先进行排序</span><br><span class="line">例如：先按个位从小到大，再按十位从小到大，再按百位从小到大</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;一、复杂度&quot;&gt;&lt;a href=&quot;#一、复杂度&quot; class=&quot;headerlink&quot; title=&quot;一、复杂度&quot;&gt;&lt;/a&gt;一、复杂度&lt;/h2&gt;&lt;div class=&quot;table-container&quot;&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;排序方式&lt;/
      
    
    </summary>
    
      <category term="笔记" scheme="http://www.zhuzongkui.top/categories/%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="数据结构" scheme="http://www.zhuzongkui.top/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"/>
    
      <category term="算法" scheme="http://www.zhuzongkui.top/tags/%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>安装 cuda、cudnn、nvidia 驱动</title>
    <link href="http://www.zhuzongkui.top/2019/04/17/install_cuda/"/>
    <id>http://www.zhuzongkui.top/2019/04/17/install_cuda/</id>
    <published>2019-04-17T10:47:56.000Z</published>
    <updated>2019-08-04T08:45:31.323Z</updated>
    
    <content type="html"><![CDATA[<h2 id="〇、TensorFlow-与-cuda-的对应版本"><a href="#〇、TensorFlow-与-cuda-的对应版本" class="headerlink" title="〇、TensorFlow 与 cuda 的对应版本"></a>〇、TensorFlow 与 cuda 的对应版本</h2><ul><li>官方链接：<a href="https://tensorflow.google.cn/install/source" target="_blank" rel="noopener">https://tensorflow.google.cn/install/source</a><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">版本                Python 版本     编译器   编译工具      cuDNNCUDA</span><br><span class="line">tensorflow_gpu-1.13.02.7、3.3-3.6GCC 4.8Bazel 0.19.27.410.0</span><br><span class="line">tensorflow_gpu-1.12.02.7、3.3-3.6GCC 4.8Bazel 0.15.079</span><br><span class="line">tensorflow_gpu-1.11.02.7、3.3-3.6GCC 4.8Bazel 0.15.079</span><br><span class="line">tensorflow_gpu-1.10.02.7、3.3-3.6GCC 4.8Bazel 0.15.079</span><br><span class="line">tensorflow_gpu-1.9.02.7、3.3-3.6GCC 4.8Bazel 0.11.079</span><br><span class="line">tensorflow_gpu-1.8.02.7、3.3-3.6GCC 4.8Bazel 0.10.079</span><br><span class="line">tensorflow_gpu-1.7.02.7、3.3-3.6GCC 4.8Bazel 0.9.079</span><br><span class="line">tensorflow_gpu-1.6.02.7、3.3-3.6GCC 4.8Bazel 0.9.079</span><br><span class="line">tensorflow_gpu-1.5.02.7、3.3-3.6GCC 4.8Bazel 0.8.079</span><br><span class="line">tensorflow_gpu-1.4.02.7、3.3-3.6GCC 4.8Bazel 0.5.468</span><br><span class="line">tensorflow_gpu-1.3.02.7、3.3-3.6GCC 4.8Bazel 0.4.568</span><br><span class="line">tensorflow_gpu-1.2.02.7、3.3-3.6GCC 4.8Bazel 0.4.55.18</span><br><span class="line">tensorflow_gpu-1.1.02.7、3.3-3.6GCC 4.8Bazel 0.4.25.18</span><br><span class="line">tensorflow_gpu-1.0.02.7、3.3-3.6GCC 4.8Bazel 0.4.25.18</span><br></pre></td></tr></table></figure></li></ul><hr><h2 id="一、查看版本"><a href="#一、查看版本" class="headerlink" title="一、查看版本"></a>一、查看版本</h2><h3 id="1、查看-cuda-版本"><a href="#1、查看-cuda-版本" class="headerlink" title="1、查看 cuda 版本"></a>1、查看 cuda 版本</h3><ul><li>cat /usr/local/cuda/version.txt</li><li>nvcc -V</li></ul><h3 id="2、查看-cudnn-版本"><a href="#2、查看-cudnn-版本" class="headerlink" title="2、查看 cudnn 版本"></a>2、查看 cudnn 版本</h3><ul><li>cat /usr/local/cuda/include/cudnn.h | grep CUDNN_MAJOR -A 2</li><li>cat /usr/include/cudnn.h | grep CUDNN_MAJOR -A 2</li></ul><h3 id="3、查看显卡驱动版本"><a href="#3、查看显卡驱动版本" class="headerlink" title="3、查看显卡驱动版本"></a>3、查看显卡驱动版本</h3><ul><li>cat /proc/driver/nvidia/version</li><li>nvidia-smi</li></ul><h2 id="二、安装"><a href="#二、安装" class="headerlink" title="二、安装"></a>二、安装</h2><h3 id="1、安装cuda"><a href="#1、安装cuda" class="headerlink" title="1、安装cuda"></a>1、安装cuda</h3><ul><li><a href="https://www.cnblogs.com/iloveblog/p/7683349.html" target="_blank" rel="noopener">Ubuntu16.04+cuda9.0安装教程</a></li><li>下载链接：<a href="https://developer.nvidia.com/cuda-toolkit-archive" target="_blank" rel="noopener">https://developer.nvidia.com/cuda-toolkit-archive</a></li><li>执行命令：<code>sudo sh cuda_9.0.176_384.81_linux.run</code> ，不要安装驱动</li><li>配置系统环境变量：<code>sudo vim /etc/profile</code> 或者用户环境变量：<code>vim ~/.bashrc</code><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">export  PATH=/usr/local/cuda-9.0/bin:$PATH</span><br><span class="line">export  LD_LIBRARY_PATH=/usr/local/cuda-9.0/lib64$LD_LIBRARY_PATH</span><br><span class="line"></span><br><span class="line"># 系统环境变量可能要重启电脑 `sudo reboot`</span><br></pre></td></tr></table></figure></li></ul><h3 id="2、安装nvidia驱动"><a href="#2、安装nvidia驱动" class="headerlink" title="2、安装nvidia驱动"></a>2、安装nvidia驱动</h3><ul><li>列出所有可用的 NVIDIA 设备信息：<code>nvidia-smi -L</code></li><li>查找适配自己电脑GPU的驱动：<a href="http://www.nvidia.cn/Download/index.aspx?lang=cn" target="_blank" rel="noopener">http://www.nvidia.cn/Download/index.aspx?lang=cn</a></li><li><a href="https://blog.csdn.net/u012897374/article/details/79966794" target="_blank" rel="noopener">解决nvidia升级驱动后版本匹配问题</a><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get purge nvidia*</span><br><span class="line">sudo add-apt-repository ppa:graphics-drivers/ppa</span><br><span class="line">sudo apt-get update</span><br><span class="line">sudo apt-get install nvidia-384 nvidia-settings</span><br><span class="line">reboot</span><br><span class="line">注：如果要是不行，可能需要在bios里将显卡先设置成CPU集卡</span><br><span class="line">验证</span><br><span class="line">1.输入nvidia-smi查看</span><br><span class="line">2.prime-select query查看当前选用的显卡</span><br></pre></td></tr></table></figure></li></ul><h3 id="3、安装cudnn"><a href="#3、安装cudnn" class="headerlink" title="3、安装cudnn"></a>3、安装cudnn</h3><ul><li>下载链接：<a href="https://developer.nvidia.com/rdp/cudnn-archive" target="_blank" rel="noopener">https://developer.nvidia.com/rdp/cudnn-archive</a></li><li><p>执行命令拷贝文件，后者用软连接</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tar -zxvf cudnn-9.0-linux-x64-v7.1.tgz</span><br><span class="line">sudo cp cuda/include/cudnn.h /usr/local/cuda/include/ </span><br><span class="line">sudo cp cuda/lib64/libcudnn* /usr/local/cuda/lib64/ -d </span><br><span class="line">sudo chmod a+r /usr/local/cuda/include/cudnn.h </span><br><span class="line">sudo chmod a+r /usr/local/cuda/lib64/libcudnn*</span><br></pre></td></tr></table></figure></li><li><p>配置系统环境变量：<code>sudo vim /etc/profile</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">export PATH=/usr/local/cuda/bin$&#123;PATH:+:$&#123;PATH&#125;&#125;</span><br><span class="line">export LD_LIBRARY_PATH=/usr/local/cuda/lib64$&#123;LD_LIBRARY_PATH:+:$&#123;LD_LIBRARY_PATH&#125;&#125;</span><br><span class="line">export CUDA_HOME=/usr/local/cuda</span><br></pre></td></tr></table></figure></li><li><p>刷新环境变量：<code>source /etc/profile</code></p></li></ul><h2 id="三、卸载"><a href="#三、卸载" class="headerlink" title="三、卸载"></a>三、卸载</h2><h3 id="卸载-cuda"><a href="#卸载-cuda" class="headerlink" title="卸载 cuda"></a>卸载 cuda</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">cd /usr/local/cuda/bin</span><br><span class="line">sudo ./uninstall_cuda_8.0.pl</span><br><span class="line">cd /usr/local</span><br><span class="line">sudo rm -rf cuda-8.0</span><br></pre></td></tr></table></figure><h2 id="四、相关链接"><a href="#四、相关链接" class="headerlink" title="四、相关链接"></a>四、相关链接</h2><h3 id="官方教程"><a href="#官方教程" class="headerlink" title="官方教程"></a><a href="https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html#handle-uninstallation" target="_blank" rel="noopener">官方教程</a></h3><h3 id="cuda、cudnn、驱动版本查看及nvidia驱动、cuda安装及卸载"><a href="#cuda、cudnn、驱动版本查看及nvidia驱动、cuda安装及卸载" class="headerlink" title="cuda、cudnn、驱动版本查看及nvidia驱动、cuda安装及卸载"></a><a href="https://blog.csdn.net/u013187057/article/details/81475806" target="_blank" rel="noopener">cuda、cudnn、驱动版本查看及nvidia驱动、cuda安装及卸载</a></h3><h3 id="ubuntu下卸载cuda8-0，和安装cuda9-0，cudnn7-0-tensorflow-gpu-1-8"><a href="#ubuntu下卸载cuda8-0，和安装cuda9-0，cudnn7-0-tensorflow-gpu-1-8" class="headerlink" title="ubuntu下卸载cuda8.0，和安装cuda9.0，cudnn7.0,tensorflow-gpu=1.8"></a><a href="https://blog.csdn.net/pursuit_zhangyu/article/details/80232550" target="_blank" rel="noopener">ubuntu下卸载cuda8.0，和安装cuda9.0，cudnn7.0,tensorflow-gpu=1.8</a></h3>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;〇、TensorFlow-与-cuda-的对应版本&quot;&gt;&lt;a href=&quot;#〇、TensorFlow-与-cuda-的对应版本&quot; class=&quot;headerlink&quot; title=&quot;〇、TensorFlow 与 cuda 的对应版本&quot;&gt;&lt;/a&gt;〇、TensorFlo
      
    
    </summary>
    
      <category term="笔记" scheme="http://www.zhuzongkui.top/categories/%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="深度学习" scheme="http://www.zhuzongkui.top/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>评价准则</title>
    <link href="http://www.zhuzongkui.top/2018/10/07/evaluation/"/>
    <id>http://www.zhuzongkui.top/2018/10/07/evaluation/</id>
    <published>2018-10-07T03:35:50.000Z</published>
    <updated>2019-08-04T08:45:31.319Z</updated>
    
    <content type="html"><![CDATA[<ul><li><a href="http://www.cnblogs.com/robert-dlut/p/5276927.html" target="_blank" rel="noopener">谈谈评价指标中的宏平均和微平均</a></li><li><a href="https://blog.csdn.net/sinat_26917383/article/details/51114244" target="_blank" rel="noopener">分类器的性能表现评价（混淆矩阵，准确率，召回率，F1,mAP、ROC曲线）</a></li><li><a href="https://blog.csdn.net/sinat_26917383/article/details/75199996?locationNum=3&amp;fps=1" target="_blank" rel="noopener">python + sklearn ︱分类效果评估——acc、recall、F1、ROC、回归、距离</a></li><li><a href="https://www.cnblogs.com/sddai/p/5696870.html" target="_blank" rel="noopener">准确率(Accuracy), 精确率(Precision), 召回率(Recall)和F1-Measure</a></li></ul><hr><p>对于二分类问题，可将样例根据其真实类别和分类器预测类别划分为：</p><div class="table-container"><table><thead><tr><th>0</th><th style="text-align:center">1</th><th style="text-align:center">2</th><th style="text-align:center">真实类别</th><th style="text-align:center">预测类别</th></tr></thead><tbody><tr><td>真正例</td><td style="text-align:center">True Positive</td><td style="text-align:center">TP</td><td style="text-align:center">正例</td><td style="text-align:center">正例 </td></tr><tr><td>假正例</td><td style="text-align:center">False Positive</td><td style="text-align:center">FP</td><td style="text-align:center">负例</td><td style="text-align:center">正例 </td></tr><tr><td>假负例</td><td style="text-align:center">False Negative</td><td style="text-align:center">FN</td><td style="text-align:center">正例</td><td style="text-align:center">负例 </td></tr><tr><td>真负例</td><td style="text-align:center">True Negative</td><td style="text-align:center">TN</td><td style="text-align:center">负例</td><td style="text-align:center">负例 </td></tr></tbody></table></div><p>然后可以构建混淆矩阵（Confusion Matrix）如下表所示：</p><div class="table-container"><table><thead><tr><th>-</th><th style="text-align:center">预测 正例</th><th style="text-align:center">预测 负例 </th></tr></thead><tbody><tr><td><strong>真实 正例</strong></td><td style="text-align:center">TP</td><td style="text-align:center">FN </td></tr><tr><td><strong>真实 负例</strong></td><td style="text-align:center">FP</td><td style="text-align:center">TN</td></tr></tbody></table></div><hr><script type="math/tex; mode=display">\text{测试集：} X_{test} = \{(x_i, y_i) | i = 1, 2, ..., N \}</script><script type="math/tex; mode=display">N \text{：表示测试集中的样本个数}</script><script type="math/tex; mode=display">x_i \text{：表示测试集中的数据样本}</script><script type="math/tex; mode=display">y_i \text{：表示数据样本的类别号}</script><script type="math/tex; mode=display">\text{假设要研究的分类问题含有 m 个类别，则} y_i \in \{c_1, c_2, ..., c_m \}</script><script type="math/tex; mode=display">\text{在分类问题中对于测试集的第j个类别，假设被正确分类的样本数量为} TP_j</script><script type="math/tex; mode=display">\text{被错误分类的样本数量为} FN_j</script><script type="math/tex; mode=display">\text{其它类别被错误分类为该类的样本数量为} FP_j</script><hr><h2 id="精确度"><a href="#精确度" class="headerlink" title="精确度"></a>精确度</h2><script type="math/tex; mode=display">Accuracy = \frac {\sum_{j=1}^mTP_j} {N} = \frac {TP+TN} {N}</script><script type="math/tex; mode=display">= \frac { \text{分类正确的样本个数} } { \text{分类的所有样本个数} }</script><h2 id="查全率-召回率-R"><a href="#查全率-召回率-R" class="headerlink" title="查全率/召回率 R"></a>查全率/召回率 R</h2><ul><li>第j个类别的查全率表示在本类样本中，被正确分类的样本所占的比例，它表示这个类别的分类精度</li></ul><script type="math/tex; mode=display">Recall_j = \frac {TP_j} {TP_j + FN_j} , 1 \leq j \leq m</script><h2 id="查准率-准确率-P"><a href="#查准率-准确率-P" class="headerlink" title="查准率/准确率 P"></a>查准率/准确率 P</h2><ul><li>第j个类别的查准率表示被分类为该类的样本中，真正属于该类的样本所占的比例，它表示这个类别的分类纯度</li></ul><script type="math/tex; mode=display">Precision_j = \frac {TP_j} {TP_j + FP_j} , 1 \leq j \leq m</script><h2 id="F1-标准"><a href="#F1-标准" class="headerlink" title="F1 标准"></a>F1 标准</h2><ul><li>F1 值比较合理地评价分类器对每一类样本的分类性能。</li></ul><script type="math/tex; mode=display">F_\beta = \frac {(1 + \beta^2) * P * R} {(\beta^2 * P) + R}</script><script type="math/tex; mode=display">F1 = \frac {2 * R_j * P_j} {R_j + P_j} , 1 \leq j \leq m, \beta = 1</script><script type="math/tex; mode=display">= \frac {2} {1/P + 1/R}</script><hr><h2 id="宏平均-Macro-averaging"><a href="#宏平均-Macro-averaging" class="headerlink" title="宏平均 Macro-averaging"></a>宏平均 Macro-averaging</h2><ul><li>先对每一个类统计指标值，然后在对所有类<strong>求算术平均值</strong></li></ul><script type="math/tex; mode=display">Macro\_P = \frac{1}{n}\sum_{i=1}^mP_i</script><script type="math/tex; mode=display">Macro\_R = \frac{1}{n}\sum_{i=1}^mR_i</script><script type="math/tex; mode=display">Macro\_F = \frac{1}{n}\sum_{i=1}^mF_i</script><script type="math/tex; mode=display">Macro\_F = \frac {2 * Macro\_P * Macro\_R} {Macro\_P + Macro\_R}</script><h2 id="微平均-Micro-averaging"><a href="#微平均-Micro-averaging" class="headerlink" title="微平均 Micro-averaging"></a>微平均 Micro-averaging</h2><ul><li>对数据集中的每一个实例不分类别进行统计建立全局混淆矩阵，然后计算相应指标</li></ul><script type="math/tex; mode=display">Micro\_P = \frac {\sum_{i=1}^mTP_i} {\sum_{i=1}^mTP_i + \sum_{i=1}^mFP_i}</script><script type="math/tex; mode=display">= \frac {\sum_{i=1}^mTP_i} {N}</script><script type="math/tex; mode=display">Micro\_R = \frac {\sum_{i=1}^mTP_i} {\sum_{i=1}^mTP_i + \sum_{i=1}^mFN_i}</script><script type="math/tex; mode=display">= \frac {\sum_{i=1}^mTP_i} {N}</script><script type="math/tex; mode=display">Micro\_F = \frac {2 * Micro\_P * Micro\_R} {Micro\_P + Micro\_R}</script><script type="math/tex; mode=display">\text{如果对所有类别求微平均，那么上面三个值是相等的，且 = accuracy。}</script><hr>]]></content>
    
    <summary type="html">
    
      
      
        &lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;http://www.cnblogs.com/robert-dlut/p/5276927.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;谈谈评价指标中的宏平均和微平均&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;ht
      
    
    </summary>
    
      <category term="笔记" scheme="http://www.zhuzongkui.top/categories/%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="统计学习" scheme="http://www.zhuzongkui.top/tags/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>特征选择方法</title>
    <link href="http://www.zhuzongkui.top/2018/10/06/feature_select/"/>
    <id>http://www.zhuzongkui.top/2018/10/06/feature_select/</id>
    <published>2018-10-06T10:24:38.000Z</published>
    <updated>2019-08-04T08:45:31.320Z</updated>
    
    <content type="html"><![CDATA[<ul><li>CHI、IG： <a href="http://songlj.iteye.com/blog/2241763" target="_blank" rel="noopener">http://songlj.iteye.com/blog/2241763</a></li><li>IG、CHI、TC、TS、IIG <a href="https://blog.csdn.net/Fighting_No1/article/details/51003386" target="_blank" rel="noopener">文本挖掘（四）——特征选择</a> </li><li><a href="https://blog.csdn.net/qq_16912257/article/details/52994788" target="_blank" rel="noopener">From My Github - 文本分类</a></li></ul><h3 id="表1：词频统计（文档数量）"><a href="#表1：词频统计（文档数量）" class="headerlink" title="表1：词频统计（文档数量）"></a>表1：词频统计（文档数量）</h3><p>其中，文档总数：N = A+B+C+D</p><div class="table-container"><table><thead><tr><th>-</th><th style="text-align:center">包含词条 t</th><th style="text-align:center">不包含词条 t</th></tr></thead><tbody><tr><td><strong>属于类别 c</strong></td><td style="text-align:center">A</td><td style="text-align:center">C</td></tr><tr><td><strong>不属于类别 c</strong></td><td style="text-align:center">B</td><td style="text-align:center">D </td></tr></tbody></table></div><hr><h2 id="1、文档频率-DF-（document-frequency）"><a href="#1、文档频率-DF-（document-frequency）" class="headerlink" title="1、文档频率 DF （document frequency）"></a>1、文档频率 DF （document frequency）</h2><ul><li>文档频率指训练集中包含该特征词条的文本总数</li></ul><script type="math/tex; mode=display">DF = A + B</script><p><code>选择 DF &gt; 某个阈值的特征词条</code></p><h2 id="2、信息增益-IG-（information-gain）"><a href="#2、信息增益-IG-（information-gain）" class="headerlink" title="2、信息增益 IG （information gain）"></a>2、信息增益 IG （information gain）</h2><ul><li>通过特征词在文本中出现和不出现前后的信息量之差来推断该特征词所带的信息量</li></ul><script type="math/tex; mode=display">IG(t) = H(c) - H(c|t)</script><script type="math/tex; mode=display">H(c) = - \sum_{i=1}^mP(c_i)log P(c_i)</script><script type="math/tex; mode=display">H(c|t) = - P(t) \sum_{i=1}^m P(c_i|t) log P(c_i|t) - P(\bar{t}) \sum_{i=1}^m P(c_i|\bar{t}) log P(c_i|\bar{t})</script><script type="math/tex; mode=display">P(t)=\frac {A+B}{N} \text{：表示样本集中包含词 t 的文本的概率}</script><script type="math/tex; mode=display">P(c_i)=\frac {A+C}{N} \text{：表示类文本在样本集中出现的概率}</script><script type="math/tex; mode=display">P(c_i|t)=\frac {A}{A+B} \text{：表示文本包含词 t 时属于 c 的条件概率}</script><h2 id="3、互信息-MI-（mutual-information）"><a href="#3、互信息-MI-（mutual-information）" class="headerlink" title="3、互信息 MI （mutual information）"></a>3、互信息 MI （mutual information）</h2><ul><li>互信息衡量了特征词条和类别之间的相关性<script type="math/tex; mode=display">MI(t,c) = log \frac {P(t,c)} {P(t)*P(c)} = log \frac {\frac{A}{N}} {\frac{A+B}{N}*\frac{A+C}{N}}</script><script type="math/tex; mode=display">= log \frac {A*N} {(A+C)*(A+B)} = log \frac { \frac{A}{A+C} } { \frac{A+B}{N} }</script><script type="math/tex; mode=display">= log \frac{A}{A+C} - log \frac{A+B}{N} = log P(t|c) - log P(t)</script><script type="math/tex; mode=display">MI_{avg}(t) = \sum_{i=1}^m P(c_i) MI(t,c_i)</script><script type="math/tex; mode=display">MI_{max}(t) = \max_{i=1}^m \{ MI(t,c_i) \}</script></li></ul><p>MI(t,c) = 0，当 t 和 c 相互独立时<br>弱点：得分被词条的边缘概率强烈的影响；（条件概率相等时，低频词比高频词有更高的分数）</p><h2 id="4、卡方统计-CHI（Chi-Square-Statistic）"><a href="#4、卡方统计-CHI（Chi-Square-Statistic）" class="headerlink" title="4、卡方统计 CHI（Chi-Square Statistic）"></a>4、卡方统计 CHI（Chi-Square Statistic）</h2><ul><li>卡方统计量也用于表征两个变量的相关性，与互信息相比，它同时考虑了特征在某类文本中出现和不出现时的情况</li><li>度量了 t 和 c 之间的独立性<script type="math/tex; mode=display">CHI(t,c) = \frac {N * (AD-BC)^2} {(A+C)*(B+D)*(A+B)*(C+D)}</script><script type="math/tex; mode=display">CHI_{avg}(t) = \sum_{i=1}^m P(c_i) CHI(t,c_i)</script><script type="math/tex; mode=display">CHI_{max}(t) = \max_{i=1}^m \{ CHI(t,c_i) \}</script></li><li>卡方统计是一个规范值，因此卡方统计值对于相同的类别可以跨词进行比较</li><li>如果列联表中的任何单元被轻微填充，这种归一化就会失效（低频词的例子）</li><li>因此，卡方统计对于低频词是不可靠的。</li></ul><h2 id="5、词条强度-单词权-TS-（term-strength）"><a href="#5、词条强度-单词权-TS-（term-strength）" class="headerlink" title="5、词条强度/单词权 TS （term strength）"></a>5、词条强度/单词权 TS （term strength）</h2><h3 id="法1：博客"><a href="#法1：博客" class="headerlink" title="法1：博客"></a>法1：<a href="https://blog.csdn.net/Fighting_No1/article/details/51003386" target="_blank" rel="noopener">博客</a></h3><ul><li>TS 计算的是一个词出现的条件概率，即该词在一对相关文本中的某一个文本中出现的条件下，在另一个文本中出现的概率<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">（1）计算文本数据集中每一对文本的相似度；</span><br><span class="line">（2）选择出所有相似度超过阈值的文本对；</span><br><span class="line">（3）对所有的单词，根据下式计算它的单词权。</span><br></pre></td></tr></table></figure></li></ul><script type="math/tex; mode=display">TS(t) = \frac {\text{均包含词t的相关文本对数}} {\text{文本集中的相关文本对总数}}</script><p>若有一个文本集，其中有N篇文本，M对相关文本有序对，有K对同时包含词t的相关文本有序对，则</p><script type="math/tex; mode=display">TS(t)=P(t|M) = \frac{K}{M} = \frac{\sum_{i=1}^m c_i\text{类包含词t相关文本对数}} {\sum_{i=1}^m c_i\text{类相关文本对数}}</script><script type="math/tex; mode=display">\approx \sum_{i=1}^m c_i\text{类包含词t相关文本对数}</script><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">即TS(t)表示在文本集的所有相关文本有序对的集合中，同时包含词t的相关文本有序对的比例。</span><br><span class="line">若TS(t)值越大，说明词t在相关文本集中出现得越多，即越重要。</span><br><span class="line">缺点：要计算文本间的相似度，所以复杂度较高；阈值不易确定。</span><br></pre></td></tr></table></figure><hr><h3 id="法2：（Yang-Yiming-论文里）文本聚类中的特征选择方法"><a href="#法2：（Yang-Yiming-论文里）文本聚类中的特征选择方法" class="headerlink" title="法2：（Yang Yiming 论文里）文本聚类中的特征选择方法"></a>法2：（Yang Yiming 论文里）<a href="http://www.docin.com/p-761617862.html" target="_blank" rel="noopener">文本聚类中的特征选择方法</a></h3><ul><li>这个方法基于词条出现在密切相关的文档中的频率来评估词条的重要性</li><li>使用一组训练文档来派生出文档对，其相似度（余弦值）高于某个阈值</li><li>x 和 y 是一对相似文档</li></ul><script type="math/tex; mode=display">TS(t) = P(t \in y | t \in x)</script><ul><li>基于文档聚类，假设有许多共享词的文档是相似的，在相关文档的重叠区域内的词条的信息量相对较大</li><li>这个方法不是基于特定任务的；不使用与词条类别相关的信息。</li></ul><hr><h3 id="法3：PPT-1-18页"><a href="#法3：PPT-1-18页" class="headerlink" title="法3：PPT-1 18页"></a>法3：<a href="https://wenku.baidu.com/view/d43cf16fb7360b4c2f3f643c.html" target="_blank" rel="noopener">PPT-1 18页</a></h3><ul><li><a href="https://wenku.baidu.com/view/2938f07f24c52cc58bd63186bceb19e8b8f6ec0d.html?re=view" target="_blank" rel="noopener">PPT-2 18页</a></li></ul><p>词强度（term strength）</p><script type="math/tex; mode=display">\text{已知一个词（特征）在某文档（实例）中出现，}</script><script type="math/tex; mode=display">\text{该词在同类（目标函数值相同）文档中出现的概率为词强度。}</script><script type="math/tex; mode=display">s(t) = P(t \in d_{Y=y}^i | t \in d_{Y=y}^j)</script>]]></content>
    
    <summary type="html">
    
      
      
        &lt;ul&gt;
&lt;li&gt;CHI、IG： &lt;a href=&quot;http://songlj.iteye.com/blog/2241763&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;http://songlj.iteye.com/blog/2241763&lt;/a&gt;&lt;/li&gt;
      
    
    </summary>
    
      <category term="笔记" scheme="http://www.zhuzongkui.top/categories/%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="统计学习" scheme="http://www.zhuzongkui.top/tags/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>StandfordNLP NLTK 中文工具配置教程</title>
    <link href="http://www.zhuzongkui.top/2018/09/10/StandfordNLP%20NLTK/"/>
    <id>http://www.zhuzongkui.top/2018/09/10/StandfordNLP NLTK/</id>
    <published>2018-09-10T07:13:05.000Z</published>
    <updated>2019-08-04T08:45:31.315Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Python3-NLTK-StandfordNLP-中文工具包配置教程"><a href="#Python3-NLTK-StandfordNLP-中文工具包配置教程" class="headerlink" title="Python3 NLTK StandfordNLP 中文工具包配置教程"></a>Python3 NLTK StandfordNLP 中文工具包配置教程</h1><ul><li><a href="https://www.cnblogs.com/baiboy/p/nltk1.html" target="_blank" rel="noopener">配置教程-1</a></li><li><a href="https://www.jianshu.com/p/4b3c7e7578e6" target="_blank" rel="noopener">配置教程-2</a></li></ul><h3 id="1-必要的安装包"><a href="#1-必要的安装包" class="headerlink" title="1. 必要的安装包"></a>1. 必要的安装包</h3><ul><li>Python3  <a href="https://www.anaconda.com/download/" target="_blank" rel="noopener">Anaconda3</a></li><li>NLTK <a href="https://www.anaconda.com/download/" target="_blank" rel="noopener">Anaconda3</a></li><li>jdk_1.8 <a href="http://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html" target="_blank" rel="noopener">Java SE Development Kit 8 Downloads</a></li><li>StanfordNLP <a href="https://nlp.stanford.edu/software/" target="_blank" rel="noopener">NLTK工具包</a></li><li>StanfordNLP 中文处理工具包 <a href="https://pan.baidu.com/s/1d5qTgpZgrgaA2LduSP6enw" target="_blank" rel="noopener">百度云链接</a> 密码：o1l3 【本人打包好】</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 中文处理工具包 stanford-jar.zip 文件内容: `ls`</span></span><br><span class="line">chinese.misc.distsim.crf.ser.gz  stanford-ner-3.9.1.jar</span><br><span class="line">chinese-distsim.tagger           stanford-parser.jar</span><br><span class="line">chinesePCFG.ser.gz               stanford-parser-3.9.1-models.jar</span><br><span class="line">data/                            stanford-postagger-3.9.1.jar</span><br><span class="line">slf4j-api-1.7.25.jar             stanford-segmenter-3.9.1.jar</span><br></pre></td></tr></table></figure><h3 id="2-调用代码"><a href="#2-调用代码" class="headerlink" title="2. 调用代码"></a>2. 调用代码</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> nltk</span><br><span class="line"></span><br><span class="line"><span class="comment">## java 环境</span></span><br><span class="line">java_path = <span class="string">"D:\Program Files\Java\jdk1.8.0_162\\bin\java.exe"</span>  <span class="comment"># java安装地址</span></span><br><span class="line">os.environ[<span class="string">'JAVAHOME'</span>] = java_path</span><br><span class="line">base_dir = <span class="string">'D:\data\stanfordnlp\stanford-jar'</span>  <span class="comment"># StanfordNLP 中文处理工具包的路径</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">## 1. 分词</span></span><br><span class="line"><span class="comment">#########################################################################################</span></span><br><span class="line"><span class="keyword">from</span> nltk.tokenize.stanford_segmenter <span class="keyword">import</span> StanfordSegmenter</span><br><span class="line">segmenter = StanfordSegmenter(</span><br><span class="line">    path_to_jar=os.path.join(base_dir, <span class="string">'stanford-segmenter-3.9.1.jar'</span>),</span><br><span class="line">    path_to_slf4j=os.path.join(base_dir, <span class="string">'slf4j-api-1.7.25.jar'</span>),</span><br><span class="line">    path_to_sihan_corpora_dict=os.path.join(base_dir, <span class="string">'data'</span>),</span><br><span class="line">    path_to_model=os.path.join(base_dir, <span class="string">'data/pku.gz'</span>),</span><br><span class="line">    path_to_dict=os.path.join(base_dir, <span class="string">'data/dict-chris6.ser.gz'</span>)</span><br><span class="line">)</span><br><span class="line">sent = <span class="string">'这是斯坦福中文分词器测试，南京市长江大桥。我在博客园开了一个博客，我的博客名叫伏草惟存，写了一些自然语言处理的文章。'</span></span><br><span class="line">print(segmenter.segment(sent))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">## 2. 命名实体识别NER</span></span><br><span class="line"><span class="comment">#########################################################################################</span></span><br><span class="line"><span class="keyword">from</span> nltk.tag <span class="keyword">import</span> StanfordNERTagger</span><br><span class="line">chi_tagger = StanfordNERTagger(</span><br><span class="line">    model_filename=os.path.join(base_dir, <span class="string">'chinese.misc.distsim.crf.ser.gz'</span>),</span><br><span class="line">    path_to_jar=os.path.join(base_dir, <span class="string">'stanford-ner-3.9.1.jar'</span>)</span><br><span class="line">    )</span><br><span class="line">result = <span class="string">'四川省 成都 信息 工程 大学 我 在 博客 园 开 了 一个 博客 ， 我 的 博客 名叫 伏 草 惟 存 ， 写 了 一些 自然语言 处理 的 文章 。\r\n'</span></span><br><span class="line"><span class="keyword">for</span> word, tag <span class="keyword">in</span>  chi_tagger.tag(result.split()):</span><br><span class="line">    print(word,tag)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">## 3. 词性标注</span></span><br><span class="line"><span class="comment">#########################################################################################</span></span><br><span class="line"><span class="keyword">from</span> nltk.tag <span class="keyword">import</span> StanfordPOSTagger</span><br><span class="line">chi_tagger = StanfordPOSTagger(</span><br><span class="line">    model_filename=os.path.join(base_dir, <span class="string">'chinese-distsim.tagger'</span>),</span><br><span class="line">    path_to_jar=os.path.join(base_dir, <span class="string">'stanford-postagger-3.9.1.jar'</span>)</span><br><span class="line">    )</span><br><span class="line">result = <span class="string">'四川省 成都 信息 工程 大学 我 在 博客 园 开 了 一个 博客 ， 我 的 博客 名叫 伏 草 惟 存 ， 写 了 一些 自然语言 处理 的 文章 。\r\n'</span></span><br><span class="line">print(chi_tagger.tag(result.split()))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">## 4. 句法分析</span></span><br><span class="line"><span class="comment">#########################################################################################</span></span><br><span class="line"><span class="keyword">from</span> nltk.parse.stanford <span class="keyword">import</span> StanfordParser</span><br><span class="line">chi_parser = StanfordParser(</span><br><span class="line">    os.path.join(base_dir, <span class="string">'stanford-parser.jar'</span>),</span><br><span class="line">    os.path.join(base_dir, <span class="string">'stanford-parser-3.9.1-models.jar'</span>),</span><br><span class="line">    os.path.join(base_dir, <span class="string">'chinesePCFG.ser.gz'</span>)</span><br><span class="line">    )</span><br><span class="line">sent = <span class="string">u'北海 已 成为 中国 对外开放 中 升起 的 一 颗 明星'</span></span><br><span class="line">print(list(chi_parser.parse(sent.split())))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">## 5. 依存句法分析</span></span><br><span class="line"><span class="comment">#########################################################################################</span></span><br><span class="line"><span class="keyword">from</span> nltk.parse.stanford <span class="keyword">import</span> StanfordDependencyParser</span><br><span class="line">chi_parser = StanfordDependencyParser(</span><br><span class="line">    os.path.join(base_dir, <span class="string">'stanford-parser.jar'</span>),</span><br><span class="line">    os.path.join(base_dir, <span class="string">'stanford-parser-3.9.1-models.jar'</span>),</span><br><span class="line">    os.path.join(base_dir, <span class="string">'chinesePCFG.ser.gz'</span>)</span><br><span class="line">    )</span><br><span class="line">res = list(chi_parser.parse(<span class="string">'四川 已 成为 中国 西部 对外开放 中 升起 的 一 颗 明星'</span>.split()))</span><br><span class="line"><span class="keyword">for</span> row <span class="keyword">in</span> res[<span class="number">0</span>].triples():</span><br><span class="line">    print(row)</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Python3-NLTK-StandfordNLP-中文工具包配置教程&quot;&gt;&lt;a href=&quot;#Python3-NLTK-StandfordNLP-中文工具包配置教程&quot; class=&quot;headerlink&quot; title=&quot;Python3 NLTK Standford
      
    
    </summary>
    
      <category term="笔记" scheme="http://www.zhuzongkui.top/categories/%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="NLP" scheme="http://www.zhuzongkui.top/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>简单使用 Git</title>
    <link href="http://www.zhuzongkui.top/2018/09/10/use_git/"/>
    <id>http://www.zhuzongkui.top/2018/09/10/use_git/</id>
    <published>2018-09-10T07:12:16.000Z</published>
    <updated>2019-08-04T08:45:31.325Z</updated>
    
    <content type="html"><![CDATA[<ul><li>Git简明指南：（可能需要翻墙）<a href="http://rogerdudler.github.io/git-guide/index.zh.html" target="_blank" rel="noopener">git - 简明指南</a></li><li>码云（Gitee.com）<a href="http://git.mydoc.io/" target="_blank" rel="noopener">帮助文档</a></li></ul><h1 id="【1】简单配置"><a href="#【1】简单配置" class="headerlink" title="【1】简单配置"></a>【1】简单配置</h1><p>1.配置用户信息<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ git config --global user.name &quot;你的用户名&quot;</span><br><span class="line">$ git config --global user.email &quot;你的邮箱地址&quot;</span><br></pre></td></tr></table></figure></p><p>2.查看配置信息<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ git config --list</span><br></pre></td></tr></table></figure></p><p>3.添加和提交<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ git add .</span><br><span class="line">$ git commit -m &quot;这一步操作的名字&quot;</span><br></pre></td></tr></table></figure></p><p>4.删除和提交<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ git rm &lt;filename&gt;</span><br><span class="line">$ git commit -m &quot;删除文件&quot;</span><br></pre></td></tr></table></figure></p><p>5.推送改动到远端<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ git push origin master</span><br></pre></td></tr></table></figure></p><p>6.远程服务器<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ git remote rename origin oschina  # 修改仓库名</span><br><span class="line">$ git remote add origin &lt;仓库地址&gt;   # 添加一个仓库</span><br><span class="line">$ git remote -v                     # 查看当前仓库对应的远程仓库地址</span><br><span class="line">$ git remote set-url origin &lt;仓库地址&gt;  # 修改仓库对应的远程仓库地址</span><br></pre></td></tr></table></figure></p><p>7.查看本地仓库的历史记录log<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ git log --author=你的用户名字</span><br></pre></td></tr></table></figure></p><p>8.可以查看改动记录<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git status</span><br></pre></td></tr></table></figure></p><h1 id="【2】简单clone项目到本地"><a href="#【2】简单clone项目到本地" class="headerlink" title="【2】简单clone项目到本地"></a>【2】简单clone项目到本地</h1><ol><li>下载一个和自己的操作系统相符的git。<a href="https://git-for-windows.github.io/" target="_blank" rel="noopener">下载链接</a></li><li>注册一个github账号。<a href="https://github.com/" target="_blank" rel="noopener">注册网址</a></li><li>将项目项目fork到自己的账户下。</li><li>将项目clone到本地<br>  先在本地新建一个文件夹：MyGit，打开文件，然后执行：<code>Git Bash Here</code><br>命令：<code>git init</code>，以创建新的git仓库。<br>命令：<code>git clone https://github.com/xxxx.git</code>， xxxx表示项目的名字。</li><li>如果下载的工程带有submodule<br>当使用<code>git clone</code>下来的工程中带有submodule时，初始的时候，<br>submodule的内容并不会自动下载下来的，此时只需执行如下命令：<br><code>git submodule update --init --recursive</code></li></ol><h1 id="【3】创建新项目，并推送到远程服务器（github-gitlab-gitee）"><a href="#【3】创建新项目，并推送到远程服务器（github-gitlab-gitee）" class="headerlink" title="【3】创建新项目，并推送到远程服务器（github, gitlab, gitee）"></a>【3】创建新项目，并推送到远程服务器（github, gitlab, gitee）</h1><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">git init</span><br><span class="line">git add README.md</span><br><span class="line">git commit -m <span class="string">"first commit"</span></span><br><span class="line">git remote add origin &lt;server&gt; <span class="comment"># 项目地址</span></span><br><span class="line">git push -u origin master</span><br><span class="line"><span class="comment"># 然后如果需要账号密码的话就输入账号密码，这样就完成了一次提交</span></span><br></pre></td></tr></table></figure><h1 id="【4】克隆远程服务器项目"><a href="#【4】克隆远程服务器项目" class="headerlink" title="【4】克隆远程服务器项目"></a>【4】克隆远程服务器项目</h1><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">clone</span> &lt;server&gt; <span class="comment"># 项目地址</span></span><br><span class="line">git pull origin master</span><br><span class="line">&lt;这里需要修改/添加文件，否则与原文件相比就没有变动&gt;</span><br><span class="line">git add .</span><br><span class="line">git commit -m <span class="string">"第一次提交"</span></span><br><span class="line">git push origin master</span><br><span class="line"><span class="comment"># 然后如果需要账号密码的话就输入账号密码，这样就完成了一次提交</span></span><br></pre></td></tr></table></figure><h1 id="【5】注意"><a href="#【5】注意" class="headerlink" title="【5】注意"></a>【5】注意</h1><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 按照本文档新建的项目时，在码云平台仓库上已经存在 readme 文件，故在提交时可能会存在冲突，这时您需要选择的是保留线上的文件或者舍弃线上的文件。</span></span><br><span class="line"><span class="comment"># 如果您舍弃线上的文件，则在推送时选择强制推送，强制推送需要执行下面的命令：</span></span><br><span class="line">git push origin master -f</span><br><span class="line"><span class="comment"># 如果您选择保留线上的 readme 文件,则需要先执行：</span></span><br><span class="line">git pull origin master</span><br><span class="line"><span class="comment"># 然后才可以推送</span></span><br></pre></td></tr></table></figure><h1 id="【6】配置ssh-key"><a href="#【6】配置ssh-key" class="headerlink" title="【6】配置ssh key"></a>【6】配置ssh key</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">ssh-keygen -t rsa -C &apos;xxx@xxx.com&apos;  # 然后一路回车(-C 参数是你的邮箱地址)</span><br><span class="line">打开文件 C:\Users\Administrator\.ssh\id_rsa.pub，复制内容</span><br><span class="line">打开 github --&gt; Settings --&gt; SSH and GPG keys --&gt; New SSH key</span><br><span class="line">把上一步中复制的内容粘贴到Key所对应的文本框，在Title对应的文本框中给这个sshkey设置一个名字，点击Add key按钮</span><br><span class="line">ssh -T git@github.com  # 验证一下</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;ul&gt;
&lt;li&gt;Git简明指南：（可能需要翻墙）&lt;a href=&quot;http://rogerdudler.github.io/git-guide/index.zh.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;git - 简明指南&lt;/a&gt;&lt;/li&gt;
&lt;
      
    
    </summary>
    
      <category term="笔记" scheme="http://www.zhuzongkui.top/categories/%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="Git" scheme="http://www.zhuzongkui.top/tags/Git/"/>
    
  </entry>
  
  <entry>
    <title>问题对语义相似度计算-参赛总结</title>
    <link href="http://www.zhuzongkui.top/2018/08/10/competition-summary/"/>
    <id>http://www.zhuzongkui.top/2018/08/10/competition-summary/</id>
    <published>2018-08-10T00:55:07.000Z</published>
    <updated>2019-08-04T08:45:31.318Z</updated>
    
    <content type="html"><![CDATA[<ul><li>时间段：2018.06.10~2018.07.20</li></ul><h1 id="问题对语义相似度计算（从0到0-5-）"><a href="#问题对语义相似度计算（从0到0-5-）" class="headerlink" title="问题对语义相似度计算（从0到0.5+）"></a>问题对语义相似度计算（从0到0.5+）</h1><ul><li>短短一个多月的时间，我学到了很多很多东西，从一个呆头小白初长成人。</li><li>首先，必须感谢我的导师能给我这个机会从头到尾完整地参加这次比赛，至始至终地为我们出谋划策，和我们探讨问题并答疑解惑，而且提供了各种宝贵的学习资料和服务器资源。</li><li>另外，也要特别感谢我的师兄一路无微不至的提点和帮助，和我一起找方法、看论文、搭模型、改代码，其实我们是从同一个起跑线开始的，到最后被师兄甩了好几条街 T_T。</li><li>虽然，比赛期间遇到了很多挫折，刚开始我们真的是一头雾水、无从下手，面对参加同样比赛的其他优秀选手（“老油条”）心里还是蛮慌的，好在勤能补拙，有团队配合，能够齐心协力、互相帮助，最终比赛的结果还算令人满意。</li></ul><h3 id="一、相关比赛"><a href="#一、相关比赛" class="headerlink" title="一、相关比赛"></a>一、相关比赛</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">任务：语句匹配问题、语义等价判别、语义等价判定、等价；（语句的意图匹配）</span><br><span class="line">输入：一个语句对</span><br><span class="line">输出：一个数值（0-1之间），表明该语句对的相似程度</span><br></pre></td></tr></table></figure><ul><li>第三届魔镜杯大赛 <a href="https://ai.ppdai.com/mirror/goToMirrorDetail?mirrorId=1" target="_blank" rel="noopener">问题相似度算法设计</a></li><li>2018 全国知识图谱与语义计算大会 <a href="http://www.ccks2018.cn/?page_id=16" target="_blank" rel="noopener">任务三：微众银行智能客服问句匹配大赛</a><ul><li>比赛平台：<a href="https://biendata.com/competition/CCKS2018_3/" target="_blank" rel="noopener">CCKS 2018 微众银行智能客服问句匹配大赛</a></li></ul></li><li>ATEC蚂蚁开发者大赛 <a href="https://dc.cloud.alipay.com/index#/topic/intro?id=3" target="_blank" rel="noopener">金融大脑-金融智能NLP服务</a>  <a href="https://blog.csdn.net/u014732537/article/details/81038260" target="_blank" rel="noopener">博客分享</a></li><li>… …</li></ul><h3 id="二、数据形式"><a href="#二、数据形式" class="headerlink" title="二、数据形式"></a>二、数据形式</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">魔镜杯：脱敏数据，所有原始文本信息都被编码成单字ID序列和词语ID序列。</span><br><span class="line">label,q1,q2</span><br><span class="line">1,Q397345,Q538594</span><br><span class="line">0,Q193805,Q699273</span><br><span class="line">0,Q085471,Q676160</span><br><span class="line">... ...</span><br><span class="line"></span><br><span class="line">CCKS：中文的真实客服语料。</span><br><span class="line">用微信都6年，微信没有微粒贷功能4。  号码来微粒贷0</span><br><span class="line">微信消费算吗还有多少钱没还0</span><br><span class="line">交易密码忘记了找回密码绑定的手机卡也掉了怎么最近安全老是要改密码呢好麻烦0</span><br><span class="line">... ...</span><br><span class="line"></span><br><span class="line">蚂蚁：蚂蚁金服金融大脑的实际应用场景。</span><br><span class="line">1怎么更改花呗手机号码我的花呗是以前的手机号码，怎么更改成现在的支付宝的号码手机号1</span><br><span class="line">2也开不了花呗，就这样了？完事了真的嘛？就是花呗付款0</span><br><span class="line">3花呗冻结以后还能开通吗我的条件可以开通花呗借款吗0</span><br><span class="line">... ...</span><br></pre></td></tr></table></figure><h3 id="三、解决方案"><a href="#三、解决方案" class="headerlink" title="三、解决方案"></a>三、解决方案</h3><h4 id="（1）问题分析"><a href="#（1）问题分析" class="headerlink" title="（1）问题分析"></a>（1）问题分析</h4><ul><li>预测问题对的相似程度，即判别问题对是属于类别1还是类别0，很明显这是一个NLP领域的分类问题，然而区别于传统的文本分类问题：</li></ul><div class="table-container"><table><thead><tr><th>区别</th><th>传统文本分类</th><th>问题对相似度计算</th></tr></thead><tbody><tr><td>输入</td><td>只有一个输入</td><td>有两个输入</td></tr><tr><td>句子长度</td><td>文本较长</td><td>句子长短不一、且较简短</td></tr><tr><td>特征</td><td>文本特征</td><td>语义特征</td></tr><tr><td>。。。</td><td>。。。</td><td>。。。</td></tr></tbody></table></div><h4 id="（2）数据分析"><a href="#（2）数据分析" class="headerlink" title="（2）数据分析"></a>（2）数据分析</h4><ul><li>1、正负样本比例接近于 1：1；</li><li>2、相似的句子之间一般都会含有公共词/字符；也会出现包含很多公共词/字符，但句子主语不一样导致两个句子不相似的情况；</li><li>3、比赛的数据是没有经过预处理的（去停用词、繁体转简体、清洗）；另外数据中也存在很多脏数据（标注有误、错别字、漏字、简写），也很容易导致分词错误；</li><li>4、预训练的词向量数据（除非比赛方提供，否则还需要跟领域相关的语料来进行训练）；</li></ul><h4 id="（3）分类模型"><a href="#（3）分类模型" class="headerlink" title="（3）分类模型"></a>（3）分类模型</h4><p>其实我们之前是没有接触过这种类型的比赛的，也没有很多参赛的经验，而是刚刚从零学起，一步一步地摸索，沿着前人的脚步再延伸。</p><ul><li>1、比赛方（魔镜杯）Demo：两个句子拼成一个文本，空格连接，以 tfidf 为特征，做逻辑回归；（研究官方Demo时发现代码里有bug：最后提交的是预测为0的概率，实际应该是1）</li><li>2、借鉴官方Demo，两个句子拼接，使用传统CNN做文本分类，准确率 80% 左右；经测试q1和q2两个句子分开单独处理后再合并做分类效果是明显好于q1和q2先合并再处理后做分类的；</li></ul><div class="table-container"><table><thead><tr><th>q1、q2 分开单独处理</th><th>共享卷积层</th><th>不共享卷积层</th></tr></thead><tbody><tr><td>log_loss</td><td>0.258995</td><td>0.28949</td></tr></tbody></table></div><ul><li>3、微软发表的一篇论文[1]：DSSM模型（把条目映射成低维向量、计算查询和文档的cosine相似度，即一个查询语句对应多个文档，所以这个模型不太适用这个比赛）<ul><li><a href="https://blog.csdn.net/zkq_1986/article/details/79128844" target="_blank" rel="noopener">DSSM深度结构化语义模型原理</a></li><li><a href="http://ju.outofmemory.cn/entry/316660" target="_blank" rel="noopener">深度语义匹配模型-DSSM 及其变种</a></li><li><a href="https://cloud.tencent.com/developer/article/1005600" target="_blank" rel="noopener">深度学习解决 NLP 问题：语义相似度计算</a></li><li><a href="https://mp.weixin.qq.com/s/nbT4GSUbgh-5d1J79IqeDA?spm=a2c4e.11153940.blogcont174908.5.7f1e74e1yYbgV4" target="_blank" rel="noopener">PaperWeekly 第37期 | 论文盘点：检索式问答系统的语义匹配模型（神经网络篇）</a></li></ul></li><li>4、Quora <a href="https://engineering.quora.com/Semantic-Question-Matching-with-Deep-Learning" target="_blank" rel="noopener">Semantic Question Matching with Deep Learning</a> 三个LSTM模型，可以作为以下介绍的模型的baseline，进本是基于LSTM和Attention展开。</li><li>5、<a href="https://nlp.stanford.edu/projects/snli/" target="_blank" rel="noopener">The Stanford Natural Language Inference (SNLI) Corpus</a> 一大堆模型及特征提取方法，很多都是用了模型融合。</li><li>6、<a href="https://github.com/faneshion/MatchZoo" target="_blank" rel="noopener">MatchZoo</a> 12个模型，可能这个工具包是针对QuoraQP数据已经调好了参数，在移植到我们这个比赛的时候效果不是很佳，但可以借鉴。</li><li>7、相关博客（句子对匹配方法）<ul><li><a href="https://blog.csdn.net/malefactor/article/details/50669741" target="_blank" rel="noopener">使用深度双向LSTM模型构造社区问答系统</a> 重现后，效果不是很好</li><li><a href="https://blog.csdn.net/mpk_no1/article/details/72618683" target="_blank" rel="noopener">深度学习笔记——基于双向RNN（LSTM、GRU）和Attention Model的句子对匹配方法</a> 4个模型，其中 Soft Attention Model 实现后效果较好。</li><li><a href="https://blog.csdn.net/diye2008/article/details/53762124?ref=myread" target="_blank" rel="noopener">CNN在NLP领域的应用（2） 文本语义相似度计算</a> 类似方法2</li><li><a href="https://www.jianshu.com/p/a649b568e8fa" target="_blank" rel="noopener">LSTM 句子相似度分析</a> LSTM 简化版</li></ul></li><li>8、Siamese Network 孪生网络<ul><li><a href="https://www.jianshu.com/p/92d7f6eaacf5" target="_blank" rel="noopener">Siamese network 孪生神经网络—一个简单神奇的结构</a>     </li><li><a href="https://blog.csdn.net/thriving_fcl/article/details/73730552" target="_blank" rel="noopener">用于文本相似的Siamese Network</a></li><li><a href="https://blog.csdn.net/sxf1061926959/article/details/54836696" target="_blank" rel="noopener">Siamese Network理解（附代码）</a></li><li><a href="https://blog.csdn.net/sinat_24143931/article/details/78919432" target="_blank" rel="noopener">Siamese Network原理</a></li></ul></li><li>9、ESIM （这个模型是所有模型里面实现后效果最好的，但也有改动，对于脱敏数据是不能实现TreeLSTM的）<ul><li>源代码：<a href="https://github.com/coetaur0/ESIM" target="_blank" rel="noopener">ESIM_keras</a></li><li>Kaggle Competition: Quora Question Pairs Problem  <a href="https://github.com/yuhsinliu1993/Quora_QuestionPairs_DL" target="_blank" rel="noopener">a single model - ESIM</a></li></ul></li><li>10、论文[2]里的4个模型：SSE、PWIM、DecAtt、ESIM<ul><li><a href="https://github.com/lanwuwei/SPM_toolkit" target="_blank" rel="noopener">论文开源代码</a> </li></ul></li><li>11、论文[3]：BiMPM模型 <a href="https://github.com/zhiguowang/BiMPM" target="_blank" rel="noopener">github源码</a><br>论文[4]：DR-BiLSTM模型</li><li>12、gensim 相似度查询<ul><li><a href="https://blog.csdn.net/questionfish/article/details/46746947" target="_blank" rel="noopener">Gensim官方教程翻译（四）——相似度查询（Similarity Queries）</a> </li><li><a href="https://blog.csdn.net/qq_19707521/article/details/79352455" target="_blank" rel="noopener">gensim 相似度查询（Similarity Queries）(三)</a></li><li><a href="https://blog.csdn.net/jdbc/article/details/49924665" target="_blank" rel="noopener">nltk-比较中文文档相似度-完整实例</a></li><li><a href="https://my.oschina.net/kakablue/blog/314513" target="_blank" rel="noopener">nltk-比较中文文档相似度-完整实例</a></li></ul></li><li>13、传统模型工具：xgboost(xgb)、lightgbm(lgb)、随机森林random forest(rf)、极端随机树 Extremely randomized trees(ET或Extra-Trees)</li><li>14、<a href="http://note.youdao.com/noteshare?id=e380c70f7ecd137a12c0bf5949fc0d03&amp;sub=7D5CCB14202F41FA965F57D6AAF16B18" target="_blank" rel="noopener">前10选手用到的模型</a></li></ul><blockquote><p>最终单模型的最好效果：log_loss = 0.205189</p></blockquote><p>比赛期间，我实现或者在实现的基础上改进前前后后大概搭建了20多个模型，其实很多模型都还有很大的提升空间，局限于比赛的时间和自己的知识能力，而且在模型的细微之处、参数的初始化以及调参方面自己都没有什么经验，以致自己实现的模型的效果都没有师兄的好 (；へ：)。<br>虽然我们没能进入拍拍贷“魔镜杯”比赛的决赛，但在导师的帮助和特殊关系下，我们也有幸了参加了 top10 选手精彩的决赛答辩(2018-7-24 09:00)，真的受益匪浅。</p><h4 id="（4）模型调参"><a href="#（4）模型调参" class="headerlink" title="（4）模型调参"></a>（4）模型调参</h4><ul><li>1、拍拍贷一同比赛的某位优秀选手（初赛第16名, 复赛第12名）分享的博客 <a href="https://www.jianshu.com/p/827dd447daf9?utm_campaign=hugo&amp;utm_medium=reader_share&amp;utm_content=note&amp;utm_source=qq" target="_blank" rel="noopener">智能客服问题相似度算法设计——第三届魔镜杯大赛第12名解决方案</a><ul><li>队伍：moka_tree 团队</li><li><a href="https://github.com/LittletreeZou/Question-Pairs-Matching" target="_blank" rel="noopener">代码分享</a></li></ul></li><li><p>2、其实，很多参数我自己设置的都是默认参数，具体没有做很多的微调：</p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">embedding_dim = 300         # 词向量的维度</span><br><span class="line">seq_length = 25             # 文本的最大长度</span><br><span class="line">filter_sizes = [3]          # 卷积核尺寸列表</span><br><span class="line">num_classes = 2             # 类别数</span><br><span class="line">  </span><br><span class="line">is_pre_train = True         # 是否为训练好的词向量</span><br><span class="line">is_trainable = True         # 动态/静态词向量</span><br><span class="line"></span><br><span class="line">num_filters = 300           # 卷积核数目</span><br><span class="line">rnn_num_layers = 2          # LSTM 隐藏层的个数</span><br><span class="line">attention_size = 300        # Attention 层的大小</span><br><span class="line">rnn_hidden_size = 300       # LSTM 隐藏层的大小</span><br><span class="line">dropout_keep_prob = 0.5     # dropout 保留比例</span><br><span class="line">learning_rate = 1e-3        # 学习率（设置自动衰减）</span><br><span class="line">batch_size = 128            # 每批训练的大小</span><br></pre></td></tr></table></figure></li><li><p>3、参数初始化：跟上面博客里分享的一样，TensorFlow里面参数初始化不同，对结果的影响非常大，师兄推荐也是使用 Xavier 初始化；原本想用keras再实现一遍的，一方面不太熟悉，另一方面由于时间紧迫未能完成。</p></li><li>4、决赛答辩里，我们了解到很多选手并没有使用官方给定词级和字符级的词向量（不知道训练方法、参数、模型等），都自己训练了两种词向量（word2vec、glove）；另外也有用 w_vector * w_tfidf 作为 w 的词向量。</li><li>5、重点关注字向量：由于中文分词难度较大，特别是不同领域内的领域分词没有很好的解决方案（比赛数据为金融领域数据源），而且实验的效果也表明词级别是好于字级别的。</li><li>6、BatchNormalization + Spatial Dropout 1D</li></ul><h4 id="（5）特征工程"><a href="#（5）特征工程" class="headerlink" title="（5）特征工程"></a>（5）特征工程</h4><p>1、人工设计特征这部分是我们团队中来也公司的几个小伙伴做的， 他们参考并设计了很多有趣的特征。</p><ul><li>统计特征：句长、公共词、fuzzywuzzy、stat_feature、cosine 欧式 明氏 切氏等距离、多项式 拉普拉斯 sigmod等核函数、重叠词、重叠字等特征；<ul><li><a href="https://www.kaggle.com/act444/lb-0-158-xgb-handcrafted-leaky" target="_blank" rel="noopener">XGB_handcrafted_leaky</a></li></ul></li><li>主题特征：powerful words、tfidf matrix、PCA、NMF、NLP feature；<ul><li><a href="https://github.com/abhishekkrthakur/is_that_a_duplicate_quora_question" target="_blank" rel="noopener">is_that_a_duplicate_quora_question</a></li></ul></li><li>图特征 <ul><li><a href="https://github.com/HouJP/kaggle-quora-question-pairs/blob/master/bin/feature_engineering/graph.py" target="_blank" rel="noopener">kaggle-quora-question-pairs</a></li></ul></li></ul><p>2、其他选手</p><ul><li>计算QA pair的几种相似度：编辑距离、马氏距离、闵可夫斯基距离、WMD</li><li>使用 SVD、NMF对句中词向量降维</li><li>根据共现图，统计节点的degree，得到了两个比较强的特征：coo_q1_q2_degree_diff（问题1和问题2的degree的差异）、coo_max_degree（问题对最大的degree，进一步离散化做1-hot又得到3个特征）</li><li>问题对公共邻居的数量/比例</li><li>第一名：提取问题出入度、pagerank等特征；问题出现的次数以及频繁程度特征；将所有已知的问题构建同义问题集。问题集的构建不参与训练，只用于数据增强；</li></ul><p>3、数据增强</p><ul><li>第一名的方法  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">假设 Q1 在所有样本里出现2次，分别是</span><br><span class="line">1，Q1，Q2</span><br><span class="line">1，Q3，Q1</span><br><span class="line">模型无法正确学习Q1与Q2/Q3的相同，而是会认为只要input里有Q1即为正样本。</span><br><span class="line">需要通过数据处理引导模型进行“比较”，而不是“拟合”。</span><br><span class="line">解决方案：通过构建一部分补充集（负例），对冲所有不平衡的问题。</span><br></pre></td></tr></table></figure></li></ul><p>4、后处理</p><ul><li>传递关系<ul><li>相似：（AB=1，AC=1）—&gt; BC=1</li><li>不相似：（AB=1，AC=0）—&gt; BC=0</li></ul></li><li>第一名的方法：infer机制：除了判断test集的每个样本得分以外，还会通过已知同义问题集的其他样本比对进行加权；融合时轻微降低得分过高的模型权重，补偿正样本过多的影响；将已知确认的样本修正为0/1。</li></ul><p>比别人差的一个重要原因：传递关系没有考虑到闭包！我们大概推了1253条，然而别人正例推了12568个样本，负例推了5129个样本。  ╥﹏╥</p><h4 id="（6）模型融合"><a href="#（6）模型融合" class="headerlink" title="（6）模型融合"></a>（6）模型融合</h4><ul><li>1、多模型的融合最常用的一个方法就是<strong>求平均</strong>，我使用这个方法后 logloss 有很大的提升（加权平均的几个结果都是线上提交后 logloss 在 0.205189~0.209739 之间）。</li></ul><div class="table-container"><table><thead><tr><th>求平均的数量</th><th>2</th><th>4</th><th>7</th><th>8</th><th>9</th></tr></thead><tbody><tr><td>线上提交 logloss</td><td>0.187845</td><td>0.185329</td><td>0.182613</td><td>0.179808</td><td>0.179063</td></tr></tbody></table></div><ul><li>2、同一个模型提升效果的常用方法就是<strong>多折交叉验证求平均</strong>，由于我们组内 GPU 服务器有限，这个就由模型效果比较好的师兄来完成了，而且提升也是非常明显的。</li><li>3、另外，也用了堆叠和混合（stacking与blending）。<ul><li>每个模型 word level（官方词向量）</li><li>每个模型 word level（word2vec）</li><li>每个模型 word level（glove）</li><li>每个模型 char level（官方字向量）</li><li>每个模型 char level（word2vec）</li><li>每个模型 char level（glove）</li></ul></li><li>4、<a href="https://blog.csdn.net/a358463121/article/details/53054686" target="_blank" rel="noopener">kaggle比赛集成指南</a></li><li>5、模型微调（Finetune）<ul><li>第一名的方法：gensim训练词向量；模型使用non_trainable词向量进行训练；将除了embedding的layer全部freeze，用低学习率finetune词向量层。</li></ul></li></ul><div class="table-container"><table><thead><tr><th>小 trick</th><th>贡献度</th></tr></thead><tbody><tr><td>多模型的预测结果求平均</td><td>logloss 降低 2.6 个百分点</td></tr><tr><td>同一个模型10折交叉验证</td><td>logloss 降低 2 个 百分点</td></tr><tr><td>传递关系推导</td><td>logloss 降低 3.1 个千分点 </td></tr></tbody></table></div><h3 id="四、比赛总结"><a href="#四、比赛总结" class="headerlink" title="四、比赛总结"></a>四、比赛总结</h3><ul><li>1、比赛成绩（logloss / F1）</li></ul><div class="table-container"><table><thead><tr><th>拍拍贷</th><th>初赛成绩（359只队伍）</th><th>复赛成绩（95只队伍）</th></tr></thead><tbody><tr><td>我们</td><td>0.166100（第22名）</td><td>0.162495（第21名）</td></tr><tr><td>moka_tree</td><td>0.163249（第16名）</td><td>0.151729（第12名）</td></tr><tr><td>SKY</td><td>0.141329（第1名）</td><td>0.142658（第1名）</td></tr></tbody></table></div><div class="table-container"><table><thead><tr><th>CCKS</th><th>初赛成绩（138只队伍）</th><th>复赛成绩（50只队伍）</th></tr></thead><tbody><tr><td>我们</td><td>0.85142（第24名）</td><td>0.84586（第4名）</td></tr><tr><td>ThunderUp</td><td>0.86485（第1名）</td><td>0.85131（第1名）</td></tr></tbody></table></div><ul><li>2、经验体会<ul><li>刚开始，我们都是尝试各种模型，不知道哪一个好，在这个上面花了不少时间，其实从平时就应该开始积累，关注最新研究、最新模型，多看一下论坛、kaggle、quora、github、和NLP相关的公众号等。</li><li>一定要从数据本身上做探索，研究各种特征，因为到比赛后期模型基本都相似了，很难再有更大的提升；从决赛答辩来看，前10的选手在数据特征上都下了非常大的功夫，比如图特征等。</li><li>一定要做交叉验证，求平均。比赛方提供的训练集如果只用了 0.9 的数据来训练模型，那么模型很大程度会丢失剩下的 0.1 的信息，如果做了交叉验证的话，就可以兼顾到所有训练集的特征信息。</li><li>从比赛角度讲，深度学习框架 keras 是好于 TensorFlow 的，因为 keras 一般在参数调试、参数初始化以及模型搭建上面都整合的非常好；从科研角度讲，Tensorflow 具有清晰的流程，可以帮助你更好的理解深度学习的完整过程。</li><li>到了比赛后期，多模型的融合一定会有帮助，因为这样可以结合不同的模型的优缺点；模型融合最简单的方法是就是求平均，再复杂点就是对不同的模型依据效果的好坏赋予不同的权重在加权求和。</li><li>之前一直很纳闷人工设计的传统特征是怎样可以和深度学习模型相结合的，通过这次比赛，我也学习到了很多传统的NLP模型（xgboost、lightgbm、随机森林、极端随机树等），设计的特征可以加入到最后一层MLP层进行训练。</li><li>一定要有团队配合，“三个臭皮匠，顶个诸葛亮”，“1+1&gt;2”，真的真的可以从别人身上学习到很多很多的东西。</li><li>一定要多看论文、多写代码，多请教师兄、导师，“纸上得来终觉浅，绝知此事要躬行。”，“冰冻三尺，非一日之寒。”，调参经验、模型的搭建很多都是来自平时的积累、练习。</li><li>作为一个小白，一定要比别人花更多的时间和努力，才能笨鸟先飞、勤能补拙。</li><li>再忙再累也要多运动、多锻炼，身体是革命的本钱，一定要爱惜身体，督促自己，实验室固然安逸，但整天坐着身体的机能肯定会下降，发际线正在颤抖。</li><li>“路漫漫其修远兮，我将上下而求索。”</li></ul></li></ul><h3 id="五、参考文献"><a href="#五、参考文献" class="headerlink" title="五、参考文献"></a>五、参考文献</h3><ul><li>[1] Huang P S, He X, Gao J, et al. Learning deep structured semantic models for web search using clickthrough data[C]// ACM International Conference on Conference on Information &amp; Knowledge Management. ACM, 2013:2333-2338.</li><li>[2] Lan W, Xu W. Neural Network Models for Paraphrase Identification, Semantic Textual Similarity, Natural Language Inference, and Question Answering[J]. 2018.</li><li>[3] Wang Z, Hamza W, Florian R. Bilateral Multi-Perspective Matching for Natural Language Sentences[J]. 2017.</li><li>[4] Ghaeini R, Hasan S A, Datla V, et al. DR-BiLSTM: Dependent Reading Bidirectional LSTM for Natural Language Inference[J]. 2018.</li></ul><h4 id="其他比赛选手总结"><a href="#其他比赛选手总结" class="headerlink" title="其他比赛选手总结"></a>其他比赛选手总结</h4><ul><li><a href="https://kexue.fm/archives/5743?from=timeline&amp;isappinstalled=0" target="_blank" rel="noopener">https://kexue.fm/archives/5743?from=timeline&amp;isappinstalled=0</a></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;ul&gt;
&lt;li&gt;时间段：2018.06.10~2018.07.20&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&quot;问题对语义相似度计算（从0到0-5-）&quot;&gt;&lt;a href=&quot;#问题对语义相似度计算（从0到0-5-）&quot; class=&quot;headerlink&quot; title=&quot;问题对语义相似度
      
    
    </summary>
    
      <category term="总结" scheme="http://www.zhuzongkui.top/categories/%E6%80%BB%E7%BB%93/"/>
    
    
      <category term="NLP" scheme="http://www.zhuzongkui.top/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="http://www.zhuzongkui.top/2018/08/10/hello-world/"/>
    <id>http://www.zhuzongkui.top/2018/08/10/hello-world/</id>
    <published>2018-08-09T16:00:00.000Z</published>
    <updated>2019-08-04T08:45:31.321Z</updated>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><h3 id="一条命令"><a href="#一条命令" class="headerlink" title="一条命令"></a>一条命令</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hexo clean &amp; hexo g &amp; hexo s # 默认端口号：4000</span><br><span class="line">hexo clean &amp; hexo g &amp; hexo server -p 5000 # 如果4000端口打不开，改用5000</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="noopener">Deployment</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Welcome to &lt;a href=&quot;https://hexo.io/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Hexo&lt;/a&gt;! This is your very first post. Check &lt;a href=&quot;https://hexo.
      
    
    </summary>
    
      <category term="笔记" scheme="http://www.zhuzongkui.top/categories/%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="Hexo" scheme="http://www.zhuzongkui.top/tags/Hexo/"/>
    
  </entry>
  
</feed>
