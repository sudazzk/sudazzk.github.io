<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[中文知识图谱问答 CCKS2019 CKBQA - 参赛总结]]></title>
    <url>%2F2019%2F08%2F04%2Fccks2019_ckbqa%2F</url>
    <content type="text"><![CDATA[[TOC] 调研时间：2019.06.05~2019.06.19 参赛时间：2019.06.28~2019.07.25 中文知识图谱问答（从0到0.6+） Chinese Knowledge Base Question Answering（CKBQA） 一、任务1、任务定义什么是知识库？ 一条条知识，而把大量的知识汇聚起来就成了知识库。 123ex1：奥巴马出生在火奴鲁鲁ex2：姚明是中国人ex3：谢霆锋的爸爸是谢贤 知识来源：维基百科、百度百科等百科全书 特点：非结构化的自然语言、不适合计算机去处理 三元组（triple）（为了方便计算机的处理和理解，需要更加形式化、简洁化的方式去表示知识） 1234- ex1：奥巴马出生在火奴鲁鲁 --&gt; (奥巴马，出生地，火奴鲁鲁)- （主语，谓语，宾语）subject predicate object- （实体，属性，属性值）entity attribute value- （头实体，关系，尾实体）head_entity relation tail_entity 进一步，把实体看作是结点，把关系看作是一条边，包含大量三元组的知识库就构成了一个庞大的知识图谱 什么是知识库问答？ 基于知识库问答（knowledge base question answering, KBQA） 即，给定自然语言问题，通过对问题进行语义理解和解析，进而利用知识库进行查询、推理得出答案。 按应用领域划分：开放领域（百科知识问答等）和特定领域（金融、医疗、宗教、客服等） 评价指标：召回率、精确率、F1值、MRR（平均倒数排序） Q\text{为问题集合，}A_i\text{为对第i个问题给出的答案集合，}G_i\text{为第i个问题的标注答案集合} Macro Precision = \frac{1}{|Q|} \sum_{i=1}^{|Q|}P_i，P_i = \frac {|A_i \cap G_i|} {|A_i|} Macro Recall = \frac{1}{|Q|} \sum_{i=1}^{|Q|}R_i，R_i = \frac {|A_i \cap G_i|} {|G_i|} Averaged F1 = \frac{1}{|Q|} \sum_{i=1}^{|Q|} \frac{2P_iR_i}{P_i+R_i} 两大关键技术 【实体链指】：将问句中的实体名字链接到知识库中特定的实体上，涉及到实体识别和实体消歧。 【关系抽取】：将问句中的实体关系抽取出来，涉及到词性标注、词法句法分析、关系分类等。 举个栗子：姚明的老婆是什么星座？ （姚明，妻子，叶莉）—&gt;（叶莉，星座，天蝎） 2、相关评测 简单问题：NLPCC 2015-2018 复杂问题：CCKS 2018-2019NLPCC2015 评测：Open Domain Question AnsweringNLPCC2016 评测：Open Domain Chinese Question AnsweringNLPCC2017 评测：Open Domain Question AnsweringNLPCC2018 评测：Open Domain Question AnsweringCCKS2018 评测 任务四：开放领域的中文问答任务，CCKS2018 COQA 比赛平台CCKS2019 评测 任务六：中文知识图谱问答，CCKS2019 CKBQA 比赛平台，PKU BASE的在线查询终端：gStore，SPARQL语法规则 3、相关数据训练集NLPCC 2016（14609条）123456&lt;question id=1&gt; 《机械设计基础》这本书的作者是谁？&lt;answer id=1&gt; 杨可桢，程光蕴，李仲生==================================================&lt;question id=2&gt; 《高等数学》是哪个出版社出版的？&lt;answer id=2&gt; 武汉大学出版社================================================== CCKS 2019 （2298条）1234567q1:莫妮卡·贝鲁奇的代表作？select ?x where &#123; &lt;莫妮卡·贝鲁奇&gt; &lt;代表作品&gt; ?x. &#125;&lt;西西里的美丽传说&gt;q2:《湖上草》是谁的诗？select ?x where &#123; ?x &lt;主要作品&gt; &lt;湖上草&gt;. &#125;&lt;柳如是_（明末&quot;秦淮八艳&quot;之一）&gt; 知识库NLPCC 2016 知识库网盘 三元组（43063796条） 12345空气干燥 ||| 别名 ||| 空气干燥空气干燥 ||| 中文名 ||| 空气干燥空气干燥 ||| 外文名 ||| air drying空气干燥 ||| 形式 ||| 两个空气干燥 ||| 作用 ||| 将空气中的水份去除 提及-实体（7623034条） 12345空气 干燥 ||| 空气干燥air drying ||| 空气干燥 氧化结膜干燥罗育德 ||| 罗育德鳞 ||| 鳞 公子鳞squama ||| 鳞 鳞片 CCKS 2019 知识库网盘，密码：hcu8 三元组（41009142条） 12345&lt;美国奥可斯（香港）国际控股集团&gt; &lt;公司名称&gt; &quot;美国奥可斯（香港）国际控股集团有限公司&quot; .&lt;美国奥可斯（香港）国际控股集团&gt; &lt;成立时间&gt; &quot;2007-06-28&quot; .&lt;美国奥可斯（香港）国际控股集团&gt; &lt;经营范围&gt; &lt;培训&gt; .&lt;美国奥可斯（香港）国际控股集团&gt; &lt;经营范围&gt; &lt;影视&gt; .&lt;美国奥可斯（香港）国际控股集团&gt; &lt;公司口号&gt; &quot;品牌立业，质量最好&quot; . 提及 + 实体 + order（13930118条） 12345献陵 献陵_（唐高祖李渊陵墓） 1献陵 明献陵 2献陵 献陵_（朝鲜太宗献陵） 3佛罗伦萨 佛罗伦萨_（意大利托斯卡纳大区首府） 1佛罗伦萨 佛罗伦萨足球俱乐部 2 实体 + 类型 + 值（25182628条） 12345&lt;美国奥可斯（香港）国际控股集团&gt; &lt;类型&gt; &lt;文学作品&gt; .&lt;美国奥可斯（香港）国际控股集团&gt; &lt;类型&gt; &lt;文化&gt; .&lt;寻美中国&gt; &lt;类型&gt; &lt;品牌&gt; .&lt;青春是我和你一杯酒的深&gt; &lt;类型&gt; &lt;文学作品&gt; .&lt;青春是我和你一杯酒的深&gt; &lt;类型&gt; &lt;网络小说&gt; . 4、相关工作 NLPCC2015 第1名评测论文 [1] NLPCC2016 第1-4名评测论文 [2-5] NLPCC2017 第1-2名评测论文 + 会议论文 [6-8] NLPCC2018 第1名评测论文 [9] CCKS2018 第1-3名评测论文 [10-12] 二、方法三、总结四、参考文献 [1] NLPCC2015 1st Ye Z, Jia Z, Yang Y, et al. Research on open domain question answering system[M]//Natural Language Processing and Chinese Computing. Springer, Cham, 2015: 527-540. [2] NLPCC2016 1st Lai Y, Lin Y, Chen J, et al. Open domain question answering system based on knowledge base[M]//Natural Language Understanding and Intelligent Applications. Springer, Cham, 2016: 722-733. [3] NLPCC2016 2nd Yang F, Gan L, Li A, et al. Combining deep learning with information retrieval for question answering[M]//Natural Language Understanding and Intelligent Applications. Springer, Cham, 2016: 917-925. [4] NLPCC2016 3rd Xie Z, Zeng Z, Zhou G, et al. Knowledge base question answering based on deep learning models[M]//Natural Language Understanding and Intelligent Applications. Springer, Cham, 2016: 300-311. [5] NLPCC2016 4th Wang L, Zhang Y, Liu T. A deep learning approach for question answering over knowledge base[M]//Natural Language Understanding and Intelligent Applications. Springer, Cham, 2016: 885-892. [6] NLPCC2017 1st Lai Y, Jia Y, Lin Y, et al. A Chinese question answering system for single-relation factoid questions[C]//National CCF Conference on Natural Language Processing and Chinese Computing. Springer, Cham, 2017: 124-135. [7] NLPCC2017 2nd Zhang H, Zhu M, Wang H. A Retrieval-Based Matching Approach to Open Domain Knowledge-Based Question Answering[C]//National CCF Conference on Natural Language Processing and Chinese Computing. Springer, Cham, 2017: 701-711. [8] NLPCC2017 会议 周博通, 孙承杰, 林磊, et al. 基于LSTM的大规模知识库自动问答[J]. 北京大学学报：自然科学版, 2018. [9] NLPCC2018 1st Ni H, Lin L, Xu G. A Relateness-Based Ranking Method for Knowledge-Based Question Answering[C]//CCF International Conference on Natural Language Processing and Chinese Computing. Springer, Cham, 2018: 393-400. [10] CCKS2018 1st A QA Search Algorithm based on the Fusion Integration of Text Similarity and Graph Computation [11] CCKS2018 2nd A Joint Model of Entity Linking and Predicate Recognition for Knowledge Base Question Answering [12] CCKS2018 3rd Semantic Parsing for Multiple-relation Chinese Question Answering 五、相关博客 基于知识图谱的问答系统入门之—NLPCC2016KBQA数据集 ，Github：bert-kbqa-NLPCC2017 基于BERT的KBQA探索，Github：KBQA-BERT CCKS2018 CKBQA 1st 方案 自由讨论 | KBQA从入门到放弃—入门篇 KBQA从入门到放弃 - Part 2 | 每周话题精选 #09 KBQA 知识库问答领域研究综述（未完待续。。） 基于知识库的问答：seq2seq模型实践 KBQA 个人总结 揭开知识库问答KB-QA的面纱0·导读篇 知识图谱问答总结 肖仰华 | 基于知识图谱的问答系统 基于知识图谱的问答系统(KBQA) 各类QA问答系统的总结与技术实现（持续更新） 知识图谱入门 (九)知识问答 KBQA: 基于开放域知识库上的QA系统 | 每周一起读]]></content>
      <categories>
        <category>总结</category>
      </categories>
      <tags>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[常用的排序算法]]></title>
    <url>%2F2019%2F04%2F17%2Fsort%2F</url>
    <content type="text"><![CDATA[一、复杂度 排序方式 平均T(n) 最坏T(n) 最好T(n) 空间复杂度 稳定性 插入排序 O(n^2) O(n^2) O(n) O(1) 稳定 冒泡排序 O(n^2) O(n^2) O(n) O(1) 稳定 选择排序 O(n^2) O(n^2) O(n^2) O(1) 不稳定 希尔排序 O(n^1.3) O(n^2) O(n) O(1) 不稳定 快速排序 O(nlogn) O(n^2) O(nlogn) O(logn) 不稳定 堆排序 O(nlogn) O(nlogn) O(nlogn) O(1) 不稳定 归并排序 O(nlogn) O(nlogn) O(nlogn) O(n) 稳定 基数排序 O(d(n+r)) O(d(n+r)) O(d(n+r)) O(n+rd) 稳定 二、代码实现1、直接插入排序 insert12345678# 将 L[i] 插入到已经有序的子序列 L[1 2 ... i-1] 中def InsertSort(arr): for i in range(1, len(arr)): j = i while j &gt; 0 and arr[j] &lt; arr[j-1]: arr[j], arr[j-1] = arr[j-1], arr[j] j -= 1 2、冒泡排序 bubble1234567# 对相邻的元素两两进行比较，顺序相反则进行交换，这样每一趟最大的元素浮到顶端def BubbleSort(arr): for i in range(len(arr)-1): for j in range(len(arr)-1-i): if arr[j] &gt; arr[j+1]: arr[j], arr[j+1] = arr[j+1], arr[j] 3、简单选择排序 select1234567891011# 第i趟排序从 L[i,i+1,...,n] 中选择关键字最小的元素与 L[i] 比较# 每一趟从待排序的数据元素中选择最小的元素作为首元素def SelectSort(arr): for i in range(len(arr)-1): minv = i for j in range(i+1, len(arr)): if arr[j] &lt; arr[minv]: minv = j if minv != i: arr[minv], arr[i] = arr[i], arr[minv] 4、希尔排序 shell（跳着插入排序）123456789101112距离为 di 的记录放在同一个组中，进行直接插入排序di = 4, 2, 1def ShellSort(arr): step = len(arr) / 2 while step &gt; 0: for i in range(step, len(arr)): # 插入排序 j = i while j &gt;= step and arr[j] &lt; arr[j-step]: arr[j], arr[j-step] = arr[j-step], arr[j] j -= step step = step / 2 5、快速排序 quick123456789101112131415161718192021222324252627282930待排序表 L[1,2,...,n] 中任取一个元素 pivot 作为基准或枢纽，将表划分成两部分，一部分小于 pivot，一部分大于或等于 pivotL[1,2,...,k-1] 和 L[k+1,...,n] , L[k] = pivot操作：以当前表中第一个元素作为枢纽，对表进行划分将表中比枢纽值大的元素向右移动，小的向左移动移动采用从两端往中间夹入的方式（可用于求n个元素中第k小的元素）def Partition(arr, begin, end): # 划分元素 pivot = arr[begin] # 选取第一个元素作为基准 left = begin + 1 right = end while True: while left &lt;= right and arr[left] &lt;= pivot: left += 1 while left &lt;= right and arr[right] &gt;= pivot: right -= 1 if left &lt; right: arr[left], arr[right] = arr[right], arr[left] else: break arr[begin], arr[right] = arr[right], pivot # 划分元素放到中间位置 return right # 返回划分元素的下标def QuickSort(arr, begin, end): if begin &lt; end: k = Partition(arr, begin, end) Partition(arr, begin, k-1) Partition(arr, k+1, end) 6、堆排序 heap sort12345678910111213141516171819202122232425大顶堆（完全二叉树）：子节点小于父节点建堆 A[0,2,...,n-1]，移除根节点，将A[0]与A[n-1]交换，做最大堆调整的递归运算，建堆 A[0,2,...,n-2]，将A[0]与A[n-2]交换，直到 A[0]与A[1]交换思想可用于求大量元素中最小的或最大的几个元素def MaxHeap_adjust(arr, low, high): tmp = arr[low] # 父节点 while 2 * low + 1 &lt;= high: child = 2 * low + 1 # 左子节点 if child &lt; high and arr[child] &lt; arr[child+1]: child += 1 if arr[child] &lt; tmp: break arr[low] = arr[child] low = child arr[low] = tmpdef MaxHeapSort(arr): n = len(arr) for i in range(n/2-1, -1, -1): # 从下往上调 MaxHeap_adjust(arr, i, n-1) for i in range(n-1, 0, -1): arr[i], arr[0] = arr[0], arr[i] # 最大值放最后面 MaxHeap_adjust(arr, 0, i-1) # 重新调整一下 7、二路归并排序 merge（分治）12345678910111213141516171819202122n 个记录看成是 n 个有序的子表，两两归并，得到 n/2 个有序表，再归并def Merge(left, right): i, j = 0, 0 res = [] # 缺点：需要辅助空间 while i &lt; len(left) and j &lt; len(right): if left[i] &lt;= right[j]: res.append(left[i]) i += 1 else: res.append(right[j]) j += 1 res += left[i:] if i &lt; len(left) else right[j:] return resdef MergeSort(arr): if len(arr) &lt;= 1: return arr middle = len(arr) / 2 left = MergeSort(arr[:middle]) right = MergeSort(arr[middle:]) return Merge(left, right) 8、基数排序 radix（桶排序）12对数字最高位优先和最低位优先进行排序例如：先按个位从小到大，再按十位从小到大，再按百位从小到大]]></content>
      <categories>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>数据结构</tag>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[安装 cuda、cudnn、nvidia 驱动]]></title>
    <url>%2F2019%2F04%2F17%2Finstall_cuda%2F</url>
    <content type="text"><![CDATA[〇、TensorFlow 与 cuda 的对应版本 官方链接：https://tensorflow.google.cn/install/source123456789101112131415版本 Python 版本 编译器 编译工具 cuDNN CUDAtensorflow_gpu-1.13.0 2.7、3.3-3.6 GCC 4.8 Bazel 0.19.2 7.4 10.0tensorflow_gpu-1.12.0 2.7、3.3-3.6 GCC 4.8 Bazel 0.15.0 7 9tensorflow_gpu-1.11.0 2.7、3.3-3.6 GCC 4.8 Bazel 0.15.0 7 9tensorflow_gpu-1.10.0 2.7、3.3-3.6 GCC 4.8 Bazel 0.15.0 7 9tensorflow_gpu-1.9.0 2.7、3.3-3.6 GCC 4.8 Bazel 0.11.0 7 9tensorflow_gpu-1.8.0 2.7、3.3-3.6 GCC 4.8 Bazel 0.10.0 7 9tensorflow_gpu-1.7.0 2.7、3.3-3.6 GCC 4.8 Bazel 0.9.0 7 9tensorflow_gpu-1.6.0 2.7、3.3-3.6 GCC 4.8 Bazel 0.9.0 7 9tensorflow_gpu-1.5.0 2.7、3.3-3.6 GCC 4.8 Bazel 0.8.0 7 9tensorflow_gpu-1.4.0 2.7、3.3-3.6 GCC 4.8 Bazel 0.5.4 6 8tensorflow_gpu-1.3.0 2.7、3.3-3.6 GCC 4.8 Bazel 0.4.5 6 8tensorflow_gpu-1.2.0 2.7、3.3-3.6 GCC 4.8 Bazel 0.4.5 5.1 8tensorflow_gpu-1.1.0 2.7、3.3-3.6 GCC 4.8 Bazel 0.4.2 5.1 8tensorflow_gpu-1.0.0 2.7、3.3-3.6 GCC 4.8 Bazel 0.4.2 5.1 8 一、查看版本1、查看 cuda 版本 cat /usr/local/cuda/version.txt nvcc -V 2、查看 cudnn 版本 cat /usr/local/cuda/include/cudnn.h | grep CUDNN_MAJOR -A 2 cat /usr/include/cudnn.h | grep CUDNN_MAJOR -A 2 3、查看显卡驱动版本 cat /proc/driver/nvidia/version nvidia-smi 二、安装1、安装cuda Ubuntu16.04+cuda9.0安装教程 下载链接：https://developer.nvidia.com/cuda-toolkit-archive 执行命令：sudo sh cuda_9.0.176_384.81_linux.run ，不要安装驱动 配置系统环境变量：sudo vim /etc/profile 或者用户环境变量：vim ~/.bashrc1234export PATH=/usr/local/cuda-9.0/bin:$PATHexport LD_LIBRARY_PATH=/usr/local/cuda-9.0/lib64$LD_LIBRARY_PATH# 系统环境变量可能要重启电脑 `sudo reboot` 2、安装nvidia驱动 列出所有可用的 NVIDIA 设备信息：nvidia-smi -L 查找适配自己电脑GPU的驱动：http://www.nvidia.cn/Download/index.aspx?lang=cn 解决nvidia升级驱动后版本匹配问题123456789sudo apt-get purge nvidia*sudo add-apt-repository ppa:graphics-drivers/ppasudo apt-get updatesudo apt-get install nvidia-384 nvidia-settingsreboot注：如果要是不行，可能需要在bios里将显卡先设置成CPU集卡验证1.输入nvidia-smi查看2.prime-select query查看当前选用的显卡 3、安装cudnn 下载链接：https://developer.nvidia.com/rdp/cudnn-archive 执行命令拷贝文件，后者用软连接 12345tar -zxvf cudnn-9.0-linux-x64-v7.1.tgzsudo cp cuda/include/cudnn.h /usr/local/cuda/include/ sudo cp cuda/lib64/libcudnn* /usr/local/cuda/lib64/ -d sudo chmod a+r /usr/local/cuda/include/cudnn.h sudo chmod a+r /usr/local/cuda/lib64/libcudnn* 配置系统环境变量：sudo vim /etc/profile 123export PATH=/usr/local/cuda/bin$&#123;PATH:+:$&#123;PATH&#125;&#125;export LD_LIBRARY_PATH=/usr/local/cuda/lib64$&#123;LD_LIBRARY_PATH:+:$&#123;LD_LIBRARY_PATH&#125;&#125;export CUDA_HOME=/usr/local/cuda 刷新环境变量：source /etc/profile 三、卸载卸载 cuda1234cd /usr/local/cuda/binsudo ./uninstall_cuda_8.0.plcd /usr/localsudo rm -rf cuda-8.0 四、相关链接官方教程cuda、cudnn、驱动版本查看及nvidia驱动、cuda安装及卸载ubuntu下卸载cuda8.0，和安装cuda9.0，cudnn7.0,tensorflow-gpu=1.8]]></content>
      <categories>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[评价准则]]></title>
    <url>%2F2018%2F10%2F07%2Fevaluation%2F</url>
    <content type="text"><![CDATA[谈谈评价指标中的宏平均和微平均 分类器的性能表现评价（混淆矩阵，准确率，召回率，F1,mAP、ROC曲线） python + sklearn ︱分类效果评估——acc、recall、F1、ROC、回归、距离 准确率(Accuracy), 精确率(Precision), 召回率(Recall)和F1-Measure 对于二分类问题，可将样例根据其真实类别和分类器预测类别划分为： 0 1 2 真实类别 预测类别 真正例 True Positive TP 正例 正例 假正例 False Positive FP 负例 正例 假负例 False Negative FN 正例 负例 真负例 True Negative TN 负例 负例 然后可以构建混淆矩阵（Confusion Matrix）如下表所示： - 预测 正例 预测 负例 真实 正例 TP FN 真实 负例 FP TN \text{测试集：} X_{test} = \{(x_i, y_i) | i = 1, 2, ..., N \}N \text{：表示测试集中的样本个数}x_i \text{：表示测试集中的数据样本}y_i \text{：表示数据样本的类别号}\text{假设要研究的分类问题含有 m 个类别，则} y_i \in \{c_1, c_2, ..., c_m \}\text{在分类问题中对于测试集的第j个类别，假设被正确分类的样本数量为} TP_j\text{被错误分类的样本数量为} FN_j\text{其它类别被错误分类为该类的样本数量为} FP_j 精确度Accuracy = \frac {\sum_{j=1}^mTP_j} {N} = \frac {TP+TN} {N}= \frac { \text{分类正确的样本个数} } { \text{分类的所有样本个数} }查全率/召回率 R 第j个类别的查全率表示在本类样本中，被正确分类的样本所占的比例，它表示这个类别的分类精度 Recall_j = \frac {TP_j} {TP_j + FN_j} , 1 \leq j \leq m查准率/准确率 P 第j个类别的查准率表示被分类为该类的样本中，真正属于该类的样本所占的比例，它表示这个类别的分类纯度 Precision_j = \frac {TP_j} {TP_j + FP_j} , 1 \leq j \leq mF1 标准 F1 值比较合理地评价分类器对每一类样本的分类性能。 F_\beta = \frac {(1 + \beta^2) * P * R} {(\beta^2 * P) + R}F1 = \frac {2 * R_j * P_j} {R_j + P_j} , 1 \leq j \leq m, \beta = 1= \frac {2} {1/P + 1/R} 宏平均 Macro-averaging 先对每一个类统计指标值，然后在对所有类求算术平均值 Macro\_P = \frac{1}{n}\sum_{i=1}^mP_iMacro\_R = \frac{1}{n}\sum_{i=1}^mR_iMacro\_F = \frac{1}{n}\sum_{i=1}^mF_iMacro\_F = \frac {2 * Macro\_P * Macro\_R} {Macro\_P + Macro\_R}微平均 Micro-averaging 对数据集中的每一个实例不分类别进行统计建立全局混淆矩阵，然后计算相应指标 Micro\_P = \frac {\sum_{i=1}^mTP_i} {\sum_{i=1}^mTP_i + \sum_{i=1}^mFP_i}= \frac {\sum_{i=1}^mTP_i} {N}Micro\_R = \frac {\sum_{i=1}^mTP_i} {\sum_{i=1}^mTP_i + \sum_{i=1}^mFN_i}= \frac {\sum_{i=1}^mTP_i} {N}Micro\_F = \frac {2 * Micro\_P * Micro\_R} {Micro\_P + Micro\_R}\text{如果对所有类别求微平均，那么上面三个值是相等的，且 = accuracy。}]]></content>
      <categories>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>统计学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[特征选择方法]]></title>
    <url>%2F2018%2F10%2F06%2Ffeature_select%2F</url>
    <content type="text"><![CDATA[CHI、IG： http://songlj.iteye.com/blog/2241763 IG、CHI、TC、TS、IIG 文本挖掘（四）——特征选择 From My Github - 文本分类 表1：词频统计（文档数量）其中，文档总数：N = A+B+C+D - 包含词条 t 不包含词条 t 属于类别 c A C 不属于类别 c B D 1、文档频率 DF （document frequency） 文档频率指训练集中包含该特征词条的文本总数 DF = A + B选择 DF &gt; 某个阈值的特征词条 2、信息增益 IG （information gain） 通过特征词在文本中出现和不出现前后的信息量之差来推断该特征词所带的信息量 IG(t) = H(c) - H(c|t)H(c) = - \sum_{i=1}^mP(c_i)log P(c_i)H(c|t) = - P(t) \sum_{i=1}^m P(c_i|t) log P(c_i|t) - P(\bar{t}) \sum_{i=1}^m P(c_i|\bar{t}) log P(c_i|\bar{t})P(t)=\frac {A+B}{N} \text{：表示样本集中包含词 t 的文本的概率}P(c_i)=\frac {A+C}{N} \text{：表示类文本在样本集中出现的概率}P(c_i|t)=\frac {A}{A+B} \text{：表示文本包含词 t 时属于 c 的条件概率}3、互信息 MI （mutual information） 互信息衡量了特征词条和类别之间的相关性MI(t,c) = log \frac {P(t,c)} {P(t)*P(c)} = log \frac {\frac{A}{N}} {\frac{A+B}{N}*\frac{A+C}{N}}= log \frac {A*N} {(A+C)*(A+B)} = log \frac { \frac{A}{A+C} } { \frac{A+B}{N} }= log \frac{A}{A+C} - log \frac{A+B}{N} = log P(t|c) - log P(t)MI_{avg}(t) = \sum_{i=1}^m P(c_i) MI(t,c_i)MI_{max}(t) = \max_{i=1}^m \{ MI(t,c_i) \} MI(t,c) = 0，当 t 和 c 相互独立时弱点：得分被词条的边缘概率强烈的影响；（条件概率相等时，低频词比高频词有更高的分数） 4、卡方统计 CHI（Chi-Square Statistic） 卡方统计量也用于表征两个变量的相关性，与互信息相比，它同时考虑了特征在某类文本中出现和不出现时的情况 度量了 t 和 c 之间的独立性CHI(t,c) = \frac {N * (AD-BC)^2} {(A+C)*(B+D)*(A+B)*(C+D)}CHI_{avg}(t) = \sum_{i=1}^m P(c_i) CHI(t,c_i)CHI_{max}(t) = \max_{i=1}^m \{ CHI(t,c_i) \} 卡方统计是一个规范值，因此卡方统计值对于相同的类别可以跨词进行比较 如果列联表中的任何单元被轻微填充，这种归一化就会失效（低频词的例子） 因此，卡方统计对于低频词是不可靠的。 5、词条强度/单词权 TS （term strength）法1：博客 TS 计算的是一个词出现的条件概率，即该词在一对相关文本中的某一个文本中出现的条件下，在另一个文本中出现的概率123（1）计算文本数据集中每一对文本的相似度；（2）选择出所有相似度超过阈值的文本对；（3）对所有的单词，根据下式计算它的单词权。 TS(t) = \frac {\text{均包含词t的相关文本对数}} {\text{文本集中的相关文本对总数}}若有一个文本集，其中有N篇文本，M对相关文本有序对，有K对同时包含词t的相关文本有序对，则 TS(t)=P(t|M) = \frac{K}{M} = \frac{\sum_{i=1}^m c_i\text{类包含词t相关文本对数}} {\sum_{i=1}^m c_i\text{类相关文本对数}}\approx \sum_{i=1}^m c_i\text{类包含词t相关文本对数}123即TS(t)表示在文本集的所有相关文本有序对的集合中，同时包含词t的相关文本有序对的比例。若TS(t)值越大，说明词t在相关文本集中出现得越多，即越重要。缺点：要计算文本间的相似度，所以复杂度较高；阈值不易确定。 法2：（Yang Yiming 论文里）文本聚类中的特征选择方法 这个方法基于词条出现在密切相关的文档中的频率来评估词条的重要性 使用一组训练文档来派生出文档对，其相似度（余弦值）高于某个阈值 x 和 y 是一对相似文档 TS(t) = P(t \in y | t \in x) 基于文档聚类，假设有许多共享词的文档是相似的，在相关文档的重叠区域内的词条的信息量相对较大 这个方法不是基于特定任务的；不使用与词条类别相关的信息。 法3：PPT-1 18页 PPT-2 18页 词强度（term strength） \text{已知一个词（特征）在某文档（实例）中出现，}\text{该词在同类（目标函数值相同）文档中出现的概率为词强度。}s(t) = P(t \in d_{Y=y}^i | t \in d_{Y=y}^j)]]></content>
      <categories>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>统计学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[StandfordNLP NLTK 中文工具配置教程]]></title>
    <url>%2F2018%2F09%2F10%2FStandfordNLP%20NLTK%2F</url>
    <content type="text"><![CDATA[Python3 NLTK StandfordNLP 中文工具包配置教程 配置教程-1 配置教程-2 1. 必要的安装包 Python3 Anaconda3 NLTK Anaconda3 jdk_1.8 Java SE Development Kit 8 Downloads StanfordNLP NLTK工具包 StanfordNLP 中文处理工具包 百度云链接 密码：o1l3 【本人打包好】 123456# 中文处理工具包 stanford-jar.zip 文件内容: `ls`chinese.misc.distsim.crf.ser.gz stanford-ner-3.9.1.jarchinese-distsim.tagger stanford-parser.jarchinesePCFG.ser.gz stanford-parser-3.9.1-models.jardata/ stanford-postagger-3.9.1.jarslf4j-api-1.7.25.jar stanford-segmenter-3.9.1.jar 2. 调用代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869import osimport nltk## java 环境java_path = "D:\Program Files\Java\jdk1.8.0_162\\bin\java.exe" # java安装地址os.environ['JAVAHOME'] = java_pathbase_dir = 'D:\data\stanfordnlp\stanford-jar' # StanfordNLP 中文处理工具包的路径## 1. 分词#########################################################################################from nltk.tokenize.stanford_segmenter import StanfordSegmentersegmenter = StanfordSegmenter( path_to_jar=os.path.join(base_dir, 'stanford-segmenter-3.9.1.jar'), path_to_slf4j=os.path.join(base_dir, 'slf4j-api-1.7.25.jar'), path_to_sihan_corpora_dict=os.path.join(base_dir, 'data'), path_to_model=os.path.join(base_dir, 'data/pku.gz'), path_to_dict=os.path.join(base_dir, 'data/dict-chris6.ser.gz'))sent = '这是斯坦福中文分词器测试，南京市长江大桥。我在博客园开了一个博客，我的博客名叫伏草惟存，写了一些自然语言处理的文章。'print(segmenter.segment(sent))## 2. 命名实体识别NER#########################################################################################from nltk.tag import StanfordNERTaggerchi_tagger = StanfordNERTagger( model_filename=os.path.join(base_dir, 'chinese.misc.distsim.crf.ser.gz'), path_to_jar=os.path.join(base_dir, 'stanford-ner-3.9.1.jar') )result = '四川省 成都 信息 工程 大学 我 在 博客 园 开 了 一个 博客 ， 我 的 博客 名叫 伏 草 惟 存 ， 写 了 一些 自然语言 处理 的 文章 。\r\n'for word, tag in chi_tagger.tag(result.split()): print(word,tag)## 3. 词性标注#########################################################################################from nltk.tag import StanfordPOSTaggerchi_tagger = StanfordPOSTagger( model_filename=os.path.join(base_dir, 'chinese-distsim.tagger'), path_to_jar=os.path.join(base_dir, 'stanford-postagger-3.9.1.jar') )result = '四川省 成都 信息 工程 大学 我 在 博客 园 开 了 一个 博客 ， 我 的 博客 名叫 伏 草 惟 存 ， 写 了 一些 自然语言 处理 的 文章 。\r\n'print(chi_tagger.tag(result.split()))## 4. 句法分析#########################################################################################from nltk.parse.stanford import StanfordParserchi_parser = StanfordParser( os.path.join(base_dir, 'stanford-parser.jar'), os.path.join(base_dir, 'stanford-parser-3.9.1-models.jar'), os.path.join(base_dir, 'chinesePCFG.ser.gz') )sent = u'北海 已 成为 中国 对外开放 中 升起 的 一 颗 明星'print(list(chi_parser.parse(sent.split())))## 5. 依存句法分析#########################################################################################from nltk.parse.stanford import StanfordDependencyParserchi_parser = StanfordDependencyParser( os.path.join(base_dir, 'stanford-parser.jar'), os.path.join(base_dir, 'stanford-parser-3.9.1-models.jar'), os.path.join(base_dir, 'chinesePCFG.ser.gz') )res = list(chi_parser.parse('四川 已 成为 中国 西部 对外开放 中 升起 的 一 颗 明星'.split()))for row in res[0].triples(): print(row)]]></content>
      <categories>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[简单使用 Git]]></title>
    <url>%2F2018%2F09%2F10%2Fuse_git%2F</url>
    <content type="text"><![CDATA[Git简明指南：（可能需要翻墙）git - 简明指南 码云（Gitee.com）帮助文档 【1】简单配置1.配置用户信息12$ git config --global user.name &quot;你的用户名&quot;$ git config --global user.email &quot;你的邮箱地址&quot; 2.查看配置信息1$ git config --list 3.添加和提交12$ git add .$ git commit -m &quot;这一步操作的名字&quot; 4.删除和提交12$ git rm &lt;filename&gt;$ git commit -m &quot;删除文件&quot; 5.推送改动到远端1$ git push origin master 6.远程服务器1234$ git remote rename origin oschina # 修改仓库名$ git remote add origin &lt;仓库地址&gt; # 添加一个仓库$ git remote -v # 查看当前仓库对应的远程仓库地址$ git remote set-url origin &lt;仓库地址&gt; # 修改仓库对应的远程仓库地址 7.查看本地仓库的历史记录log1$ git log --author=你的用户名字 8.可以查看改动记录1git status 【2】简单clone项目到本地 下载一个和自己的操作系统相符的git。下载链接 注册一个github账号。注册网址 将项目项目fork到自己的账户下。 将项目clone到本地 先在本地新建一个文件夹：MyGit，打开文件，然后执行：Git Bash Here命令：git init，以创建新的git仓库。命令：git clone https://github.com/xxxx.git， xxxx表示项目的名字。 如果下载的工程带有submodule当使用git clone下来的工程中带有submodule时，初始的时候，submodule的内容并不会自动下载下来的，此时只需执行如下命令：git submodule update --init --recursive 【3】创建新项目，并推送到远程服务器（github, gitlab, gitee）123456git initgit add README.mdgit commit -m "first commit"git remote add origin &lt;server&gt; # 项目地址git push -u origin master# 然后如果需要账号密码的话就输入账号密码，这样就完成了一次提交 【4】克隆远程服务器项目1234567git clone &lt;server&gt; # 项目地址git pull origin master&lt;这里需要修改/添加文件，否则与原文件相比就没有变动&gt;git add .git commit -m "第一次提交"git push origin master# 然后如果需要账号密码的话就输入账号密码，这样就完成了一次提交 【5】注意123456# 按照本文档新建的项目时，在码云平台仓库上已经存在 readme 文件，故在提交时可能会存在冲突，这时您需要选择的是保留线上的文件或者舍弃线上的文件。# 如果您舍弃线上的文件，则在推送时选择强制推送，强制推送需要执行下面的命令：git push origin master -f# 如果您选择保留线上的 readme 文件,则需要先执行：git pull origin master# 然后才可以推送 【6】配置ssh key12345ssh-keygen -t rsa -C &apos;xxx@xxx.com&apos; # 然后一路回车(-C 参数是你的邮箱地址)打开文件 C:\Users\Administrator\.ssh\id_rsa.pub，复制内容打开 github --&gt; Settings --&gt; SSH and GPG keys --&gt; New SSH key把上一步中复制的内容粘贴到Key所对应的文本框，在Title对应的文本框中给这个sshkey设置一个名字，点击Add key按钮ssh -T git@github.com # 验证一下]]></content>
      <categories>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>Git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[问题对语义相似度计算-参赛总结]]></title>
    <url>%2F2018%2F08%2F10%2Fcompetition-summary%2F</url>
    <content type="text"><![CDATA[时间段：2018.06.10~2018.07.20 问题对语义相似度计算（从0到0.5+） 短短一个多月的时间，我学到了很多很多东西，从一个呆头小白初长成人。 首先，必须感谢我的导师能给我这个机会从头到尾完整地参加这次比赛，至始至终地为我们出谋划策，和我们探讨问题并答疑解惑，而且提供了各种宝贵的学习资料和服务器资源。 另外，也要特别感谢我的师兄一路无微不至的提点和帮助，和我一起找方法、看论文、搭模型、改代码，其实我们是从同一个起跑线开始的，到最后被师兄甩了好几条街 T_T。 虽然，比赛期间遇到了很多挫折，刚开始我们真的是一头雾水、无从下手，面对参加同样比赛的其他优秀选手（“老油条”）心里还是蛮慌的，好在勤能补拙，有团队配合，能够齐心协力、互相帮助，最终比赛的结果还算令人满意。 一、相关比赛123任务：语句匹配问题、语义等价判别、语义等价判定、等价；（语句的意图匹配）输入：一个语句对输出：一个数值（0-1之间），表明该语句对的相似程度 第三届魔镜杯大赛 问题相似度算法设计 2018 全国知识图谱与语义计算大会 任务三：微众银行智能客服问句匹配大赛 比赛平台：CCKS 2018 微众银行智能客服问句匹配大赛 ATEC蚂蚁开发者大赛 金融大脑-金融智能NLP服务 博客分享 … … 二、数据形式123456789101112131415161718魔镜杯：脱敏数据，所有原始文本信息都被编码成单字ID序列和词语ID序列。label,q1,q21,Q397345,Q5385940,Q193805,Q6992730,Q085471,Q676160... ...CCKS：中文的真实客服语料。用微信都6年，微信没有微粒贷功能 4。 号码来微粒贷 0微信消费算吗 还有多少钱没还 0交易密码忘记了找回密码绑定的手机卡也掉了 怎么最近安全老是要改密码呢好麻烦 0... ...蚂蚁：蚂蚁金服金融大脑的实际应用场景。1 怎么更改花呗手机号码 我的花呗是以前的手机号码，怎么更改成现在的支付宝的号码手机号 12 也开不了花呗，就这样了？完事了 真的嘛？就是花呗付款 03 花呗冻结以后还能开通吗 我的条件可以开通花呗借款吗 0... ... 三、解决方案（1）问题分析 预测问题对的相似程度，即判别问题对是属于类别1还是类别0，很明显这是一个NLP领域的分类问题，然而区别于传统的文本分类问题： 区别 传统文本分类 问题对相似度计算 输入 只有一个输入 有两个输入 句子长度 文本较长 句子长短不一、且较简短 特征 文本特征 语义特征 。。。 。。。 。。。 （2）数据分析 1、正负样本比例接近于 1：1； 2、相似的句子之间一般都会含有公共词/字符；也会出现包含很多公共词/字符，但句子主语不一样导致两个句子不相似的情况； 3、比赛的数据是没有经过预处理的（去停用词、繁体转简体、清洗）；另外数据中也存在很多脏数据（标注有误、错别字、漏字、简写），也很容易导致分词错误； 4、预训练的词向量数据（除非比赛方提供，否则还需要跟领域相关的语料来进行训练）； （3）分类模型其实我们之前是没有接触过这种类型的比赛的，也没有很多参赛的经验，而是刚刚从零学起，一步一步地摸索，沿着前人的脚步再延伸。 1、比赛方（魔镜杯）Demo：两个句子拼成一个文本，空格连接，以 tfidf 为特征，做逻辑回归；（研究官方Demo时发现代码里有bug：最后提交的是预测为0的概率，实际应该是1） 2、借鉴官方Demo，两个句子拼接，使用传统CNN做文本分类，准确率 80% 左右；经测试q1和q2两个句子分开单独处理后再合并做分类效果是明显好于q1和q2先合并再处理后做分类的； q1、q2 分开单独处理 共享卷积层 不共享卷积层 log_loss 0.258995 0.28949 3、微软发表的一篇论文[1]：DSSM模型（把条目映射成低维向量、计算查询和文档的cosine相似度，即一个查询语句对应多个文档，所以这个模型不太适用这个比赛） DSSM深度结构化语义模型原理 深度语义匹配模型-DSSM 及其变种 深度学习解决 NLP 问题：语义相似度计算 PaperWeekly 第37期 | 论文盘点：检索式问答系统的语义匹配模型（神经网络篇） 4、Quora Semantic Question Matching with Deep Learning 三个LSTM模型，可以作为以下介绍的模型的baseline，进本是基于LSTM和Attention展开。 5、The Stanford Natural Language Inference (SNLI) Corpus 一大堆模型及特征提取方法，很多都是用了模型融合。 6、MatchZoo 12个模型，可能这个工具包是针对QuoraQP数据已经调好了参数，在移植到我们这个比赛的时候效果不是很佳，但可以借鉴。 7、相关博客（句子对匹配方法） 使用深度双向LSTM模型构造社区问答系统 重现后，效果不是很好 深度学习笔记——基于双向RNN（LSTM、GRU）和Attention Model的句子对匹配方法 4个模型，其中 Soft Attention Model 实现后效果较好。 CNN在NLP领域的应用（2） 文本语义相似度计算 类似方法2 LSTM 句子相似度分析 LSTM 简化版 8、Siamese Network 孪生网络 Siamese network 孪生神经网络—一个简单神奇的结构 用于文本相似的Siamese Network Siamese Network理解（附代码） Siamese Network原理 9、ESIM （这个模型是所有模型里面实现后效果最好的，但也有改动，对于脱敏数据是不能实现TreeLSTM的） 源代码：ESIM_keras Kaggle Competition: Quora Question Pairs Problem a single model - ESIM 10、论文[2]里的4个模型：SSE、PWIM、DecAtt、ESIM 论文开源代码 11、论文[3]：BiMPM模型 github源码论文[4]：DR-BiLSTM模型 12、gensim 相似度查询 Gensim官方教程翻译（四）——相似度查询（Similarity Queries） gensim 相似度查询（Similarity Queries）(三) nltk-比较中文文档相似度-完整实例 nltk-比较中文文档相似度-完整实例 13、传统模型工具：xgboost(xgb)、lightgbm(lgb)、随机森林random forest(rf)、极端随机树 Extremely randomized trees(ET或Extra-Trees) 14、前10选手用到的模型 最终单模型的最好效果：log_loss = 0.205189 比赛期间，我实现或者在实现的基础上改进前前后后大概搭建了20多个模型，其实很多模型都还有很大的提升空间，局限于比赛的时间和自己的知识能力，而且在模型的细微之处、参数的初始化以及调参方面自己都没有什么经验，以致自己实现的模型的效果都没有师兄的好 (；へ：)。虽然我们没能进入拍拍贷“魔镜杯”比赛的决赛，但在导师的帮助和特殊关系下，我们也有幸了参加了 top10 选手精彩的决赛答辩(2018-7-24 09:00)，真的受益匪浅。 （4）模型调参 1、拍拍贷一同比赛的某位优秀选手（初赛第16名, 复赛第12名）分享的博客 智能客服问题相似度算法设计——第三届魔镜杯大赛第12名解决方案 队伍：moka_tree 团队 代码分享 2、其实，很多参数我自己设置的都是默认参数，具体没有做很多的微调： 123456789101112131415embedding_dim = 300 # 词向量的维度seq_length = 25 # 文本的最大长度filter_sizes = [3] # 卷积核尺寸列表num_classes = 2 # 类别数 is_pre_train = True # 是否为训练好的词向量is_trainable = True # 动态/静态词向量 num_filters = 300 # 卷积核数目rnn_num_layers = 2 # LSTM 隐藏层的个数attention_size = 300 # Attention 层的大小rnn_hidden_size = 300 # LSTM 隐藏层的大小dropout_keep_prob = 0.5 # dropout 保留比例learning_rate = 1e-3 # 学习率（设置自动衰减）batch_size = 128 # 每批训练的大小 3、参数初始化：跟上面博客里分享的一样，TensorFlow里面参数初始化不同，对结果的影响非常大，师兄推荐也是使用 Xavier 初始化；原本想用keras再实现一遍的，一方面不太熟悉，另一方面由于时间紧迫未能完成。 4、决赛答辩里，我们了解到很多选手并没有使用官方给定词级和字符级的词向量（不知道训练方法、参数、模型等），都自己训练了两种词向量（word2vec、glove）；另外也有用 w_vector * w_tfidf 作为 w 的词向量。 5、重点关注字向量：由于中文分词难度较大，特别是不同领域内的领域分词没有很好的解决方案（比赛数据为金融领域数据源），而且实验的效果也表明词级别是好于字级别的。 6、BatchNormalization + Spatial Dropout 1D （5）特征工程1、人工设计特征这部分是我们团队中来也公司的几个小伙伴做的， 他们参考并设计了很多有趣的特征。 统计特征：句长、公共词、fuzzywuzzy、stat_feature、cosine 欧式 明氏 切氏等距离、多项式 拉普拉斯 sigmod等核函数、重叠词、重叠字等特征； XGB_handcrafted_leaky 主题特征：powerful words、tfidf matrix、PCA、NMF、NLP feature； is_that_a_duplicate_quora_question 图特征 kaggle-quora-question-pairs 2、其他选手 计算QA pair的几种相似度：编辑距离、马氏距离、闵可夫斯基距离、WMD 使用 SVD、NMF对句中词向量降维 根据共现图，统计节点的degree，得到了两个比较强的特征：coo_q1_q2_degree_diff（问题1和问题2的degree的差异）、coo_max_degree（问题对最大的degree，进一步离散化做1-hot又得到3个特征） 问题对公共邻居的数量/比例 第一名：提取问题出入度、pagerank等特征；问题出现的次数以及频繁程度特征；将所有已知的问题构建同义问题集。问题集的构建不参与训练，只用于数据增强； 3、数据增强 第一名的方法 123456假设 Q1 在所有样本里出现2次，分别是1，Q1，Q21，Q3，Q1模型无法正确学习Q1与Q2/Q3的相同，而是会认为只要input里有Q1即为正样本。需要通过数据处理引导模型进行“比较”，而不是“拟合”。解决方案：通过构建一部分补充集（负例），对冲所有不平衡的问题。 4、后处理 传递关系 相似：（AB=1，AC=1）—&gt; BC=1 不相似：（AB=1，AC=0）—&gt; BC=0 第一名的方法：infer机制：除了判断test集的每个样本得分以外，还会通过已知同义问题集的其他样本比对进行加权；融合时轻微降低得分过高的模型权重，补偿正样本过多的影响；将已知确认的样本修正为0/1。 比别人差的一个重要原因：传递关系没有考虑到闭包！我们大概推了1253条，然而别人正例推了12568个样本，负例推了5129个样本。 ╥﹏╥ （6）模型融合 1、多模型的融合最常用的一个方法就是求平均，我使用这个方法后 logloss 有很大的提升（加权平均的几个结果都是线上提交后 logloss 在 0.205189~0.209739 之间）。 求平均的数量 2 4 7 8 9 线上提交 logloss 0.187845 0.185329 0.182613 0.179808 0.179063 2、同一个模型提升效果的常用方法就是多折交叉验证求平均，由于我们组内 GPU 服务器有限，这个就由模型效果比较好的师兄来完成了，而且提升也是非常明显的。 3、另外，也用了堆叠和混合（stacking与blending）。 每个模型 word level（官方词向量） 每个模型 word level（word2vec） 每个模型 word level（glove） 每个模型 char level（官方字向量） 每个模型 char level（word2vec） 每个模型 char level（glove） 4、kaggle比赛集成指南 5、模型微调（Finetune） 第一名的方法：gensim训练词向量；模型使用non_trainable词向量进行训练；将除了embedding的layer全部freeze，用低学习率finetune词向量层。 小 trick 贡献度 多模型的预测结果求平均 logloss 降低 2.6 个百分点 同一个模型10折交叉验证 logloss 降低 2 个 百分点 传递关系推导 logloss 降低 3.1 个千分点 四、比赛总结 1、比赛成绩（logloss / F1） 拍拍贷 初赛成绩（359只队伍） 复赛成绩（95只队伍） 我们 0.166100（第22名） 0.162495（第21名） moka_tree 0.163249（第16名） 0.151729（第12名） SKY 0.141329（第1名） 0.142658（第1名） CCKS 初赛成绩（138只队伍） 复赛成绩（50只队伍） 我们 0.85142（第24名） 0.84586（第4名） ThunderUp 0.86485（第1名） 0.85131（第1名） 2、经验体会 刚开始，我们都是尝试各种模型，不知道哪一个好，在这个上面花了不少时间，其实从平时就应该开始积累，关注最新研究、最新模型，多看一下论坛、kaggle、quora、github、和NLP相关的公众号等。 一定要从数据本身上做探索，研究各种特征，因为到比赛后期模型基本都相似了，很难再有更大的提升；从决赛答辩来看，前10的选手在数据特征上都下了非常大的功夫，比如图特征等。 一定要做交叉验证，求平均。比赛方提供的训练集如果只用了 0.9 的数据来训练模型，那么模型很大程度会丢失剩下的 0.1 的信息，如果做了交叉验证的话，就可以兼顾到所有训练集的特征信息。 从比赛角度讲，深度学习框架 keras 是好于 TensorFlow 的，因为 keras 一般在参数调试、参数初始化以及模型搭建上面都整合的非常好；从科研角度讲，Tensorflow 具有清晰的流程，可以帮助你更好的理解深度学习的完整过程。 到了比赛后期，多模型的融合一定会有帮助，因为这样可以结合不同的模型的优缺点；模型融合最简单的方法是就是求平均，再复杂点就是对不同的模型依据效果的好坏赋予不同的权重在加权求和。 之前一直很纳闷人工设计的传统特征是怎样可以和深度学习模型相结合的，通过这次比赛，我也学习到了很多传统的NLP模型（xgboost、lightgbm、随机森林、极端随机树等），设计的特征可以加入到最后一层MLP层进行训练。 一定要有团队配合，“三个臭皮匠，顶个诸葛亮”，“1+1&gt;2”，真的真的可以从别人身上学习到很多很多的东西。 一定要多看论文、多写代码，多请教师兄、导师，“纸上得来终觉浅，绝知此事要躬行。”，“冰冻三尺，非一日之寒。”，调参经验、模型的搭建很多都是来自平时的积累、练习。 作为一个小白，一定要比别人花更多的时间和努力，才能笨鸟先飞、勤能补拙。 再忙再累也要多运动、多锻炼，身体是革命的本钱，一定要爱惜身体，督促自己，实验室固然安逸，但整天坐着身体的机能肯定会下降，发际线正在颤抖。 “路漫漫其修远兮，我将上下而求索。” 五、参考文献 [1] Huang P S, He X, Gao J, et al. Learning deep structured semantic models for web search using clickthrough data[C]// ACM International Conference on Conference on Information &amp; Knowledge Management. ACM, 2013:2333-2338. [2] Lan W, Xu W. Neural Network Models for Paraphrase Identification, Semantic Textual Similarity, Natural Language Inference, and Question Answering[J]. 2018. [3] Wang Z, Hamza W, Florian R. Bilateral Multi-Perspective Matching for Natural Language Sentences[J]. 2017. [4] Ghaeini R, Hasan S A, Datla V, et al. DR-BiLSTM: Dependent Reading Bidirectional LSTM for Natural Language Inference[J]. 2018. 其他比赛选手总结 https://kexue.fm/archives/5743?from=timeline&amp;isappinstalled=0]]></content>
      <categories>
        <category>总结</category>
      </categories>
      <tags>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2018%2F08%2F10%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy 一条命令12hexo clean &amp; hexo g &amp; hexo s # 默认端口号：4000hexo clean &amp; hexo g &amp; hexo server -p 5000 # 如果4000端口打不开，改用5000 More info: Deployment]]></content>
      <categories>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
      </tags>
  </entry>
</search>
