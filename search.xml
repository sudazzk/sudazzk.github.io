<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[一些小配置]]></title>
    <url>%2F2019%2F08%2F11%2Fsmall_config%2F</url>
    <content type="text"><![CDATA[[TOC] Python pip 国内镜像临时使用： 可以在使用pip的时候在后面加上-i参数，指定pip源 12eg: pip install scrapy -i https://pypi.tuna.tsinghua.edu.cn/simple-i https://pypi.mirrors.ustc.edu.cn/simple 永久修改：123456789101112131415@ Linuxmkdir ~/.pipcd ~/.pipvim pip.conf@ WindowsC:\Users\你的用户名\pip\pip.ini例：C:\Users\Administrator\pip\pip.ini文件内容：[global]index-url = https://pypi.tuna.tsinghua.edu.cn/simple[install]trusted-host=pypi.tuna.tsinghua.edu.cn 深度学习框架指定GPU的id123456789CUDA_VISIBLE_DEVICES=1 python my_script.pyexport CUDA_VISIBLE_DEVICES=1import osos.environ[&quot;CUDA_VISIBLE_DEVICES&quot;] = &quot;0&quot;import torchtorch.cuda.set_device(id) Anaconda3 安装与卸载Linux1234567891011121314151617# 依次输入以下命令wget https://repo.continuum.io/archive/Anaconda3-5.1.0-Linux-x86_64.sh # 等待下载完成bash Anaconda3-5.1.0-Linux-x86_64.sh # 调用shell程序ENTER # 按回车键q # 输入q，不用按回车键yes # 输入yes，按回车键ENTER # 等待安装完成yes # 输入yes，添加环境变量到当前用户目录下# 下一步不用输入 yes，直接重新打开 Linux 终端，当前用户根的目录下会有一个 anaconda3/ 目录# 测试是否已安装好 Anaconda 3，输入 python 按回车键会显示如下信息：Python 3.6.4 |Anaconda, Inc.| (default, Jan 16 2018, 18:10:19)[GCC 7.2.0] on linuxType &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.&gt;&gt;&gt;# 再输入 exit()，按回车键退出 python 环境 1rm -rf ~/anaconda3 # 卸载anaconda Windows1234# 环境变量配置F:\Anaconda3 F:\Anaconda3\Scripts F:\Anaconda3\Library\bin Anaconda 虚拟环境教程：https://www.cnblogs.com/swje/p/7642929.html1234567891011121314151617181920211. 查看当前存在哪些虚拟环境conda env list 或 conda info -e2. 创建Python虚拟环境conda create -n zeronet python=3.63. 激活虚拟环境source activate zeronetLinux: source activate your_env_name(虚拟环境名称)Windows: activate your_env_name(虚拟环境名称)4. 对虚拟环境中安装额外的包conda install -n zeronet [package]5. 关闭虚拟环境source deactivate6. 删除虚拟环境conda remove -n your_env_name --all 123456789conda create -n tf_1.4 python=3.6conda activate tf_1.4python -m pip install tensorflow-gpu==1.4source deactivateconda create -n tf_1.9 python=3.6conda activate tf_1.9python -m pip install tensorflow-gpu==1.9source deactivate 12345conda create -n tf_0.12 python=3.5.2source activate tf_0.12conda listpython -m pip install tensorflow==0.12conda list jupyter中添加conda环境—-kernel配置1234conda create -n PY2 python=2.7source activate PY2conda install -n PY2 ipykernelpython -m ipykernel install --user --name PY2 --display-name &quot;PY2&quot; jupyter notebook 配置 Jupyter notebook远程访问服务器的方法 远程连接服务器jupyter notebook、浏览器以及深度学习可视化方法 远程访问jupyter notebook jupyter notebook启动出错解决方法 1234567891011121314151617jupyter notebook --generate-configpyhonfrom notebook.auth import passwdpasswd()Enter password: Verify password: Out[2]: &apos;sha1:ce23d945972f:34769685a7ccd3d08c84a18c63968a41f1140274&apos;vim ~/.jupyter/jupyter_notebook_config.pyc.NotebookApp.ip=&apos;*&apos;c.NotebookApp.password = u&apos;sha:ce...刚才复制的那个密文&apos;c.NotebookApp.open_browser = Falsec.NotebookApp.port =8888 #随便指定一个端口 最详尽使用指南：超快上手Jupyter Notebook 修改主题： https://github.com/dunovank/jupyter-themes 安装插件：https://github.com/ipython-contrib/jupyter_contrib_nbextensions 123python -m pip install jupyter_contrib_nbextensionsjupyter contrib nbextension install --userjupyter nbextension enable codefolding/main 配置插件：http://192.168.126.129:8888/nbextensions?nbextension=codefolding/main 1234567891011121314151617Collapsible headings放下/收起notebook的某些内容NotifyNotify功能就能在任务处理完后及时向你发送通知Codefolding折叠代码tqdm_notebook显示进度条%debug调试代码，直接跳到错误的地方Table of Contents自动生成目录]]></content>
      <categories>
        <category>教程</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Vim 基本配置]]></title>
    <url>%2F2019%2F08%2F11%2Fvim_base_config%2F</url>
    <content type="text"><![CDATA[教程 Vim初级：配置和使用 http://harttle.land/2013/11/08/vim-config.html Vim配置（python版）https://www.cnblogs.com/cjy15639731813/p/5886158.html 李老师的 vim 配置 主页 123456789101112131415161718192021222324252627282930313233343536373839set nocompatible &quot; 关闭 vi 兼容模式syntax on &quot; 自动语法高亮set number &quot; 显示行号set cursorline &quot; 突出显示当前行set ruler &quot; 打开状态栏标尺set shiftwidth=4 &quot; 设定 &lt;&lt; 和 &gt;&gt; 命令移动时的宽度为 4set softtabstop=4 &quot; 使得按退格键时可以一次删掉 4 个空格set tabstop=4 &quot; 设定 tab 长度为 4set nobackup &quot; 覆盖文件时不备份set autochdir &quot; 自动切换当前目录为当前文件所在的目录filetype plugin indent on &quot; 开启插件set backupcopy=yes &quot; 设置备份时的行为为覆盖set ignorecase smartcase &quot; 搜索时忽略大小写，但在有一个或以上大写字母时仍保持对大小写敏感set nowrapscan &quot; 禁止在搜索到文件两端时重新搜索set incsearch &quot; 输入搜索内容时就显示搜索结果set hlsearch &quot; 搜索时高亮显示被找到的文本set noerrorbells &quot; 关闭错误信息响铃set novisualbell &quot; 关闭使用可视响铃代替呼叫set t_vb= &quot; 置空错误铃声的终端代码&quot; set showmatch &quot; 插入括号时，短暂地跳转到匹配的对应括号&quot; set matchtime=2 &quot; 短暂跳转到匹配括号的时间set magic &quot; 设置魔术set hidden &quot; 允许在有未保存的修改时切换缓冲区，此时的修改由 vim 负责保存set guioptions-=T &quot; 隐藏工具栏set guioptions-=m &quot; 隐藏菜单栏set smartindent &quot; 开启新行时使用智能自动缩进set backspace=indent,eol,start&quot; 不设定在插入状态无法用退格键和 Delete 键删除回车符set cmdheight=1 &quot; 设定命令行的行数为 1set laststatus=2 &quot; 显示状态栏 (默认值为 1, 无法显示状态栏)set statusline=\ %&lt;%F[%1*%M%*%n%R%H]%=\ %y\ %0(%&#123;&amp;fileformat&#125;\ %&#123;&amp;encoding&#125;\ %c:%l/%L%)\ &quot; 设置在状态行显示的信息&quot;set foldenable &quot; 开始折叠&quot;set foldmethod=syntax &quot; 设置语法折叠set foldcolumn=0 &quot; 设置折叠区域的宽度setlocal foldlevel=1 &quot; 设置折叠层数为&quot; set foldclose=all &quot; 设置为自动关闭折叠 nnoremap &lt;space&gt; @=((foldclosed(line(&apos;.&apos;)) &lt; 0) ? &apos;zc&apos; : &apos;zo&apos;)&lt;CR&gt;&quot; 用空格键来开关折叠 python 注释12345678910111213多行注释： 1. 首先按esc进入命令行模式下，按下Ctrl + v，进入列（也叫区块）模式; 2. 在行首使用上下键选择需要注释的多行; 3. 按下键盘（大写）“I”键，进入插入模式； 4. 然后输入注释符（“//”、“#”等）; 5. 最后按下“Esc”键。注：在按下esc键后，会稍等一会才会出现注释，不要着急~~时间很短的 删除多行注释： 1. 首先按esc进入命令行模式下，按下Ctrl + v, 进入列模式; 2. 选定要取消注释的多行; 3. 按下“x”或者“d”.注意：如果是“//”注释，那需要执行两次该操作，如果是“#”注释，一次即可]]></content>
      <categories>
        <category>教程</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[NLP 基本任务]]></title>
    <url>%2F2019%2F08%2F11%2Fnlp_base_task%2F</url>
    <content type="text"><![CDATA[[TOC] NLP基本任务1、词法分析（Lexical Analysis）：对自然语言进行词汇层面的分析，是NLP基础性工作 分词（Word Segmentation/Tokenization）：对没有明显边界的文本进行切分，得到词序列 新词发现（New Words Identification）：找出文本中具有新形势、新意义或是新用法的词 形态分析（Morphological Analysis）：分析单词的形态组成，包括词干（Sterms）、词根（Roots）、词缀（Prefixes and Suffixes）等 词性标注（Part-of-speech Tagging）：确定文本中每个词的词性。词性包括动词（Verb）、名词（Noun）、代词（pronoun）等 拼写校正（Spelling Correction）：找出拼写错误的词并进行纠正 2、句子分析（Sentence Analysis）：对自然语言进行句子层面的分析，包括句法分析和其他句子级别的分析任务 组块分析（Chunking）：标出句子中的短语块，例如名词短语（NP），动词短语（VP）等 超级标签标注（Super Tagging）：给每个句子中的每个词标注上超级标签，超级标签是句法树中与该词相关的树形结构 成分句法分析（Constituency Parsing）：分析句子的成分，给出一棵树由终结符和非终结符构成的句法树 依存句法分析（Dependency Parsing）：分析句子中词与词之间的依存关系，给一棵由词语依存关系构成的依存句法树 语言模型（Language Modeling）：对给定的一个句子进行打分，该分数代表句子合理性（流畅度）的程度 语种识别（Language Identification）：给定一段文本，确定该文本属于哪个语种句子边界检测（Sentence Boundary Detection）：给没有明显句子边界的文本加边界 3、语义分析（Semantic Analysis）：对给定文本进行分析和理解，形成能勾够表达语义的形式化表示或分布式表示 词义消歧（Word Sense Disambiguation）：对有歧义的词，确定其准确的词义 语义角色标注（Semantic Role Labeling）：标注句子中的语义角色类标，语义角色，语义角色包括施事、受事、影响等 抽象语义表示分析（Abstract Meaning Representation Parsing）：AMR是一种抽象语义表示形式，AMR parser把句子解析成AMR结构 一阶谓词逻辑演算（First Order Predicate Calculus）：使用一阶谓词逻辑系统表达语义 框架语义分析（Frame Semantic Parsing）：根据框架语义学的观点，对句子进行语义分析 词汇/句子/段落的向量化表示（Word/Sentence/Paragraph Vector）：研究词汇、句子、段落的向量化方法，向量的性质和应用 4、信息抽取（Information Extraction）：从无结构文本中抽取结构化的信息 命名实体识别（Named Entity Recognition）：从文本中识别出命名实体，实体一般包括人名、地名、机构名、时间、日期、货币、百分比等 实体消歧（Entity Disambiguation）：确定实体指代的现实世界中的对象 术语抽取（Terminology/Giossary Extraction）：从文本中确定术语 共指消解（Coreference Resolution）：确定不同实体的等价描述，包括代词消解和名词消解 关系抽取（Relationship Extraction）：确定文本中两个实体之间的关系类型 事件抽取（Event Extraction）：从无结构的文本中抽取结构化事件 情感分析（Sentiment Analysis）：对文本的主观性情绪进行提取 意图识别（Intent Detection）：对话系统中的一个重要模块，对用户给定的对话内容进行分析，识别用户意图 槽位填充（Slot Filling）：对话系统中的一个重要模块，从对话内容中分析出于用户意图相关的有效信息 5、顶层任务（High-level Tasks）：直接面向普通用户，提供自然语言处理产品服务的系统级任务，会用到多个层面的自然语言处理技术 机器翻译（Machine Translation）：通过计算机自动化的把一种语言翻译成另外一种语言 文本摘要（Text summarization/Simplication）：对较长文本进行内容梗概的提取 问答系统（Question-Answering Systerm）：针对用户提出的问题，系统给出相应的答案 对话系统（Dialogue Systerm）：能够与用户进行聊天对话，从对话中捕获用户的意图，并分析执行 阅读理解（Reading Comprehension）：机器阅读完一篇文章后，给定一些文章相关问题，机器能够回答 自动文章分级（Automatic Essay Grading）：给定一篇文章，对文章的质量进行打分或分级]]></content>
      <categories>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[解压缩命令-使用教程]]></title>
    <url>%2F2019%2F08%2F11%2Fcompress_commit%2F</url>
    <content type="text"><![CDATA[解压缩命令 后缀：.zip, .tar, .gz, .bz2, .tar.gz, .tar.bz2, .rar Linux下文件的打包、解压缩指令——tar，gzip，bzip2，unzip，rar zip / unzip12解压：unzip filename.zip # 默认解压到当前目录，-d ./压缩：zip -r filename.zip dirname tar123456789-j：使用bzip2进行解压缩，一般使用.tar.bz2后缀-z：使用gzip进行解压缩，一般使用.tar.gz后缀-c：压缩，建立新的备份文件-x：解压，从备份文件中还原文件-v：显示指令执行过程-f：指定备份文件解包：tar zxvf filename.tar打包：tar zcvf filename.tar dirname gzip / gunzip123456解压1：gunzip filename.gz # 不保留原文件解压1：gunzip –c filename.gz &gt; filename # 保留原文件解压2：gzip -d filename.gz压缩1：gzip filename / gzip -r dirname # 不保留原文件压缩2：gzip –c filename &gt; filename.gz # 保留原文件 bzip2 / bunzip2123456解压1：bzip2 -d filename.bz2 # 不保留原文件解压1：bzip2 -d -k filename.bz2 # 保留原文件解压2：bunzip2 filename.bz2压缩：bzip2 filename # -k 保留原文件压缩：bzip2 -z filename # 强制压缩 rar12压缩：rar a -r experiment.rar ~/experiment/解压：rar x experment.rar ~/test/ # 将文件 experiment.rar 文件解压至指定的文件夹 Python读取压缩文件123456789import gzipfr = gzip.open(&apos;xxx.gz&apos;, &apos;r&apos;)line = fr.readline().decode(&apos;utf8&apos;)import bz2fr = bz2.BZ2File(&apos;xxx.bz2&apos;, &apos;r&apos;)import zipfilefr = zipfile.ZipFile(&quot;xxx.zip&quot;, &quot;r&quot;)]]></content>
      <categories>
        <category>教程</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL 安装配置教程（Windows、Linux）]]></title>
    <url>%2F2019%2F08%2F10%2FMySQL_Config%2F</url>
    <content type="text"><![CDATA[[TOC] MySQL 配置安装教程 下载链接 一、博客教程1、Windows 系统安装 Win10安装mysql-8.0.11-winx64详细步骤 win10下安装MySQL8小结 windows10+mysql8.0.11zip安装 2、Linux 系统安装 在线安装：ubuntu 安装Mysql 8.0 离线安装：ubuntu mysql8.0安装配置过程linux-generic(linux通用版本)、如何在Ubuntu Linux上安装 MySQL 8.0.11、Linux 离线安装mysql8.0 3、MySQL 连接和常用命令 Python3 MySQL 数据库连接 - PyMySQL 驱动 MySQL 教程 二、详细操作① Windows 1、mysql-8.0.17-winx64.zip 文件解压到D盘根目录，新建配置文件 my.ini，新建数据存储目录 Data/ 1234567891011121314151617181920212223242526272829## my.ini 文件内容[mysqld]# 设置3306端口port=3306# 设置mysql的安装目录basedir=D:\mysql-8.0.16-winx64# 设置mysql数据库的数据的存放目录datadir=D:\mysql-8.0.16-winx64\Data# 允许最大连接数max_connections=200# 允许连接失败的次数。这是为了防止有人从该主机试图攻击数据库系统max_connect_errors=10# 服务端使用的字符集默认为UTF8character-set-server=utf8mb4# 创建新表时将使用的默认存储引擎default-storage-engine=INNODB# 默认使用“mysql_native_password”插件认证default_authentication_plugin=mysql_native_password#开启查询缓存explicit_defaults_for_timestamp=true#skip-grant-tables[mysql]# 设置mysql客户端默认字符集default-character-set=utf8mb4[client]# 设置mysql客户端连接服务端时默认使用的端口port=3306default-character-set=utf8mb4# 随机初始密码：xxxxxxxxxxxxxx 2、配置系统环境变量：D:\mysql-8.0.16-winx64\bin 3、数据库初始化 1234mysqld --initialize --console # 会默认生成一个随机初始密码，临时保存一下mysqld --install # 安装net start mysql # 启动服务net stop mysql # 停止服务（不执行） 4、打开数据库 cmd 进入 1234567891011121314mysql -u root -p # 输入保存的初始密码ALTER USER 'root'@'localhost' IDENTIFIED WITH mysql_native_password BY '新密码'; # 修改数据库密码# 常用命令查看数据库信息：create database 库名;show databases;use 库名;show tables;desc 表名； 显示表的信息show variables like 'character%'; 显示编码select * from 表名;select count(*) from 表名;drop table 表名; # 删表 5、Python 访问数据库 123456789import pymysqlread_con = pymysql.connect(host="localhost", user='root', password='mysql', database='ccks2019', charset='utf8mb4')cur = read_con.cursor()sql = "show tables;" ## sql 命令cur.execute(sql)data = cur.fetchall() ## 获取执行的所有结果print(data)read_con.close()cur.close() 6、建立访客用户 教程 123create user 'guest'@'%' identified by '123456'; # 用户名guest，密码123456，%表示任意主机ipGRANT SELECT ON ccks2019.* TO 'guest'@'%'; # 授权只可以使用 select 权限mysql -uguest -p123456 -h[远程IP地址] -P3306 -Dccks2019 # 访问远程的 MySQL 数据库 ② Ubuntu 1、离线安装 12345678910111213141516tar -xvf mysql-8.0.12-linux-glibc2.12-x86_64.tar.xzmv mysql-8.0.12-linux-glibc2.12-x86_64/ mysqlmv mysql/ /usr/local/mysqlgroupadd mysqluseradd -r -g mysql -s /bin/false mysqlcd /usr/localcd mysqlmkdir mysql-filessudo chown mysql:mysql mysql-filessudo chmod 750 mysql-filesbin/mysqld --initialize --user=mysql# 如果报错，安装apt-cache search libaio apt-get install libaio1./support-files/mysql.server start 2、环境变量 123vim /etc/profileexport PATH="$PATH:/usr/local/mysql/bin"source /etc/profile 3、其他命令同 Windows 系统 step4]]></content>
      <categories>
        <category>教程</category>
      </categories>
      <tags>
        <tag>数据库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[中文知识图谱问答 CCKS2019 CKBQA - 参赛总结]]></title>
    <url>%2F2019%2F08%2F04%2Fccks2019_ckbqa%2F</url>
    <content type="text"><![CDATA[[TOC] 调研时间：2019.06.05~2019.06.19 参赛时间：2019.06.28~2019.07.25 中文知识图谱问答（从0到0.6+） Chinese Knowledge Base Question Answering（CKBQA） 一、任务1、任务定义什么是知识库？ 一条条知识，而把大量的知识汇聚起来就成了知识库。 123ex1：奥巴马出生在火奴鲁鲁ex2：姚明是中国人ex3：谢霆锋的爸爸是谢贤 知识来源：维基百科、百度百科等百科全书 特点：非结构化的自然语言、不适合计算机去处理 三元组（triple）（为了方便计算机的处理和理解，需要更加形式化、简洁化的方式去表示知识） 1234- ex1：奥巴马出生在火奴鲁鲁 --&gt; (奥巴马，出生地，火奴鲁鲁)- （主语，谓语，宾语）subject predicate object- （实体，属性，属性值）entity attribute value- （头实体，关系，尾实体）head_entity relation tail_entity 进一步，把实体看作是结点，把关系看作是一条边，包含大量三元组的知识库就构成了一个庞大的知识图谱 什么是知识库问答？ 基于知识库问答（knowledge base question answering, KBQA） 即，给定自然语言问题，通过对问题进行语义理解和解析，进而利用知识库进行查询、推理得出答案。 按应用领域划分：开放领域（百科知识问答等）和特定领域（金融、医疗、宗教、客服等） 评价指标：召回率、精确率、F1值、MRR（平均倒数排序） Q\text{为问题集合，}A_i\text{为对第i个问题给出的答案集合，}G_i\text{为第i个问题的标准答案集合} Macro Precision = \frac{1}{|Q|} \sum_{i=1}^{|Q|}P_i，P_i = \frac {|A_i \cap G_i|} {|A_i|} Macro Recall = \frac{1}{|Q|} \sum_{i=1}^{|Q|}R_i，R_i = \frac {|A_i \cap G_i|} {|G_i|} Averaged F1 = \frac{1}{|Q|} \sum_{i=1}^{|Q|} \frac{2P_iR_i}{P_i+R_i} 两大关键技术 【实体链指】：将问句中的实体名字链接到知识库中特定的实体上，涉及到实体识别和实体消歧。 【关系抽取】：将问句中的实体关系抽取出来，涉及到词性标注、词法句法分析、关系分类等。 举个栗子：姚明的老婆是什么星座？ （姚明，妻子，叶莉）—&gt;（叶莉，星座，天蝎） 2、相关评测简单问题：NLPCC 2015-2018 NLPCC2015 评测：Open Domain Question Answering NLPCC2016 评测：Open Domain Chinese Question Answering NLPCC2017 评测：Open Domain Question Answering NLPCC2018 评测：Open Domain Question Answering 复杂问题：CCKS 2018-2019 CCKS2018 评测 任务四：开放领域的中文问答任务，CCKS2018 COQA 比赛平台 CCKS2019 评测 任务六：中文知识图谱问答，CCKS2019 CKBQA 比赛平台，PKU BASE的在线查询终端：gStore，SPARQL语法规则 3、相关数据训练集NLPCC 2016（14609条）2016、2018测试集答案123456&lt;question id=1&gt; 《机械设计基础》这本书的作者是谁？&lt;answer id=1&gt; 杨可桢，程光蕴，李仲生==================================================&lt;question id=2&gt; 《高等数学》是哪个出版社出版的？&lt;answer id=2&gt; 武汉大学出版社================================================== CCKS 2019 （2298条）1234567q1:莫妮卡·贝鲁奇的代表作？select ?x where &#123; &lt;莫妮卡·贝鲁奇&gt; &lt;代表作品&gt; ?x. &#125;&lt;西西里的美丽传说&gt;q2:《湖上草》是谁的诗？select ?x where &#123; ?x &lt;主要作品&gt; &lt;湖上草&gt;. &#125;&lt;柳如是_（明末&quot;秦淮八艳&quot;之一）&gt; 知识库NLPCC 2016 知识库网盘 三元组（43063796条） 12345空气干燥 ||| 别名 ||| 空气干燥空气干燥 ||| 中文名 ||| 空气干燥空气干燥 ||| 外文名 ||| air drying空气干燥 ||| 形式 ||| 两个空气干燥 ||| 作用 ||| 将空气中的水份去除 提及-实体（7623034条） 12345空气 干燥 ||| 空气干燥air drying ||| 空气干燥 氧化结膜干燥罗育德 ||| 罗育德鳞 ||| 鳞 公子鳞squama ||| 鳞 鳞片 CCKS 2019 知识库网盘，密码：hcu8 三元组（41009142条） 12345&lt;美国奥可斯（香港）国际控股集团&gt; &lt;公司名称&gt; &quot;美国奥可斯（香港）国际控股集团有限公司&quot; .&lt;美国奥可斯（香港）国际控股集团&gt; &lt;成立时间&gt; &quot;2007-06-28&quot; .&lt;美国奥可斯（香港）国际控股集团&gt; &lt;经营范围&gt; &lt;培训&gt; .&lt;美国奥可斯（香港）国际控股集团&gt; &lt;经营范围&gt; &lt;影视&gt; .&lt;美国奥可斯（香港）国际控股集团&gt; &lt;公司口号&gt; &quot;品牌立业，质量最好&quot; . 提及 + 实体 + order（13930118条） 12345献陵 献陵_（唐高祖李渊陵墓） 1献陵 明献陵 2献陵 献陵_（朝鲜太宗献陵） 3佛罗伦萨 佛罗伦萨_（意大利托斯卡纳大区首府） 1佛罗伦萨 佛罗伦萨足球俱乐部 2 实体 + 类型 + 值（25182628条） 12345&lt;美国奥可斯（香港）国际控股集团&gt; &lt;类型&gt; &lt;文学作品&gt; .&lt;美国奥可斯（香港）国际控股集团&gt; &lt;类型&gt; &lt;文化&gt; .&lt;寻美中国&gt; &lt;类型&gt; &lt;品牌&gt; .&lt;青春是我和你一杯酒的深&gt; &lt;类型&gt; &lt;文学作品&gt; .&lt;青春是我和你一杯酒的深&gt; &lt;类型&gt; &lt;网络小说&gt; . 4、相关工作 NLPCC2015 第1名评测论文 [1] NLPCC2016 第1-4名评测论文 [2-5] NLPCC2017 第1-2名评测论文 + 会议论文 [6-8] NLPCC2018 第1名评测论文 [9] CCKS2018 第1-3名评测论文 [10-12] 二、方法1、创建数据库 图数据库：NEO4J、JENA、gStore（Linux） 关系数据库：MySQL（ours）、PostgreSQL 2、创建训练数据分类单多跳问句（二分类） 单跳：SPQRQL 只出现一个三元组 123q1:莫妮卡·贝鲁奇的代表作？select ?x where &#123; &lt;莫妮卡·贝鲁奇&gt; &lt;代表作品&gt; ?x. &#125;&lt;西西里的美丽传说&gt; 双跳：SPQRQL 出现两个以上三元组 123q616:红豆的演唱者出生在？select ?y where &#123; ?x &lt;代表作品&gt; &lt;红豆_（王菲演唱歌曲）&gt;. ?x &lt;出生地&gt; ?y. &#125;&lt;东城区_（北京市东城区）&gt; 分类链式问句（二分类） 链式：SPQRQL 多个三元组呈递进关系，x-&gt;y-&gt;z，非交集关系1234567q894:纳兰性德的父亲担任过什么官职？select ?y where &#123; &lt;纳兰性德&gt; &lt;父亲&gt; ?x. ?x &lt;主要职位&gt; ?y. &#125;&quot;武英殿大学士&quot; &quot;太子太傅&quot;q554:宗馥莉任董事长的公司的公司口号是？select ?y where &#123; ?x &lt;董事长&gt; &lt;宗馥莉&gt;. ?x &lt;公司口号&gt; ?y. &#125;&quot;win happy health,娃哈哈就在你身边&quot; 实体提及识别（NER序列标注） 根据训练语料的SPARQL语句，查找实体的提及，反向构建训练数据123456789101112131415161718q1408:电影《怦然心动》的主要演员？select ?x where &#123; &lt;怦然心动_（美国2010年罗伯·莱纳执导电影）&gt; &lt;主演&gt; ?x. &#125;&lt;艾丹·奎因&gt; &lt;玛德琳·卡罗尔&gt; &lt;卡兰·麦克奥利菲&gt; &lt;约翰·玛哈尼&gt; &lt;摩根·莉莉&gt;电 O影 O《 O怦 B-LOC然 I-LOC心 I-LOC动 I-LOC》 O的 O主 O要 O演 O员 O？ O 关系抽取（语义相似度计算，二分类） 查找实体的关系中与问句最相近的关系 一个正例，再从对应实体的关系中随机抽取5个作为负例12345678910q267:里奥·梅西的生日是什么时候？select ?x where &#123; &lt;里奥·梅西_（阿根廷足球运动员）&gt; &lt;出生日期&gt; ?x . &#125;&quot;1987-06-24&quot;里奥·梅西的生日是什么时候？ 出生日期 1里奥·梅西的生日是什么时候？ 妻子 0里奥·梅西的生日是什么时候？ 所属运动队 0里奥·梅西的生日是什么时候？ 中文名 0里奥·梅西的生日是什么时候？ 类型 0里奥·梅西的生日是什么时候？ 外文名 0 主谓宾分类（三分类） 问句的答案对应三元组里面的主语，spo=0 123q70:《悼李夫人赋》是谁的作品？select ?x where &#123; ?x &lt;代表作品&gt; &lt;悼李夫人赋&gt;. &#125;&lt;汉武帝_（汉朝皇帝）&gt; 问句的答案对应三元组里面的谓语，spo=1 123q506:林徽因和梁思成是什么关系？select ?x where &#123; &lt;林徽因_（中国建筑师、诗人、作家）&gt; ?x &lt;梁思成&gt;. &#125;&lt;丈夫&gt; 问句的答案对应三元组里面的宾语，spo=2 123q458:天津大学的现任校长是谁？select ?x where &#123; &lt;天津大学&gt; &lt;现任校长&gt; ?x . &#125;&lt;李家俊_（天津市委委员，天津大学校长）&gt; 实体链接（二分类） 查找问句中实体提及对应的唯一实体 6个特征：order、提及初始分、问题和提及字符匹配度、问题和实体语义相似度、问题和实体字符匹配度、问题和实体关系的最大相似度 —&gt; 标签1234567q2035:张三丰创立了什么门派？select ?x where &#123; &lt;张三丰_（武侠小说人物）&gt; &lt;荣誉&gt; ?x . &#125;&quot;创立武当派&quot;张三丰创立了什么门派？ &lt;张三丰_（南宋至明初道士）&gt; 1.0 1.0 0.43 0.15978466 0.6 0.99257445 0张三丰创立了什么门派？ &lt;张三丰_（武侠小说人物）&gt; 0.9 1.0 0.43 0.97132427 0.58 0.99660385 1张三丰创立了什么门派？ &lt;张三丰_（桌游《英雄杀》中的英雄之一）&gt; 0.8 1.0 0.43 0.920861 0.38 0.0007164952 0 3、训练模型 分类单多跳问句（二分类：BERT 做单句子分类） 分类单多跳问句（二分类：BERT 做单句子分类） 实体提及识别（NER序列标注：BERT+BiLSTM+CRF 的 NER 模型） 关系抽取（语义相似度计算，二分类：BERT 做句子对分类） 主谓宾分类（三分类：BERT 做单句子分类） 实体链接（二分类：xgboost 做分类） 4、预测数据 实体提及识别要考虑问句中出现的空格问题 5、搜索答案 注释 123456hop=0 表示是单跳问题（如：姚明的女儿是谁？），hop=1 表示是多跳问题（单跳问题仅需一个三元组，多跳需两个以上）spo=0 表示已知谓宾求主语，spo=1 表示已知主宾求谓语，spo=2表示已知主谓求宾语（如：姚明的女儿是谁？）chain=0 表示非链式问题，chain= 表示链式问题（如：姚明的女儿的年龄是多少？）ner 表示用NER模型识别出问句中的实体提及（如：姚明的女儿是谁？ 提及：姚明）Entity Linking 表示实体链指，找出提及对应的实体（如：姚明的女儿是谁？ 实体：&lt;姚明_（中职联公司董事长兼总经理）&gt;）Relation Extraction 表示关系抽取，找出问题中关系（如：姚明的女儿是谁？ 关系：&lt;女儿&gt;） 详细介绍 12345step1：先对问题进行分类（判断是否单多跳、是否主谓宾、是否链式）和提及识别；step2：根据识别到的提及，进行左右扩展或删减，搜索所有的候选实体，根据一组特征对候选实体打分排序（实体链接模型），取top1；step3：根据spo值，搜索实体对应的所有关系，与当前问题计算语义相似度（关系抽取模型），取top1，搜索数据库得到统一单挑问题的求解；step4：如果是链式且是多跳问题，将step3得到的答案作为实体再进行一遍step3，得到多跳链式问题的求解；step5：如果是非链式且识别到多个实体，对每个实体搜索数据库，查询对应的所有候选三元组，然后求交集，得到多跳多实体问题的求解。 三、总结 参加完CCKS2019评测会议后再补充 。。。。。。 四、参考文献 [1] NLPCC2015 1st Ye Z, Jia Z, Yang Y, et al. Research on open domain question answering system[M]//Natural Language Processing and Chinese Computing. Springer, Cham, 2015: 527-540. [2] NLPCC2016 1st Lai Y, Lin Y, Chen J, et al. Open domain question answering system based on knowledge base[M]//Natural Language Understanding and Intelligent Applications. Springer, Cham, 2016: 722-733. [3] NLPCC2016 2nd Yang F, Gan L, Li A, et al. Combining deep learning with information retrieval for question answering[M]//Natural Language Understanding and Intelligent Applications. Springer, Cham, 2016: 917-925. [4] NLPCC2016 3rd Xie Z, Zeng Z, Zhou G, et al. Knowledge base question answering based on deep learning models[M]//Natural Language Understanding and Intelligent Applications. Springer, Cham, 2016: 300-311. [5] NLPCC2016 4th Wang L, Zhang Y, Liu T. A deep learning approach for question answering over knowledge base[M]//Natural Language Understanding and Intelligent Applications. Springer, Cham, 2016: 885-892. [6] NLPCC2017 1st Lai Y, Jia Y, Lin Y, et al. A Chinese question answering system for single-relation factoid questions[C]//National CCF Conference on Natural Language Processing and Chinese Computing. Springer, Cham, 2017: 124-135. [7] NLPCC2017 2nd Zhang H, Zhu M, Wang H. A Retrieval-Based Matching Approach to Open Domain Knowledge-Based Question Answering[C]//National CCF Conference on Natural Language Processing and Chinese Computing. Springer, Cham, 2017: 701-711. [8] NLPCC2017 会议 周博通, 孙承杰, 林磊, et al. 基于LSTM的大规模知识库自动问答[J]. 北京大学学报：自然科学版, 2018. [9] NLPCC2018 1st Ni H, Lin L, Xu G. A Relateness-Based Ranking Method for Knowledge-Based Question Answering[C]//CCF International Conference on Natural Language Processing and Chinese Computing. Springer, Cham, 2018: 393-400. [10] CCKS2018 1st A QA Search Algorithm based on the Fusion Integration of Text Similarity and Graph Computation [11] CCKS2018 2nd A Joint Model of Entity Linking and Predicate Recognition for Knowledge Base Question Answering [12] CCKS2018 3rd Semantic Parsing for Multiple-relation Chinese Question Answering 五、相关博客 基于知识图谱的问答系统入门之—NLPCC2016KBQA数据集 ，Github：bert-kbqa-NLPCC2017 基于BERT的KBQA探索，Github：KBQA-BERT CCKS2018 CKBQA 1st 方案 自由讨论 | KBQA从入门到放弃—入门篇 KBQA从入门到放弃 - Part 2 | 每周话题精选 #09 KBQA 知识库问答领域研究综述（未完待续。。） 基于知识库的问答：seq2seq模型实践 KBQA 个人总结 揭开知识库问答KB-QA的面纱0·导读篇 知识图谱问答总结 肖仰华 | 基于知识图谱的问答系统 基于知识图谱的问答系统(KBQA) 各类QA问答系统的总结与技术实现（持续更新） 知识图谱入门 (九)知识问答 KBQA: 基于开放域知识库上的QA系统 | 每周一起读]]></content>
      <categories>
        <category>总结</category>
      </categories>
      <tags>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[常用的排序算法]]></title>
    <url>%2F2019%2F04%2F17%2Fsort%2F</url>
    <content type="text"><![CDATA[一、复杂度 排序方式 平均T(n) 最坏T(n) 最好T(n) 空间复杂度 稳定性 插入排序 O(n^2) O(n^2) O(n) O(1) 稳定 冒泡排序 O(n^2) O(n^2) O(n) O(1) 稳定 选择排序 O(n^2) O(n^2) O(n^2) O(1) 不稳定 希尔排序 O(n^1.3) O(n^2) O(n) O(1) 不稳定 快速排序 O(nlogn) O(n^2) O(nlogn) O(logn) 不稳定 堆排序 O(nlogn) O(nlogn) O(nlogn) O(1) 不稳定 归并排序 O(nlogn) O(nlogn) O(nlogn) O(n) 稳定 基数排序 O(d(n+r)) O(d(n+r)) O(d(n+r)) O(n+rd) 稳定 二、代码实现1、直接插入排序 insert12345678# 将 L[i] 插入到已经有序的子序列 L[1 2 ... i-1] 中def InsertSort(arr): for i in range(1, len(arr)): j = i while j &gt; 0 and arr[j] &lt; arr[j-1]: arr[j], arr[j-1] = arr[j-1], arr[j] j -= 1 2、冒泡排序 bubble1234567# 对相邻的元素两两进行比较，顺序相反则进行交换，这样每一趟最大的元素浮到顶端def BubbleSort(arr): for i in range(len(arr)-1): for j in range(len(arr)-1-i): if arr[j] &gt; arr[j+1]: arr[j], arr[j+1] = arr[j+1], arr[j] 3、简单选择排序 select1234567891011# 第i趟排序从 L[i,i+1,...,n] 中选择关键字最小的元素与 L[i] 比较# 每一趟从待排序的数据元素中选择最小的元素作为首元素def SelectSort(arr): for i in range(len(arr)-1): minv = i for j in range(i+1, len(arr)): if arr[j] &lt; arr[minv]: minv = j if minv != i: arr[minv], arr[i] = arr[i], arr[minv] 4、希尔排序 shell（跳着插入排序）123456789101112距离为 di 的记录放在同一个组中，进行直接插入排序di = 4, 2, 1def ShellSort(arr): step = len(arr) / 2 while step &gt; 0: for i in range(step, len(arr)): # 插入排序 j = i while j &gt;= step and arr[j] &lt; arr[j-step]: arr[j], arr[j-step] = arr[j-step], arr[j] j -= step step = step / 2 5、快速排序 quick123456789101112131415161718192021222324252627282930待排序表 L[1,2,...,n] 中任取一个元素 pivot 作为基准或枢纽，将表划分成两部分，一部分小于 pivot，一部分大于或等于 pivotL[1,2,...,k-1] 和 L[k+1,...,n] , L[k] = pivot操作：以当前表中第一个元素作为枢纽，对表进行划分将表中比枢纽值大的元素向右移动，小的向左移动移动采用从两端往中间夹入的方式（可用于求n个元素中第k小的元素）def Partition(arr, begin, end): # 划分元素 pivot = arr[begin] # 选取第一个元素作为基准 left = begin + 1 right = end while True: while left &lt;= right and arr[left] &lt;= pivot: left += 1 while left &lt;= right and arr[right] &gt;= pivot: right -= 1 if left &lt; right: arr[left], arr[right] = arr[right], arr[left] else: break arr[begin], arr[right] = arr[right], pivot # 划分元素放到中间位置 return right # 返回划分元素的下标def QuickSort(arr, begin, end): if begin &lt; end: k = Partition(arr, begin, end) Partition(arr, begin, k-1) Partition(arr, k+1, end) 6、堆排序 heap sort12345678910111213141516171819202122232425大顶堆（完全二叉树）：子节点小于父节点建堆 A[0,2,...,n-1]，移除根节点，将A[0]与A[n-1]交换，做最大堆调整的递归运算，建堆 A[0,2,...,n-2]，将A[0]与A[n-2]交换，直到 A[0]与A[1]交换思想可用于求大量元素中最小的或最大的几个元素def MaxHeap_adjust(arr, low, high): tmp = arr[low] # 父节点 while 2 * low + 1 &lt;= high: child = 2 * low + 1 # 左子节点 if child &lt; high and arr[child] &lt; arr[child+1]: child += 1 if arr[child] &lt; tmp: break arr[low] = arr[child] low = child arr[low] = tmpdef MaxHeapSort(arr): n = len(arr) for i in range(n/2-1, -1, -1): # 从下往上调 MaxHeap_adjust(arr, i, n-1) for i in range(n-1, 0, -1): arr[i], arr[0] = arr[0], arr[i] # 最大值放最后面 MaxHeap_adjust(arr, 0, i-1) # 重新调整一下 7、二路归并排序 merge（分治）12345678910111213141516171819202122n 个记录看成是 n 个有序的子表，两两归并，得到 n/2 个有序表，再归并def Merge(left, right): i, j = 0, 0 res = [] # 缺点：需要辅助空间 while i &lt; len(left) and j &lt; len(right): if left[i] &lt;= right[j]: res.append(left[i]) i += 1 else: res.append(right[j]) j += 1 res += left[i:] if i &lt; len(left) else right[j:] return resdef MergeSort(arr): if len(arr) &lt;= 1: return arr middle = len(arr) / 2 left = MergeSort(arr[:middle]) right = MergeSort(arr[middle:]) return Merge(left, right) 8、基数排序 radix（桶排序）12对数字最高位优先和最低位优先进行排序例如：先按个位从小到大，再按十位从小到大，再按百位从小到大]]></content>
      <categories>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>数据结构</tag>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[安装 cuda、cudnn、nvidia 驱动]]></title>
    <url>%2F2019%2F04%2F17%2Finstall_cuda%2F</url>
    <content type="text"><![CDATA[〇、TensorFlow 与 cuda 的对应版本 官方链接：https://tensorflow.google.cn/install/source123456789101112131415版本 Python 版本 编译器 编译工具 cuDNN CUDAtensorflow_gpu-1.13.0 2.7、3.3-3.6 GCC 4.8 Bazel 0.19.2 7.4 10.0tensorflow_gpu-1.12.0 2.7、3.3-3.6 GCC 4.8 Bazel 0.15.0 7 9tensorflow_gpu-1.11.0 2.7、3.3-3.6 GCC 4.8 Bazel 0.15.0 7 9tensorflow_gpu-1.10.0 2.7、3.3-3.6 GCC 4.8 Bazel 0.15.0 7 9tensorflow_gpu-1.9.0 2.7、3.3-3.6 GCC 4.8 Bazel 0.11.0 7 9tensorflow_gpu-1.8.0 2.7、3.3-3.6 GCC 4.8 Bazel 0.10.0 7 9tensorflow_gpu-1.7.0 2.7、3.3-3.6 GCC 4.8 Bazel 0.9.0 7 9tensorflow_gpu-1.6.0 2.7、3.3-3.6 GCC 4.8 Bazel 0.9.0 7 9tensorflow_gpu-1.5.0 2.7、3.3-3.6 GCC 4.8 Bazel 0.8.0 7 9tensorflow_gpu-1.4.0 2.7、3.3-3.6 GCC 4.8 Bazel 0.5.4 6 8tensorflow_gpu-1.3.0 2.7、3.3-3.6 GCC 4.8 Bazel 0.4.5 6 8tensorflow_gpu-1.2.0 2.7、3.3-3.6 GCC 4.8 Bazel 0.4.5 5.1 8tensorflow_gpu-1.1.0 2.7、3.3-3.6 GCC 4.8 Bazel 0.4.2 5.1 8tensorflow_gpu-1.0.0 2.7、3.3-3.6 GCC 4.8 Bazel 0.4.2 5.1 8 一、查看版本1、查看 cuda 版本 cat /usr/local/cuda/version.txt nvcc -V 2、查看 cudnn 版本 cat /usr/local/cuda/include/cudnn.h | grep CUDNN_MAJOR -A 2 cat /usr/include/cudnn.h | grep CUDNN_MAJOR -A 2 3、查看显卡驱动版本 cat /proc/driver/nvidia/version nvidia-smi 二、安装1、安装cuda Ubuntu16.04+cuda9.0安装教程 下载链接：https://developer.nvidia.com/cuda-toolkit-archive 执行命令：sudo sh cuda_9.0.176_384.81_linux.run ，不要安装驱动 配置系统环境变量：sudo vim /etc/profile 或者用户环境变量：vim ~/.bashrc1234export PATH=/usr/local/cuda-9.0/bin:$PATHexport LD_LIBRARY_PATH=/usr/local/cuda-9.0/lib64$LD_LIBRARY_PATH# 系统环境变量可能要重启电脑 `sudo reboot` 2、安装nvidia驱动 列出所有可用的 NVIDIA 设备信息：nvidia-smi -L 查找适配自己电脑GPU的驱动：http://www.nvidia.cn/Download/index.aspx?lang=cn 解决nvidia升级驱动后版本匹配问题123456789sudo apt-get purge nvidia*sudo add-apt-repository ppa:graphics-drivers/ppasudo apt-get updatesudo apt-get install nvidia-384 nvidia-settingsreboot注：如果要是不行，可能需要在bios里将显卡先设置成CPU集卡验证1.输入nvidia-smi查看2.prime-select query查看当前选用的显卡 3、安装cudnn 下载链接：https://developer.nvidia.com/rdp/cudnn-archive 执行命令拷贝文件，后者用软连接 12345tar -zxvf cudnn-9.0-linux-x64-v7.1.tgzsudo cp cuda/include/cudnn.h /usr/local/cuda/include/ sudo cp cuda/lib64/libcudnn* /usr/local/cuda/lib64/ -d sudo chmod a+r /usr/local/cuda/include/cudnn.h sudo chmod a+r /usr/local/cuda/lib64/libcudnn* 配置系统环境变量：sudo vim /etc/profile 123export PATH=/usr/local/cuda/bin$&#123;PATH:+:$&#123;PATH&#125;&#125;export LD_LIBRARY_PATH=/usr/local/cuda/lib64$&#123;LD_LIBRARY_PATH:+:$&#123;LD_LIBRARY_PATH&#125;&#125;export CUDA_HOME=/usr/local/cuda 刷新环境变量：source /etc/profile 三、卸载卸载 cuda1234cd /usr/local/cuda/binsudo ./uninstall_cuda_8.0.plcd /usr/localsudo rm -rf cuda-8.0 四、相关链接官方教程cuda、cudnn、驱动版本查看及nvidia驱动、cuda安装及卸载ubuntu下卸载cuda8.0，和安装cuda9.0，cudnn7.0,tensorflow-gpu=1.8]]></content>
      <categories>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[评价准则]]></title>
    <url>%2F2018%2F10%2F07%2Fevaluation%2F</url>
    <content type="text"><![CDATA[谈谈评价指标中的宏平均和微平均 分类器的性能表现评价（混淆矩阵，准确率，召回率，F1,mAP、ROC曲线） python + sklearn ︱分类效果评估——acc、recall、F1、ROC、回归、距离 准确率(Accuracy), 精确率(Precision), 召回率(Recall)和F1-Measure 对于二分类问题，可将样例根据其真实类别和分类器预测类别划分为： 0 1 2 真实类别 预测类别 真正例 True Positive TP 正例 正例 假正例 False Positive FP 负例 正例 假负例 False Negative FN 正例 负例 真负例 True Negative TN 负例 负例 然后可以构建混淆矩阵（Confusion Matrix）如下表所示： - 预测 正例 预测 负例 真实 正例 TP FN 真实 负例 FP TN \text{测试集：} X_{test} = \{(x_i, y_i) | i = 1, 2, ..., N \}N \text{：表示测试集中的样本个数}x_i \text{：表示测试集中的数据样本}y_i \text{：表示数据样本的类别号}\text{假设要研究的分类问题含有 m 个类别，则} y_i \in \{c_1, c_2, ..., c_m \}\text{在分类问题中对于测试集的第j个类别，假设被正确分类的样本数量为} TP_j\text{被错误分类的样本数量为} FN_j\text{其它类别被错误分类为该类的样本数量为} FP_j 精确度Accuracy = \frac {\sum_{j=1}^mTP_j} {N} = \frac {TP+TN} {N}= \frac { \text{分类正确的样本个数} } { \text{分类的所有样本个数} }查全率/召回率 R 第j个类别的查全率表示在本类样本中，被正确分类的样本所占的比例，它表示这个类别的分类精度 Recall_j = \frac {TP_j} {TP_j + FN_j} , 1 \leq j \leq m查准率/准确率 P 第j个类别的查准率表示被分类为该类的样本中，真正属于该类的样本所占的比例，它表示这个类别的分类纯度 Precision_j = \frac {TP_j} {TP_j + FP_j} , 1 \leq j \leq mF1 标准 F1 值比较合理地评价分类器对每一类样本的分类性能。 F_\beta = \frac {(1 + \beta^2) * P * R} {(\beta^2 * P) + R}F1 = \frac {2 * R_j * P_j} {R_j + P_j} , 1 \leq j \leq m, \beta = 1= \frac {2} {1/P + 1/R} 宏平均 Macro-averaging 先对每一个类统计指标值，然后在对所有类求算术平均值 Macro\_P = \frac{1}{n}\sum_{i=1}^mP_iMacro\_R = \frac{1}{n}\sum_{i=1}^mR_iMacro\_F = \frac{1}{n}\sum_{i=1}^mF_iMacro\_F = \frac {2 * Macro\_P * Macro\_R} {Macro\_P + Macro\_R}微平均 Micro-averaging 对数据集中的每一个实例不分类别进行统计建立全局混淆矩阵，然后计算相应指标 Micro\_P = \frac {\sum_{i=1}^mTP_i} {\sum_{i=1}^mTP_i + \sum_{i=1}^mFP_i}= \frac {\sum_{i=1}^mTP_i} {N}Micro\_R = \frac {\sum_{i=1}^mTP_i} {\sum_{i=1}^mTP_i + \sum_{i=1}^mFN_i}= \frac {\sum_{i=1}^mTP_i} {N}Micro\_F = \frac {2 * Micro\_P * Micro\_R} {Micro\_P + Micro\_R}\text{如果对所有类别求微平均，那么上面三个值是相等的，且 = accuracy。}]]></content>
      <categories>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>统计学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[特征选择方法]]></title>
    <url>%2F2018%2F10%2F06%2Ffeature_select%2F</url>
    <content type="text"><![CDATA[CHI、IG： http://songlj.iteye.com/blog/2241763 IG、CHI、TC、TS、IIG 文本挖掘（四）——特征选择 From My Github - 文本分类 表1：词频统计（文档数量）其中，文档总数：N = A+B+C+D - 包含词条 t 不包含词条 t 属于类别 c A C 不属于类别 c B D 1、文档频率 DF （document frequency） 文档频率指训练集中包含该特征词条的文本总数 DF = A + B选择 DF &gt; 某个阈值的特征词条 2、信息增益 IG （information gain） 通过特征词在文本中出现和不出现前后的信息量之差来推断该特征词所带的信息量 IG(t) = H(c) - H(c|t)H(c) = - \sum_{i=1}^mP(c_i)log P(c_i)H(c|t) = - P(t) \sum_{i=1}^m P(c_i|t) log P(c_i|t) - P(\bar{t}) \sum_{i=1}^m P(c_i|\bar{t}) log P(c_i|\bar{t})P(t)=\frac {A+B}{N} \text{：表示样本集中包含词 t 的文本的概率}P(c_i)=\frac {A+C}{N} \text{：表示类文本在样本集中出现的概率}P(c_i|t)=\frac {A}{A+B} \text{：表示文本包含词 t 时属于 c 的条件概率}3、互信息 MI （mutual information） 互信息衡量了特征词条和类别之间的相关性MI(t,c) = log \frac {P(t,c)} {P(t)*P(c)} = log \frac {\frac{A}{N}} {\frac{A+B}{N}*\frac{A+C}{N}}= log \frac {A*N} {(A+C)*(A+B)} = log \frac { \frac{A}{A+C} } { \frac{A+B}{N} }= log \frac{A}{A+C} - log \frac{A+B}{N} = log P(t|c) - log P(t)MI_{avg}(t) = \sum_{i=1}^m P(c_i) MI(t,c_i)MI_{max}(t) = \max_{i=1}^m \{ MI(t,c_i) \} MI(t,c) = 0，当 t 和 c 相互独立时弱点：得分被词条的边缘概率强烈的影响；（条件概率相等时，低频词比高频词有更高的分数） 4、卡方统计 CHI（Chi-Square Statistic） 卡方统计量也用于表征两个变量的相关性，与互信息相比，它同时考虑了特征在某类文本中出现和不出现时的情况 度量了 t 和 c 之间的独立性CHI(t,c) = \frac {N * (AD-BC)^2} {(A+C)*(B+D)*(A+B)*(C+D)}CHI_{avg}(t) = \sum_{i=1}^m P(c_i) CHI(t,c_i)CHI_{max}(t) = \max_{i=1}^m \{ CHI(t,c_i) \} 卡方统计是一个规范值，因此卡方统计值对于相同的类别可以跨词进行比较 如果列联表中的任何单元被轻微填充，这种归一化就会失效（低频词的例子） 因此，卡方统计对于低频词是不可靠的。 5、词条强度/单词权 TS （term strength）法1：博客 TS 计算的是一个词出现的条件概率，即该词在一对相关文本中的某一个文本中出现的条件下，在另一个文本中出现的概率123（1）计算文本数据集中每一对文本的相似度；（2）选择出所有相似度超过阈值的文本对；（3）对所有的单词，根据下式计算它的单词权。 TS(t) = \frac {\text{均包含词t的相关文本对数}} {\text{文本集中的相关文本对总数}}若有一个文本集，其中有N篇文本，M对相关文本有序对，有K对同时包含词t的相关文本有序对，则 TS(t)=P(t|M) = \frac{K}{M} = \frac{\sum_{i=1}^m c_i\text{类包含词t相关文本对数}} {\sum_{i=1}^m c_i\text{类相关文本对数}}\approx \sum_{i=1}^m c_i\text{类包含词t相关文本对数}123即TS(t)表示在文本集的所有相关文本有序对的集合中，同时包含词t的相关文本有序对的比例。若TS(t)值越大，说明词t在相关文本集中出现得越多，即越重要。缺点：要计算文本间的相似度，所以复杂度较高；阈值不易确定。 法2：（Yang Yiming 论文里）文本聚类中的特征选择方法 这个方法基于词条出现在密切相关的文档中的频率来评估词条的重要性 使用一组训练文档来派生出文档对，其相似度（余弦值）高于某个阈值 x 和 y 是一对相似文档 TS(t) = P(t \in y | t \in x) 基于文档聚类，假设有许多共享词的文档是相似的，在相关文档的重叠区域内的词条的信息量相对较大 这个方法不是基于特定任务的；不使用与词条类别相关的信息。 法3：PPT-1 18页 PPT-2 18页 词强度（term strength） \text{已知一个词（特征）在某文档（实例）中出现，}\text{该词在同类（目标函数值相同）文档中出现的概率为词强度。}s(t) = P(t \in d_{Y=y}^i | t \in d_{Y=y}^j)]]></content>
      <categories>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>统计学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[StandfordNLP NLTK 中文工具配置教程]]></title>
    <url>%2F2018%2F09%2F10%2FStandfordNLP%20NLTK%2F</url>
    <content type="text"><![CDATA[Python3 NLTK StandfordNLP 中文工具包配置教程 配置教程-1 配置教程-2 1. 必要的安装包 Python3 Anaconda3 NLTK Anaconda3 jdk_1.8 Java SE Development Kit 8 Downloads StanfordNLP NLTK工具包 StanfordNLP 中文处理工具包 百度云链接 密码：o1l3 【本人打包好】 123456# 中文处理工具包 stanford-jar.zip 文件内容: `ls`chinese.misc.distsim.crf.ser.gz stanford-ner-3.9.1.jarchinese-distsim.tagger stanford-parser.jarchinesePCFG.ser.gz stanford-parser-3.9.1-models.jardata/ stanford-postagger-3.9.1.jarslf4j-api-1.7.25.jar stanford-segmenter-3.9.1.jar 2. 调用代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869import osimport nltk## java 环境java_path = "D:\Program Files\Java\jdk1.8.0_162\\bin\java.exe" # java安装地址os.environ['JAVAHOME'] = java_pathbase_dir = 'D:\data\stanfordnlp\stanford-jar' # StanfordNLP 中文处理工具包的路径## 1. 分词#########################################################################################from nltk.tokenize.stanford_segmenter import StanfordSegmentersegmenter = StanfordSegmenter( path_to_jar=os.path.join(base_dir, 'stanford-segmenter-3.9.1.jar'), path_to_slf4j=os.path.join(base_dir, 'slf4j-api-1.7.25.jar'), path_to_sihan_corpora_dict=os.path.join(base_dir, 'data'), path_to_model=os.path.join(base_dir, 'data/pku.gz'), path_to_dict=os.path.join(base_dir, 'data/dict-chris6.ser.gz'))sent = '这是斯坦福中文分词器测试，南京市长江大桥。我在博客园开了一个博客，我的博客名叫伏草惟存，写了一些自然语言处理的文章。'print(segmenter.segment(sent))## 2. 命名实体识别NER#########################################################################################from nltk.tag import StanfordNERTaggerchi_tagger = StanfordNERTagger( model_filename=os.path.join(base_dir, 'chinese.misc.distsim.crf.ser.gz'), path_to_jar=os.path.join(base_dir, 'stanford-ner-3.9.1.jar') )result = '四川省 成都 信息 工程 大学 我 在 博客 园 开 了 一个 博客 ， 我 的 博客 名叫 伏 草 惟 存 ， 写 了 一些 自然语言 处理 的 文章 。\r\n'for word, tag in chi_tagger.tag(result.split()): print(word,tag)## 3. 词性标注#########################################################################################from nltk.tag import StanfordPOSTaggerchi_tagger = StanfordPOSTagger( model_filename=os.path.join(base_dir, 'chinese-distsim.tagger'), path_to_jar=os.path.join(base_dir, 'stanford-postagger-3.9.1.jar') )result = '四川省 成都 信息 工程 大学 我 在 博客 园 开 了 一个 博客 ， 我 的 博客 名叫 伏 草 惟 存 ， 写 了 一些 自然语言 处理 的 文章 。\r\n'print(chi_tagger.tag(result.split()))## 4. 句法分析#########################################################################################from nltk.parse.stanford import StanfordParserchi_parser = StanfordParser( os.path.join(base_dir, 'stanford-parser.jar'), os.path.join(base_dir, 'stanford-parser-3.9.1-models.jar'), os.path.join(base_dir, 'chinesePCFG.ser.gz') )sent = u'北海 已 成为 中国 对外开放 中 升起 的 一 颗 明星'print(list(chi_parser.parse(sent.split())))## 5. 依存句法分析#########################################################################################from nltk.parse.stanford import StanfordDependencyParserchi_parser = StanfordDependencyParser( os.path.join(base_dir, 'stanford-parser.jar'), os.path.join(base_dir, 'stanford-parser-3.9.1-models.jar'), os.path.join(base_dir, 'chinesePCFG.ser.gz') )res = list(chi_parser.parse('四川 已 成为 中国 西部 对外开放 中 升起 的 一 颗 明星'.split()))for row in res[0].triples(): print(row)]]></content>
      <categories>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[简单使用 Git]]></title>
    <url>%2F2018%2F09%2F10%2Fuse_git%2F</url>
    <content type="text"><![CDATA[Git简明指南：（可能需要翻墙）git - 简明指南 码云（Gitee.com）帮助文档 【1】简单配置1.配置用户信息12$ git config --global user.name &quot;你的用户名&quot;$ git config --global user.email &quot;你的邮箱地址&quot; 2.查看配置信息1$ git config --list 3.添加和提交12$ git add .$ git commit -m &quot;这一步操作的名字&quot; 4.删除和提交12$ git rm &lt;filename&gt;$ git commit -m &quot;删除文件&quot; 5.推送改动到远端1$ git push origin master 6.远程服务器1234$ git remote rename origin oschina # 修改仓库名$ git remote add origin &lt;仓库地址&gt; # 添加一个仓库$ git remote -v # 查看当前仓库对应的远程仓库地址$ git remote set-url origin &lt;仓库地址&gt; # 修改仓库对应的远程仓库地址 7.查看本地仓库的历史记录log1$ git log --author=你的用户名字 8.可以查看改动记录1git status 【2】简单clone项目到本地 下载一个和自己的操作系统相符的git。下载链接 注册一个github账号。注册网址 将项目项目fork到自己的账户下。 将项目clone到本地 先在本地新建一个文件夹：MyGit，打开文件，然后执行：Git Bash Here命令：git init，以创建新的git仓库。命令：git clone https://github.com/xxxx.git， xxxx表示项目的名字。 如果下载的工程带有submodule当使用git clone下来的工程中带有submodule时，初始的时候，submodule的内容并不会自动下载下来的，此时只需执行如下命令：git submodule update --init --recursive 【3】创建新项目，并推送到远程服务器（github, gitlab, gitee）123456git initgit add README.mdgit commit -m "first commit"git remote add origin &lt;server&gt; # 项目地址git push -u origin master# 然后如果需要账号密码的话就输入账号密码，这样就完成了一次提交 【4】克隆远程服务器项目1234567git clone &lt;server&gt; # 项目地址git pull origin master&lt;这里需要修改/添加文件，否则与原文件相比就没有变动&gt;git add .git commit -m "第一次提交"git push origin master# 然后如果需要账号密码的话就输入账号密码，这样就完成了一次提交 【5】注意123456# 按照本文档新建的项目时，在码云平台仓库上已经存在 readme 文件，故在提交时可能会存在冲突，这时您需要选择的是保留线上的文件或者舍弃线上的文件。# 如果您舍弃线上的文件，则在推送时选择强制推送，强制推送需要执行下面的命令：git push origin master -f# 如果您选择保留线上的 readme 文件,则需要先执行：git pull origin master# 然后才可以推送 【6】配置ssh key12345ssh-keygen -t rsa -C &apos;xxx@xxx.com&apos; # 然后一路回车(-C 参数是你的邮箱地址)打开文件 C:\Users\Administrator\.ssh\id_rsa.pub，复制内容打开 github --&gt; Settings --&gt; SSH and GPG keys --&gt; New SSH key把上一步中复制的内容粘贴到Key所对应的文本框，在Title对应的文本框中给这个sshkey设置一个名字，点击Add key按钮ssh -T git@github.com # 验证一下]]></content>
      <categories>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>Git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[问题对语义相似度计算-参赛总结]]></title>
    <url>%2F2018%2F08%2F10%2Fcompetition-summary%2F</url>
    <content type="text"><![CDATA[时间段：2018.06.10~2018.07.20 问题对语义相似度计算（从0到0.5+） 短短一个多月的时间，我学到了很多很多东西，从一个呆头小白初长成人。 首先，必须感谢我的导师能给我这个机会从头到尾完整地参加这次比赛，至始至终地为我们出谋划策，和我们探讨问题并答疑解惑，而且提供了各种宝贵的学习资料和服务器资源。 另外，也要特别感谢我的师兄一路无微不至的提点和帮助，和我一起找方法、看论文、搭模型、改代码，其实我们是从同一个起跑线开始的，到最后被师兄甩了好几条街 T_T。 虽然，比赛期间遇到了很多挫折，刚开始我们真的是一头雾水、无从下手，面对参加同样比赛的其他优秀选手（“老油条”）心里还是蛮慌的，好在勤能补拙，有团队配合，能够齐心协力、互相帮助，最终比赛的结果还算令人满意。 一、相关比赛123任务：语句匹配问题、语义等价判别、语义等价判定、等价；（语句的意图匹配）输入：一个语句对输出：一个数值（0-1之间），表明该语句对的相似程度 第三届魔镜杯大赛 问题相似度算法设计 2018 全国知识图谱与语义计算大会 任务三：微众银行智能客服问句匹配大赛 比赛平台：CCKS 2018 微众银行智能客服问句匹配大赛 ATEC蚂蚁开发者大赛 金融大脑-金融智能NLP服务 博客分享 … … 二、数据形式123456789101112131415161718魔镜杯：脱敏数据，所有原始文本信息都被编码成单字ID序列和词语ID序列。label,q1,q21,Q397345,Q5385940,Q193805,Q6992730,Q085471,Q676160... ...CCKS：中文的真实客服语料。用微信都6年，微信没有微粒贷功能 4。 号码来微粒贷 0微信消费算吗 还有多少钱没还 0交易密码忘记了找回密码绑定的手机卡也掉了 怎么最近安全老是要改密码呢好麻烦 0... ...蚂蚁：蚂蚁金服金融大脑的实际应用场景。1 怎么更改花呗手机号码 我的花呗是以前的手机号码，怎么更改成现在的支付宝的号码手机号 12 也开不了花呗，就这样了？完事了 真的嘛？就是花呗付款 03 花呗冻结以后还能开通吗 我的条件可以开通花呗借款吗 0... ... 三、解决方案（1）问题分析 预测问题对的相似程度，即判别问题对是属于类别1还是类别0，很明显这是一个NLP领域的分类问题，然而区别于传统的文本分类问题： 区别 传统文本分类 问题对相似度计算 输入 只有一个输入 有两个输入 句子长度 文本较长 句子长短不一、且较简短 特征 文本特征 语义特征 。。。 。。。 。。。 （2）数据分析 1、正负样本比例接近于 1：1； 2、相似的句子之间一般都会含有公共词/字符；也会出现包含很多公共词/字符，但句子主语不一样导致两个句子不相似的情况； 3、比赛的数据是没有经过预处理的（去停用词、繁体转简体、清洗）；另外数据中也存在很多脏数据（标注有误、错别字、漏字、简写），也很容易导致分词错误； 4、预训练的词向量数据（除非比赛方提供，否则还需要跟领域相关的语料来进行训练）； （3）分类模型其实我们之前是没有接触过这种类型的比赛的，也没有很多参赛的经验，而是刚刚从零学起，一步一步地摸索，沿着前人的脚步再延伸。 1、比赛方（魔镜杯）Demo：两个句子拼成一个文本，空格连接，以 tfidf 为特征，做逻辑回归；（研究官方Demo时发现代码里有bug：最后提交的是预测为0的概率，实际应该是1） 2、借鉴官方Demo，两个句子拼接，使用传统CNN做文本分类，准确率 80% 左右；经测试q1和q2两个句子分开单独处理后再合并做分类效果是明显好于q1和q2先合并再处理后做分类的； q1、q2 分开单独处理 共享卷积层 不共享卷积层 log_loss 0.258995 0.28949 3、微软发表的一篇论文[1]：DSSM模型（把条目映射成低维向量、计算查询和文档的cosine相似度，即一个查询语句对应多个文档，所以这个模型不太适用这个比赛） DSSM深度结构化语义模型原理 深度语义匹配模型-DSSM 及其变种 深度学习解决 NLP 问题：语义相似度计算 PaperWeekly 第37期 | 论文盘点：检索式问答系统的语义匹配模型（神经网络篇） 4、Quora Semantic Question Matching with Deep Learning 三个LSTM模型，可以作为以下介绍的模型的baseline，进本是基于LSTM和Attention展开。 5、The Stanford Natural Language Inference (SNLI) Corpus 一大堆模型及特征提取方法，很多都是用了模型融合。 6、MatchZoo 12个模型，可能这个工具包是针对QuoraQP数据已经调好了参数，在移植到我们这个比赛的时候效果不是很佳，但可以借鉴。 7、相关博客（句子对匹配方法） 使用深度双向LSTM模型构造社区问答系统 重现后，效果不是很好 深度学习笔记——基于双向RNN（LSTM、GRU）和Attention Model的句子对匹配方法 4个模型，其中 Soft Attention Model 实现后效果较好。 CNN在NLP领域的应用（2） 文本语义相似度计算 类似方法2 LSTM 句子相似度分析 LSTM 简化版 8、Siamese Network 孪生网络 Siamese network 孪生神经网络—一个简单神奇的结构 用于文本相似的Siamese Network Siamese Network理解（附代码） Siamese Network原理 9、ESIM （这个模型是所有模型里面实现后效果最好的，但也有改动，对于脱敏数据是不能实现TreeLSTM的） 源代码：ESIM_keras Kaggle Competition: Quora Question Pairs Problem a single model - ESIM 10、论文[2]里的4个模型：SSE、PWIM、DecAtt、ESIM 论文开源代码 11、论文[3]：BiMPM模型 github源码论文[4]：DR-BiLSTM模型 12、gensim 相似度查询 Gensim官方教程翻译（四）——相似度查询（Similarity Queries） gensim 相似度查询（Similarity Queries）(三) nltk-比较中文文档相似度-完整实例 nltk-比较中文文档相似度-完整实例 13、传统模型工具：xgboost(xgb)、lightgbm(lgb)、随机森林random forest(rf)、极端随机树 Extremely randomized trees(ET或Extra-Trees) 14、前10选手用到的模型 最终单模型的最好效果：log_loss = 0.205189 比赛期间，我实现或者在实现的基础上改进前前后后大概搭建了20多个模型，其实很多模型都还有很大的提升空间，局限于比赛的时间和自己的知识能力，而且在模型的细微之处、参数的初始化以及调参方面自己都没有什么经验，以致自己实现的模型的效果都没有师兄的好 (；へ：)。虽然我们没能进入拍拍贷“魔镜杯”比赛的决赛，但在导师的帮助和特殊关系下，我们也有幸了参加了 top10 选手精彩的决赛答辩(2018-7-24 09:00)，真的受益匪浅。 （4）模型调参 1、拍拍贷一同比赛的某位优秀选手（初赛第16名, 复赛第12名）分享的博客 智能客服问题相似度算法设计——第三届魔镜杯大赛第12名解决方案 队伍：moka_tree 团队 代码分享 2、其实，很多参数我自己设置的都是默认参数，具体没有做很多的微调： 123456789101112131415embedding_dim = 300 # 词向量的维度seq_length = 25 # 文本的最大长度filter_sizes = [3] # 卷积核尺寸列表num_classes = 2 # 类别数 is_pre_train = True # 是否为训练好的词向量is_trainable = True # 动态/静态词向量 num_filters = 300 # 卷积核数目rnn_num_layers = 2 # LSTM 隐藏层的个数attention_size = 300 # Attention 层的大小rnn_hidden_size = 300 # LSTM 隐藏层的大小dropout_keep_prob = 0.5 # dropout 保留比例learning_rate = 1e-3 # 学习率（设置自动衰减）batch_size = 128 # 每批训练的大小 3、参数初始化：跟上面博客里分享的一样，TensorFlow里面参数初始化不同，对结果的影响非常大，师兄推荐也是使用 Xavier 初始化；原本想用keras再实现一遍的，一方面不太熟悉，另一方面由于时间紧迫未能完成。 4、决赛答辩里，我们了解到很多选手并没有使用官方给定词级和字符级的词向量（不知道训练方法、参数、模型等），都自己训练了两种词向量（word2vec、glove）；另外也有用 w_vector * w_tfidf 作为 w 的词向量。 5、重点关注字向量：由于中文分词难度较大，特别是不同领域内的领域分词没有很好的解决方案（比赛数据为金融领域数据源），而且实验的效果也表明词级别是好于字级别的。 6、BatchNormalization + Spatial Dropout 1D （5）特征工程1、人工设计特征这部分是我们团队中来也公司的几个小伙伴做的， 他们参考并设计了很多有趣的特征。 统计特征：句长、公共词、fuzzywuzzy、stat_feature、cosine 欧式 明氏 切氏等距离、多项式 拉普拉斯 sigmod等核函数、重叠词、重叠字等特征； XGB_handcrafted_leaky 主题特征：powerful words、tfidf matrix、PCA、NMF、NLP feature； is_that_a_duplicate_quora_question 图特征 kaggle-quora-question-pairs 2、其他选手 计算QA pair的几种相似度：编辑距离、马氏距离、闵可夫斯基距离、WMD 使用 SVD、NMF对句中词向量降维 根据共现图，统计节点的degree，得到了两个比较强的特征：coo_q1_q2_degree_diff（问题1和问题2的degree的差异）、coo_max_degree（问题对最大的degree，进一步离散化做1-hot又得到3个特征） 问题对公共邻居的数量/比例 第一名：提取问题出入度、pagerank等特征；问题出现的次数以及频繁程度特征；将所有已知的问题构建同义问题集。问题集的构建不参与训练，只用于数据增强； 3、数据增强 第一名的方法 123456假设 Q1 在所有样本里出现2次，分别是1，Q1，Q21，Q3，Q1模型无法正确学习Q1与Q2/Q3的相同，而是会认为只要input里有Q1即为正样本。需要通过数据处理引导模型进行“比较”，而不是“拟合”。解决方案：通过构建一部分补充集（负例），对冲所有不平衡的问题。 4、后处理 传递关系 相似：（AB=1，AC=1）—&gt; BC=1 不相似：（AB=1，AC=0）—&gt; BC=0 第一名的方法：infer机制：除了判断test集的每个样本得分以外，还会通过已知同义问题集的其他样本比对进行加权；融合时轻微降低得分过高的模型权重，补偿正样本过多的影响；将已知确认的样本修正为0/1。 比别人差的一个重要原因：传递关系没有考虑到闭包！我们大概推了1253条，然而别人正例推了12568个样本，负例推了5129个样本。 ╥﹏╥ （6）模型融合 1、多模型的融合最常用的一个方法就是求平均，我使用这个方法后 logloss 有很大的提升（加权平均的几个结果都是线上提交后 logloss 在 0.205189~0.209739 之间）。 求平均的数量 2 4 7 8 9 线上提交 logloss 0.187845 0.185329 0.182613 0.179808 0.179063 2、同一个模型提升效果的常用方法就是多折交叉验证求平均，由于我们组内 GPU 服务器有限，这个就由模型效果比较好的师兄来完成了，而且提升也是非常明显的。 3、另外，也用了堆叠和混合（stacking与blending）。 每个模型 word level（官方词向量） 每个模型 word level（word2vec） 每个模型 word level（glove） 每个模型 char level（官方字向量） 每个模型 char level（word2vec） 每个模型 char level（glove） 4、kaggle比赛集成指南 5、模型微调（Finetune） 第一名的方法：gensim训练词向量；模型使用non_trainable词向量进行训练；将除了embedding的layer全部freeze，用低学习率finetune词向量层。 小 trick 贡献度 多模型的预测结果求平均 logloss 降低 2.6 个百分点 同一个模型10折交叉验证 logloss 降低 2 个 百分点 传递关系推导 logloss 降低 3.1 个千分点 四、比赛总结 1、比赛成绩（logloss / F1） 拍拍贷 初赛成绩（359只队伍） 复赛成绩（95只队伍） 我们 0.166100（第22名） 0.162495（第21名） moka_tree 0.163249（第16名） 0.151729（第12名） SKY 0.141329（第1名） 0.142658（第1名） CCKS 初赛成绩（138只队伍） 复赛成绩（50只队伍） 我们 0.85142（第24名） 0.84586（第4名） ThunderUp 0.86485（第1名） 0.85131（第1名） 2、经验体会 刚开始，我们都是尝试各种模型，不知道哪一个好，在这个上面花了不少时间，其实从平时就应该开始积累，关注最新研究、最新模型，多看一下论坛、kaggle、quora、github、和NLP相关的公众号等。 一定要从数据本身上做探索，研究各种特征，因为到比赛后期模型基本都相似了，很难再有更大的提升；从决赛答辩来看，前10的选手在数据特征上都下了非常大的功夫，比如图特征等。 一定要做交叉验证，求平均。比赛方提供的训练集如果只用了 0.9 的数据来训练模型，那么模型很大程度会丢失剩下的 0.1 的信息，如果做了交叉验证的话，就可以兼顾到所有训练集的特征信息。 从比赛角度讲，深度学习框架 keras 是好于 TensorFlow 的，因为 keras 一般在参数调试、参数初始化以及模型搭建上面都整合的非常好；从科研角度讲，Tensorflow 具有清晰的流程，可以帮助你更好的理解深度学习的完整过程。 到了比赛后期，多模型的融合一定会有帮助，因为这样可以结合不同的模型的优缺点；模型融合最简单的方法是就是求平均，再复杂点就是对不同的模型依据效果的好坏赋予不同的权重在加权求和。 之前一直很纳闷人工设计的传统特征是怎样可以和深度学习模型相结合的，通过这次比赛，我也学习到了很多传统的NLP模型（xgboost、lightgbm、随机森林、极端随机树等），设计的特征可以加入到最后一层MLP层进行训练。 一定要有团队配合，“三个臭皮匠，顶个诸葛亮”，“1+1&gt;2”，真的真的可以从别人身上学习到很多很多的东西。 一定要多看论文、多写代码，多请教师兄、导师，“纸上得来终觉浅，绝知此事要躬行。”，“冰冻三尺，非一日之寒。”，调参经验、模型的搭建很多都是来自平时的积累、练习。 作为一个小白，一定要比别人花更多的时间和努力，才能笨鸟先飞、勤能补拙。 再忙再累也要多运动、多锻炼，身体是革命的本钱，一定要爱惜身体，督促自己，实验室固然安逸，但整天坐着身体的机能肯定会下降，发际线正在颤抖。 “路漫漫其修远兮，我将上下而求索。” 五、参考文献 [1] Huang P S, He X, Gao J, et al. Learning deep structured semantic models for web search using clickthrough data[C]// ACM International Conference on Conference on Information &amp; Knowledge Management. ACM, 2013:2333-2338. [2] Lan W, Xu W. Neural Network Models for Paraphrase Identification, Semantic Textual Similarity, Natural Language Inference, and Question Answering[J]. 2018. [3] Wang Z, Hamza W, Florian R. Bilateral Multi-Perspective Matching for Natural Language Sentences[J]. 2017. [4] Ghaeini R, Hasan S A, Datla V, et al. DR-BiLSTM: Dependent Reading Bidirectional LSTM for Natural Language Inference[J]. 2018. 其他比赛选手总结 https://kexue.fm/archives/5743?from=timeline&amp;isappinstalled=0]]></content>
      <categories>
        <category>总结</category>
      </categories>
      <tags>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2018%2F08%2F10%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy 一条命令12hexo clean &amp; hexo g &amp; hexo s # 默认端口号：4000hexo clean &amp; hexo g &amp; hexo server -p 5000 # 如果4000端口打不开，改用5000 More info: Deployment]]></content>
      <categories>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
      </tags>
  </entry>
</search>
